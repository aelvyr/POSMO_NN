{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#col_array = ['label', 'label2' 'label_id', 'speed', 'heading_x', 'heading_y', 'heading_z', 'acceleration_x', 'acceleration_y', 'acceleration_z', 'gyroscope_x', 'gyroscope_y', 'gyroscope_z', 'magnetic_field_x', 'magnetic_field_y', 'magnetic_field_z', 'pressure_hPa', 'log_timestamp']\n",
    "col_array = ['label_id', 'acceleration_x', 'acceleration_y', 'acceleration_z', 'gyroscope_x', 'gyroscope_y', 'gyroscope_z', 'magnetic_field_x', 'magnetic_field_y', 'magnetic_field_z']\n",
    "path = \"../posmo/experiments/data/posmo_segments/v8/\" \n",
    "def read_values(csv, col_array, path=\"../posmo/experiments/data/posmo_segments/v8/\"):\n",
    "    return pd.read_csv(path + csv).loc[:, col_array] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = []\n",
    "for fname in glob.glob(\"../posmo/experiments/data/posmo_segments/v8/*.csv\"):\n",
    "    files.append(fname)\n",
    "for file in files:\n",
    "    if file in glob.glob(\"../posmo/experiments/data/posmo_segments/v8/Other*\"):\n",
    "        files.remove(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbike1 = read_values(\"Bike-20180425T600 PM-aelvyr@gmail.com-8DD36F40-3778-454F-A8D8-E330C6F39830.csv\", col_array)\\nbike2 = read_values(\"Bike-20180426T518 PM-aelvyr@gmail.com-33A201D1-213B-4B6B-A555-84A9CF015E87.csv\", col_array)\\nbike3 = read_values(\"Bike-20180426T527 PM-aelvyr@gmail.com-8623C55E-8B27-4D36-93C5-7F91F53CFF6C.csv\", col_array)\\nbike4 = read_values(\"Bike-20180427T149 PM-aelvyr@gmail.com-1222C4CE-8523-45AD-A22B-030C80EEEDDC.csv\", col_array)\\nbike5 = read_values(\"Bike-20180427T1218 AM-aelvyr@gmail.com-09521095-A9D3-4662-A914-7DCBC1422947.csv\", col_array)\\nbus1 = read_values(\"Bus-20180423T0929-djordje@datamap.io-F068A314-DC4D-4FB0-9D97-E79929993E41.csv\", col_array)\\nbus2 = read_values(\"Bus-20180423T2150-djordje@datamap.io-DEB56810-DB1E-4E14-A144-130B52E0F405.csv\", col_array)\\ncar1 = read_values(\"Car-20180420T0000-djordje@datamap.io-B2876225-A37C-4A72-B045-790E5437B114.csv\", col_array)\\ncar2 = read_values(\"Car-20180420T0912-djordje@datamap.io-37B35625-AAF7-4217-9535-38D6BCF3FF93.csv\", col_array)\\ncar3 = read_values(\"Car-20180420T1328-roger@datamap.io-A3DB32B6-A6AC-4BD9-88EF-8AFFCFCCEDC1.csv\", col_array)\\nflat1 = read_values(\"Other-Flat-20180424T1445-roger@datamap.io-1A6D2916-7C62-48D4-8CB9-9CBBA9FD7E20.csv\", col_array)\\nflat2 = read_values(\"Other-flat-reversed-20180424T1538-roger@datamap.io-7419A222-F45B-415A-A3DF-08691E85B99A.csv\", col_array)\\nstanding = read_values(\"Other-InPocket-20180424T1446-roger@datamap.io-6DA03A16-DE1F-4508-BFC8-E8403F621954.csv\", col_array)\\nscooter = read_values(\"Other-Scooter-20180422T2049-roger@datamap.io-0C33D54C-1B7B-4C45-92D3-2F89A7A82C16.csv\", col_array)\\ntram1 = read_values(\"Tram-20180426T1825-roger@datamap.io-7C72A146-5733-446A-8418-5126D1BB74DE.csv\", col_array)\\ntram2 = read_values(\"Tram-20180427T1806-roger@datamap.io-9D3D94ED-4E6B-4382-87A0-0F9A3BA4AA82.csv\", col_array)\\ntrain = read_values(\"Train-20180420T1731-roger@datamap.io-E15B0F2F-910B-49CC-A96D-D54276492C97.csv\", col_array)\\nwalk1 = read_values(\"Walk-20180418T1004-roger@datamap.io-D7CA4A5B-0BDC-4E28-9BF4-3DD46B6F91AC.csv\", col_array)\\nwalk2 = read_values(\"Walk-20180418T1507-roger@datamap.io-F3AD9D9D-6B43-4AD1-885B-EAFF3E531A25.csv\", col_array)\\nwalk3 = read_values(\"Walk-20180418T1753-roger@datamap.io-AE01B243-595E-4F19-BAE3-8068038A1AE0.csv\", col_array)\\nwalk4 = read_values(\"Walk-20180419T0011-roger@datamap.io-50A0AB4B-4DB1-4231-8BA6-179DF3EFA381.csv\", col_array)\\nwalk5 = read_values(\"Walk-20180419T1025-roger@datamap.io-9F5D6118-143B-4625-8CD2-A94C5E7E62A0.csv\", col_array)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bike1 = read_values(\"Bike-20180425T600 PM-aelvyr@gmail.com-8DD36F40-3778-454F-A8D8-E330C6F39830.csv\", col_array)\n",
    "bike2 = read_values(\"Bike-20180426T518 PM-aelvyr@gmail.com-33A201D1-213B-4B6B-A555-84A9CF015E87.csv\", col_array)\n",
    "bike3 = read_values(\"Bike-20180426T527 PM-aelvyr@gmail.com-8623C55E-8B27-4D36-93C5-7F91F53CFF6C.csv\", col_array)\n",
    "bike4 = read_values(\"Bike-20180427T149 PM-aelvyr@gmail.com-1222C4CE-8523-45AD-A22B-030C80EEEDDC.csv\", col_array)\n",
    "bike5 = read_values(\"Bike-20180427T1218 AM-aelvyr@gmail.com-09521095-A9D3-4662-A914-7DCBC1422947.csv\", col_array)\n",
    "bus1 = read_values(\"Bus-20180423T0929-djordje@datamap.io-F068A314-DC4D-4FB0-9D97-E79929993E41.csv\", col_array)\n",
    "bus2 = read_values(\"Bus-20180423T2150-djordje@datamap.io-DEB56810-DB1E-4E14-A144-130B52E0F405.csv\", col_array)\n",
    "car1 = read_values(\"Car-20180420T0000-djordje@datamap.io-B2876225-A37C-4A72-B045-790E5437B114.csv\", col_array)\n",
    "car2 = read_values(\"Car-20180420T0912-djordje@datamap.io-37B35625-AAF7-4217-9535-38D6BCF3FF93.csv\", col_array)\n",
    "car3 = read_values(\"Car-20180420T1328-roger@datamap.io-A3DB32B6-A6AC-4BD9-88EF-8AFFCFCCEDC1.csv\", col_array)\n",
    "flat1 = read_values(\"Other-Flat-20180424T1445-roger@datamap.io-1A6D2916-7C62-48D4-8CB9-9CBBA9FD7E20.csv\", col_array)\n",
    "flat2 = read_values(\"Other-flat-reversed-20180424T1538-roger@datamap.io-7419A222-F45B-415A-A3DF-08691E85B99A.csv\", col_array)\n",
    "standing = read_values(\"Other-InPocket-20180424T1446-roger@datamap.io-6DA03A16-DE1F-4508-BFC8-E8403F621954.csv\", col_array)\n",
    "scooter = read_values(\"Other-Scooter-20180422T2049-roger@datamap.io-0C33D54C-1B7B-4C45-92D3-2F89A7A82C16.csv\", col_array)\n",
    "tram1 = read_values(\"Tram-20180426T1825-roger@datamap.io-7C72A146-5733-446A-8418-5126D1BB74DE.csv\", col_array)\n",
    "tram2 = read_values(\"Tram-20180427T1806-roger@datamap.io-9D3D94ED-4E6B-4382-87A0-0F9A3BA4AA82.csv\", col_array)\n",
    "train = read_values(\"Train-20180420T1731-roger@datamap.io-E15B0F2F-910B-49CC-A96D-D54276492C97.csv\", col_array)\n",
    "walk1 = read_values(\"Walk-20180418T1004-roger@datamap.io-D7CA4A5B-0BDC-4E28-9BF4-3DD46B6F91AC.csv\", col_array)\n",
    "walk2 = read_values(\"Walk-20180418T1507-roger@datamap.io-F3AD9D9D-6B43-4AD1-885B-EAFF3E531A25.csv\", col_array)\n",
    "walk3 = read_values(\"Walk-20180418T1753-roger@datamap.io-AE01B243-595E-4F19-BAE3-8068038A1AE0.csv\", col_array)\n",
    "walk4 = read_values(\"Walk-20180419T0011-roger@datamap.io-50A0AB4B-4DB1-4231-8BA6-179DF3EFA381.csv\", col_array)\n",
    "walk5 = read_values(\"Walk-20180419T1025-roger@datamap.io-9F5D6118-143B-4625-8CD2-A94C5E7E62A0.csv\", col_array)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'Bike': 0, 'Bus': 1, 'Car': 2, 'Tram': 3, 'Train': 4, 'Walk': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df, f_train, l_train, f_test, l_test, seq_len=250):\n",
    "    df_np = df.as_matrix()\n",
    "    df_np = df_np[:(df_np.shape[0] - df_np.shape[0] % seq_len), :]\n",
    "    df_np = df_np.reshape(int(df_np.shape[0]/seq_len), seq_len, df_np.shape[1])\n",
    "    for i in range(df_np.shape[0]):\n",
    "        label = df_np[i, 0, 0]\n",
    "        feature = np.expand_dims(df_np[i, :, 1:10], axis=0)\n",
    "        if i % 10 == 0:\n",
    "            l_test = np.append(l_test, label)\n",
    "            f_test = np.concatenate([f_test, feature], axis=0)\n",
    "        else:\n",
    "            l_train = np.append(l_train, label)\n",
    "            f_train = np.concatenate([f_train, feature], axis=0)\n",
    "    #feature = df_np[:, :, 1:18]\n",
    "    #len = feature.shape[0]\n",
    "    #n = int((len - len % 10) / 10)\n",
    "    #feature_test = df_np[:n, :, 1:18]\n",
    "    #feature_train = df_np[n:, :, 1:18]\n",
    "    #f_test = np.concatenate([f_test, feature_test], axis=0)\n",
    "    #f_train = np.concatenate([f_train, feature_train], axis=0)\n",
    "    return f_train, l_train, f_test, l_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_shapes():\n",
    "    return f_train.shape, l_train.shape, f_test.shape, l_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = np.empty([1, 250, 9])\n",
    "l_train = np.array([])\n",
    "f_test = np.empty([1, 250, 9])\n",
    "l_test = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    df = read_values(file, col_array, path=\"\")\n",
    "    f_train, l_train, f_test, l_test = feature_extraction(df, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "f_train, l_train, f_test, l_test = feature_extraction(bike1, f_train, l_train, f_test, l_test)\n",
    "f_train, l_train, f_test, l_test = feature_extraction(bike2, f_train, l_train, f_test, l_test)\n",
    "f_train, l_train, f_test, l_test = feature_extraction(bike3, f_train, l_train, f_test, l_test)\n",
    "f_train, l_train, f_test, l_test = feature_extraction(bike4, f_train, l_train, f_test, l_test)\n",
    "f_train, l_train, f_test, l_test = feature_extraction(bike5, f_train, l_train, f_test, l_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = f_train[1:, :, :]\n",
    "f_test = f_test[1:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20537, 250, 9), (20537,), (2358, 250, 9), (2358,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_train, l_train, f_test, l_test = feature_extraction(bus1, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(bus2, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((573, 250, 9), (573,), (68, 250, 9), (68,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_train, l_train, f_test, l_test = feature_extraction(car1, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(car2, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(car3, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1070, 250, 9), (1070,), (124, 250, 9), (124,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_train, l_train, f_test, l_test = feature_extraction(tram1, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(tram2, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1165, 250, 9), (1165,), (136, 250, 9), (136,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_train, l_train, f_test, l_test = feature_extraction(train, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1260, 250, 9), (1260,), (147, 250, 9), (147,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_train, l_train, f_test, l_test = feature_extraction(walk1, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(walk2, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(walk3, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(walk4, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(walk5, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2202, 250, 9), (2202,), (254, 250, 9), (254,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_train = tf.one_hot(l_train, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test = tf.one_hot(l_test, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1-Loss:    51.6942\n",
      "Epoch  2-Loss:    49.0757\n",
      "Epoch  3-Loss:    45.6509\n",
      "Epoch  4-Loss:    43.7992\n",
      "Epoch  5-Loss:    41.6886\n",
      "Epoch  6-Loss:    40.1330\n",
      "Epoch  7-Loss:    38.6049\n",
      "Epoch  8-Loss:    36.9378\n",
      "Epoch  9-Loss:    36.0681\n",
      "Epoch 10-Loss:    34.8903\n",
      "Epoch 11-Loss:    33.6272\n",
      "Epoch 12-Loss:    32.5347\n",
      "Epoch 13-Loss:    31.6643\n",
      "Epoch 14-Loss:    30.8735\n",
      "Epoch 15-Loss:    29.7632\n",
      "Epoch 16-Loss:    28.9796\n",
      "Epoch 17-Loss:    28.3103\n",
      "Epoch 18-Loss:    27.6892\n",
      "Epoch 19-Loss:    27.1041\n",
      "Epoch 20-Loss:    26.1096\n",
      "Epoch 21-Loss:    25.5710\n",
      "Epoch 22-Loss:    24.9325\n",
      "Epoch 23-Loss:    24.4280\n",
      "Epoch 24-Loss:    24.0572\n",
      "Epoch 25-Loss:    23.4647\n",
      "Epoch 26-Loss:    22.8892\n",
      "Epoch 27-Loss:    22.0913\n",
      "Epoch 28-Loss:    21.5627\n",
      "Epoch 29-Loss:    21.0387\n",
      "Epoch 30-Loss:    20.4674\n",
      "Epoch 31-Loss:    20.1006\n",
      "Epoch 32-Loss:    19.5421\n",
      "Epoch 33-Loss:    19.0405\n",
      "Epoch 34-Loss:    18.6748\n",
      "Epoch 35-Loss:    18.3499\n",
      "Epoch 36-Loss:    18.1861\n",
      "Epoch 37-Loss:    17.8631\n",
      "Epoch 38-Loss:    17.3691\n",
      "Epoch 39-Loss:    16.8481\n",
      "Epoch 40-Loss:    16.3268\n",
      "Epoch 41-Loss:    15.8095\n",
      "Epoch 42-Loss:    15.5132\n",
      "Epoch 43-Loss:    15.1284\n",
      "Epoch 44-Loss:    14.7742\n",
      "Epoch 45-Loss:    14.4962\n",
      "Epoch 46-Loss:    14.1101\n",
      "Epoch 47-Loss:    13.7900\n",
      "Epoch 48-Loss:    13.2785\n",
      "Epoch 49-Loss:    12.9651\n",
      "Epoch 50-Loss:    12.7931\n",
      "Epoch 51-Loss:    12.5142\n",
      "Epoch 52-Loss:    12.2004\n",
      "Epoch 53-Loss:    11.8635\n",
      "Epoch 54-Loss:    11.8832\n",
      "Epoch 55-Loss:    11.5929\n",
      "Epoch 56-Loss:    11.3447\n",
      "Epoch 57-Loss:    11.0725\n",
      "Epoch 58-Loss:    10.8430\n",
      "Epoch 59-Loss:    10.7536\n",
      "Epoch 60-Loss:    10.4721\n",
      "Epoch 61-Loss:    10.3388\n",
      "Epoch 62-Loss:    10.2526\n",
      "Epoch 63-Loss:    10.1612\n",
      "Epoch 64-Loss:     9.9682\n",
      "Epoch 65-Loss:     9.8031\n",
      "Epoch 66-Loss:     9.6946\n",
      "Epoch 67-Loss:     9.4918\n",
      "Epoch 68-Loss:     9.3498\n",
      "Epoch 69-Loss:     9.2452\n",
      "Epoch 70-Loss:     9.0298\n",
      "Epoch 71-Loss:     8.8617\n",
      "Epoch 72-Loss:     8.8226\n",
      "Epoch 73-Loss:     8.8657\n",
      "Epoch 74-Loss:     8.8310\n",
      "Epoch 75-Loss:     8.8262\n",
      "Epoch 76-Loss:     8.8407\n",
      "Epoch 77-Loss:     8.9353\n",
      "Epoch 78-Loss:     8.9256\n",
      "Epoch 79-Loss:     8.9107\n",
      "Epoch 80-Loss:     8.8662\n",
      "Epoch 81-Loss:     8.6681\n",
      "Epoch 82-Loss:     8.5784\n",
      "Epoch 83-Loss:     8.5098\n",
      "Epoch 84-Loss:     8.5433\n",
      "Epoch 85-Loss:     8.3103\n",
      "Epoch 86-Loss:     8.2481\n",
      "Epoch 87-Loss:     8.2887\n",
      "Epoch 88-Loss:     8.3613\n",
      "Epoch 89-Loss:     8.3512\n",
      "Epoch 90-Loss:     8.2966\n",
      "Epoch 91-Loss:     8.0681\n",
      "Epoch 92-Loss:     7.9363\n",
      "Epoch 93-Loss:     7.8811\n",
      "Epoch 94-Loss:     7.8619\n",
      "Epoch 95-Loss:     7.9160\n",
      "Epoch 96-Loss:     7.8565\n",
      "Epoch 97-Loss:     8.0155\n",
      "Epoch 98-Loss:     7.9379\n",
      "Epoch 99-Loss:     8.0663\n",
      "Epoch 100-Loss:     7.8953\n",
      "Epoch 101-Loss:     7.8560\n",
      "Epoch 102-Loss:     7.7686\n",
      "Epoch 103-Loss:     7.7228\n",
      "Epoch 104-Loss:     7.7315\n",
      "Epoch 105-Loss:     7.6365\n",
      "Epoch 106-Loss:     7.5753\n",
      "Epoch 107-Loss:     7.6214\n",
      "Epoch 108-Loss:     7.4612\n",
      "Epoch 109-Loss:     7.4633\n",
      "Epoch 110-Loss:     7.4834\n",
      "Epoch 111-Loss:     7.4707\n",
      "Epoch 112-Loss:     7.3698\n",
      "Epoch 113-Loss:     7.4506\n",
      "Epoch 114-Loss:     7.3455\n",
      "Epoch 115-Loss:     7.3435\n",
      "Epoch 116-Loss:     7.5134\n",
      "Epoch 117-Loss:     7.4618\n",
      "Epoch 118-Loss:     7.3995\n",
      "Epoch 119-Loss:     7.3387\n",
      "Epoch 120-Loss:     7.3266\n",
      "Epoch 121-Loss:     7.3082\n",
      "Epoch 122-Loss:     7.3571\n",
      "Epoch 123-Loss:     7.2110\n",
      "Epoch 124-Loss:     7.2220\n",
      "Epoch 125-Loss:     7.2613\n",
      "Epoch 126-Loss:     7.1776\n",
      "Epoch 127-Loss:     7.1793\n",
      "Epoch 128-Loss:     7.2493\n",
      "Epoch 129-Loss:     7.2976\n",
      "Epoch 130-Loss:     7.3206\n",
      "Epoch 131-Loss:     7.2298\n",
      "Epoch 132-Loss:     7.3715\n",
      "Epoch 133-Loss:     7.4253\n",
      "Epoch 134-Loss:     7.3468\n",
      "Epoch 135-Loss:     7.4985\n",
      "Epoch 136-Loss:     7.3975\n",
      "Epoch 137-Loss:     7.3733\n",
      "Epoch 138-Loss:     7.4267\n",
      "Epoch 139-Loss:     7.2858\n",
      "Epoch 140-Loss:     7.3254\n",
      "Epoch 141-Loss:     7.3070\n",
      "Epoch 142-Loss:     7.2796\n",
      "Epoch 143-Loss:     7.1548\n",
      "Epoch 144-Loss:     7.2231\n",
      "Epoch 145-Loss:     7.2939\n",
      "Epoch 146-Loss:     7.3176\n",
      "Epoch 147-Loss:     7.1444\n",
      "Epoch 148-Loss:     7.1471\n",
      "Epoch 149-Loss:     7.1728\n",
      "Epoch 150-Loss:     7.1175\n",
      "Epoch 151-Loss:     7.1618\n",
      "Epoch 152-Loss:     7.1245\n",
      "Epoch 153-Loss:     7.1244\n",
      "Epoch 154-Loss:     7.1448\n",
      "Epoch 155-Loss:     7.1186\n",
      "Epoch 156-Loss:     6.9944\n",
      "Epoch 157-Loss:     6.9318\n",
      "Epoch 158-Loss:     6.9278\n",
      "Epoch 159-Loss:     7.0572\n",
      "Epoch 160-Loss:     6.9822\n",
      "Epoch 161-Loss:     6.8034\n",
      "Epoch 162-Loss:     6.7740\n",
      "Epoch 163-Loss:     6.8319\n",
      "Epoch 164-Loss:     6.7744\n",
      "Epoch 165-Loss:     6.7419\n",
      "Epoch 166-Loss:     6.6586\n",
      "Epoch 167-Loss:     6.5470\n",
      "Epoch 168-Loss:     6.5027\n",
      "Epoch 169-Loss:     6.5227\n",
      "Epoch 170-Loss:     6.6418\n",
      "Epoch 171-Loss:     6.6506\n",
      "Epoch 172-Loss:     6.5989\n",
      "Epoch 173-Loss:     6.6877\n",
      "Epoch 174-Loss:     6.6505\n",
      "Epoch 175-Loss:     6.5952\n",
      "Epoch 176-Loss:     6.5629\n",
      "Epoch 177-Loss:     6.5577\n",
      "Epoch 178-Loss:     6.5053\n",
      "Epoch 179-Loss:     6.5356\n",
      "Epoch 180-Loss:     6.5100\n",
      "Epoch 181-Loss:     6.4668\n",
      "Epoch 182-Loss:     6.4673\n",
      "Epoch 183-Loss:     6.4868\n",
      "Epoch 184-Loss:     6.5309\n",
      "Epoch 185-Loss:     6.5488\n",
      "Epoch 186-Loss:     6.5449\n",
      "Epoch 187-Loss:     6.5344\n",
      "Epoch 188-Loss:     6.5368\n",
      "Epoch 189-Loss:     6.4966\n",
      "Epoch 190-Loss:     6.5864\n",
      "Epoch 191-Loss:     6.4196\n",
      "Epoch 192-Loss:     6.3113\n",
      "Epoch 193-Loss:     6.1961\n",
      "Epoch 194-Loss:     6.2638\n",
      "Epoch 195-Loss:     6.2571\n",
      "Epoch 196-Loss:     6.2458\n",
      "Epoch 197-Loss:     6.1841\n",
      "Epoch 198-Loss:     6.1659\n",
      "Epoch 199-Loss:     6.1551\n",
      "Epoch 200-Loss:     6.1429\n",
      "Epoch 201-Loss:     6.2271\n",
      "Epoch 202-Loss:     6.1491\n",
      "Epoch 203-Loss:     6.2386\n",
      "Epoch 204-Loss:     6.2061\n",
      "Epoch 205-Loss:     6.2508\n",
      "Epoch 206-Loss:     6.0787\n",
      "Epoch 207-Loss:     6.1819\n",
      "Epoch 208-Loss:     6.1142\n",
      "Epoch 209-Loss:     6.1303\n",
      "Epoch 210-Loss:     6.2099\n",
      "Epoch 211-Loss:     6.0596\n",
      "Epoch 212-Loss:     6.0144\n",
      "Epoch 213-Loss:     5.9912\n",
      "Epoch 214-Loss:     5.9136\n",
      "Epoch 215-Loss:     5.9521\n",
      "Epoch 216-Loss:     6.0272\n",
      "Epoch 217-Loss:     6.1117\n",
      "Epoch 218-Loss:     6.2988\n",
      "Epoch 219-Loss:     6.1229\n",
      "Epoch 220-Loss:     5.9898\n",
      "Epoch 221-Loss:     6.0933\n",
      "Epoch 222-Loss:     6.0800\n",
      "Epoch 223-Loss:     6.0123\n",
      "Epoch 224-Loss:     6.0485\n",
      "Epoch 225-Loss:     6.0285\n",
      "Epoch 226-Loss:     5.9627\n",
      "Epoch 227-Loss:     5.9305\n",
      "Epoch 228-Loss:     5.9592\n",
      "Epoch 229-Loss:     5.9507\n",
      "Epoch 230-Loss:     5.9317\n",
      "Epoch 231-Loss:     5.8319\n",
      "Epoch 232-Loss:     5.8879\n",
      "Epoch 233-Loss:     6.0485\n",
      "Epoch 234-Loss:     5.9692\n",
      "Epoch 235-Loss:     5.8915\n",
      "Epoch 236-Loss:     5.8469\n",
      "Epoch 237-Loss:     5.8816\n",
      "Epoch 238-Loss:     5.9336\n",
      "Epoch 239-Loss:     5.9400\n",
      "Epoch 240-Loss:     5.9266\n",
      "Epoch 241-Loss:     5.9276\n",
      "Epoch 242-Loss:     5.9095\n",
      "Epoch 243-Loss:     5.8644\n",
      "Epoch 244-Loss:     5.9013\n",
      "Epoch 245-Loss:     5.6821\n",
      "Epoch 246-Loss:     5.6727\n",
      "Epoch 247-Loss:     5.6840\n",
      "Epoch 248-Loss:     5.6697\n",
      "Epoch 249-Loss:     5.6675\n",
      "Epoch 250-Loss:     5.6823\n",
      "Epoch 251-Loss:     5.6551\n",
      "Epoch 252-Loss:     5.5443\n",
      "Epoch 253-Loss:     5.5516\n",
      "Epoch 254-Loss:     5.4931\n",
      "Epoch 255-Loss:     5.4366\n",
      "Epoch 256-Loss:     5.4393\n",
      "Epoch 257-Loss:     5.4939\n",
      "Epoch 258-Loss:     5.5303\n",
      "Epoch 259-Loss:     5.5630\n",
      "Epoch 260-Loss:     5.6170\n",
      "Epoch 261-Loss:     5.5515\n",
      "Epoch 262-Loss:     5.5219\n",
      "Epoch 263-Loss:     5.5986\n",
      "Epoch 264-Loss:     5.6346\n",
      "Epoch 265-Loss:     5.6440\n",
      "Epoch 266-Loss:     5.6667\n",
      "Epoch 267-Loss:     5.7463\n",
      "Epoch 268-Loss:     5.7611\n",
      "Epoch 269-Loss:     5.6783\n",
      "Epoch 270-Loss:     5.7430\n",
      "Epoch 271-Loss:     5.8185\n",
      "Epoch 272-Loss:     5.7041\n",
      "Epoch 273-Loss:     5.6056\n",
      "Epoch 274-Loss:     5.6254\n",
      "Epoch 275-Loss:     5.6385\n",
      "Epoch 276-Loss:     5.6143\n",
      "Epoch 277-Loss:     5.5498\n",
      "Epoch 278-Loss:     5.4907\n",
      "Epoch 279-Loss:     5.4486\n",
      "Epoch 280-Loss:     5.4395\n",
      "Epoch 281-Loss:     5.5453\n",
      "Epoch 282-Loss:     5.4980\n",
      "Epoch 283-Loss:     5.4802\n",
      "Epoch 284-Loss:     5.5422\n",
      "Epoch 285-Loss:     5.4469\n",
      "Epoch 286-Loss:     5.4743\n",
      "Epoch 287-Loss:     5.4252\n",
      "Epoch 288-Loss:     5.4901\n",
      "Epoch 289-Loss:     5.2988\n",
      "Epoch 290-Loss:     5.4499\n",
      "Epoch 291-Loss:     5.3626\n",
      "Epoch 292-Loss:     5.4557\n",
      "Epoch 293-Loss:     5.4136\n",
      "Epoch 294-Loss:     5.3830\n",
      "Epoch 295-Loss:     5.3903\n",
      "Epoch 296-Loss:     5.3456\n",
      "Epoch 297-Loss:     5.1278\n",
      "Epoch 298-Loss:     5.1969\n",
      "Epoch 299-Loss:     5.1959\n",
      "Epoch 300-Loss:     5.3061\n",
      "Epoch 301-Loss:     5.3361\n",
      "Epoch 302-Loss:     5.3403\n",
      "Epoch 303-Loss:     5.3718\n",
      "Epoch 304-Loss:     5.7532\n",
      "Epoch 305-Loss:     5.9403\n",
      "Epoch 306-Loss:     5.7194\n",
      "Epoch 307-Loss:     5.3738\n",
      "Epoch 308-Loss:     5.3410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309-Loss:     5.3945\n",
      "Epoch 310-Loss:     5.3098\n",
      "Epoch 311-Loss:     5.1254\n",
      "Epoch 312-Loss:     5.1359\n",
      "Epoch 313-Loss:     4.9295\n",
      "Epoch 314-Loss:     5.0106\n",
      "Epoch 315-Loss:     5.0940\n",
      "Epoch 316-Loss:     5.0966\n",
      "Epoch 317-Loss:     5.0661\n",
      "Epoch 318-Loss:     5.0627\n",
      "Epoch 319-Loss:     5.1066\n",
      "Epoch 320-Loss:     5.1820\n",
      "Epoch 321-Loss:     5.3262\n",
      "Epoch 322-Loss:     5.1485\n",
      "Epoch 323-Loss:     5.1459\n",
      "Epoch 324-Loss:     5.1968\n",
      "Epoch 325-Loss:     5.1740\n",
      "Epoch 326-Loss:     5.2237\n",
      "Epoch 327-Loss:     5.0686\n",
      "Epoch 328-Loss:     5.0118\n",
      "Epoch 329-Loss:     4.9419\n",
      "Epoch 330-Loss:     5.0130\n",
      "Epoch 331-Loss:     5.1008\n",
      "Epoch 332-Loss:     5.0885\n",
      "Epoch 333-Loss:     5.1480\n",
      "Epoch 334-Loss:     5.2188\n",
      "Epoch 335-Loss:     5.1138\n",
      "Epoch 336-Loss:     5.0950\n",
      "Epoch 337-Loss:     5.0395\n",
      "Epoch 338-Loss:     4.9802\n",
      "Epoch 339-Loss:     5.0155\n",
      "Epoch 340-Loss:     5.1318\n",
      "Epoch 341-Loss:     4.9975\n",
      "Epoch 342-Loss:     5.0506\n",
      "Epoch 343-Loss:     4.9556\n",
      "Epoch 344-Loss:     4.8846\n",
      "Epoch 345-Loss:     4.8794\n",
      "Epoch 346-Loss:     4.9079\n",
      "Epoch 347-Loss:     4.8435\n",
      "Epoch 348-Loss:     4.8567\n",
      "Epoch 349-Loss:     4.8795\n",
      "Epoch 350-Loss:     5.0936\n",
      "Epoch 351-Loss:     4.9996\n",
      "Epoch 352-Loss:     4.9945\n",
      "Epoch 353-Loss:     4.8483\n",
      "Epoch 354-Loss:     4.7554\n",
      "Epoch 355-Loss:     4.7377\n",
      "Epoch 356-Loss:     4.7869\n",
      "Epoch 357-Loss:     4.8552\n",
      "Epoch 358-Loss:     4.7383\n",
      "Epoch 359-Loss:     4.8550\n",
      "Epoch 360-Loss:     4.7871\n",
      "Epoch 361-Loss:     4.7385\n",
      "Epoch 362-Loss:     4.6584\n",
      "Epoch 363-Loss:     4.6741\n",
      "Epoch 364-Loss:     4.7085\n",
      "Epoch 365-Loss:     4.7884\n",
      "Epoch 366-Loss:     4.6980\n",
      "Epoch 367-Loss:     4.6444\n",
      "Epoch 368-Loss:     4.6553\n",
      "Epoch 369-Loss:     4.8014\n",
      "Epoch 370-Loss:     4.7929\n",
      "Epoch 371-Loss:     4.8094\n",
      "Epoch 372-Loss:     5.0099\n",
      "Epoch 373-Loss:     4.9178\n",
      "Epoch 374-Loss:     4.7763\n",
      "Epoch 375-Loss:     4.8417\n",
      "Epoch 376-Loss:     4.8837\n",
      "Epoch 377-Loss:     4.7911\n",
      "Epoch 378-Loss:     4.7651\n",
      "Epoch 379-Loss:     4.7730\n",
      "Epoch 380-Loss:     4.8938\n",
      "Epoch 381-Loss:     4.8303\n",
      "Epoch 382-Loss:     4.9268\n",
      "Epoch 383-Loss:     4.8274\n",
      "Epoch 384-Loss:     4.8839\n",
      "Epoch 385-Loss:     4.8076\n",
      "Epoch 386-Loss:     4.7967\n",
      "Epoch 387-Loss:     4.6917\n",
      "Epoch 388-Loss:     4.7676\n",
      "Epoch 389-Loss:     4.7916\n",
      "Epoch 390-Loss:     4.6832\n",
      "Epoch 391-Loss:     4.7101\n",
      "Epoch 392-Loss:     4.8097\n",
      "Epoch 393-Loss:     4.7896\n",
      "Epoch 394-Loss:     4.6504\n",
      "Epoch 395-Loss:     4.6315\n",
      "Epoch 396-Loss:     4.4243\n",
      "Epoch 397-Loss:     4.4212\n",
      "Epoch 398-Loss:     4.3942\n",
      "Epoch 399-Loss:     4.3799\n",
      "Epoch 400-Loss:     4.4098\n",
      "Epoch 401-Loss:     4.4392\n",
      "Epoch 402-Loss:     4.3432\n",
      "Epoch 403-Loss:     4.3251\n",
      "Epoch 404-Loss:     4.3898\n",
      "Epoch 405-Loss:     4.4348\n",
      "Epoch 406-Loss:     4.4791\n",
      "Epoch 407-Loss:     4.5032\n",
      "Epoch 408-Loss:     4.5724\n",
      "Epoch 409-Loss:     4.5218\n",
      "Epoch 410-Loss:     4.5151\n",
      "Epoch 411-Loss:     4.5870\n",
      "Epoch 412-Loss:     4.6626\n",
      "Epoch 413-Loss:     4.6327\n",
      "Epoch 414-Loss:     4.5164\n",
      "Epoch 415-Loss:     4.5097\n",
      "Epoch 416-Loss:     4.4564\n",
      "Epoch 417-Loss:     4.3991\n",
      "Epoch 418-Loss:     4.3780\n",
      "Epoch 419-Loss:     4.3787\n",
      "Epoch 420-Loss:     4.3909\n",
      "Epoch 421-Loss:     4.3743\n",
      "Epoch 422-Loss:     4.2750\n",
      "Epoch 423-Loss:     4.2577\n",
      "Epoch 424-Loss:     4.2980\n",
      "Epoch 425-Loss:     4.2625\n",
      "Epoch 426-Loss:     4.5427\n",
      "Epoch 427-Loss:     4.4701\n",
      "Epoch 428-Loss:     4.4628\n",
      "Epoch 429-Loss:     4.4415\n",
      "Epoch 430-Loss:     4.4970\n",
      "Epoch 431-Loss:     4.4573\n",
      "Epoch 432-Loss:     4.5734\n",
      "Epoch 433-Loss:     4.3836\n",
      "Epoch 434-Loss:     4.5298\n",
      "Epoch 435-Loss:     4.6394\n",
      "Epoch 436-Loss:     4.4884\n",
      "Epoch 437-Loss:     4.5011\n",
      "Epoch 438-Loss:     4.3398\n",
      "Epoch 439-Loss:     4.3692\n",
      "Epoch 440-Loss:     4.3832\n",
      "Epoch 441-Loss:     4.3942\n",
      "Epoch 442-Loss:     4.2516\n",
      "Epoch 443-Loss:     4.2873\n",
      "Epoch 444-Loss:     4.3543\n",
      "Epoch 445-Loss:     4.2631\n",
      "Epoch 446-Loss:     4.2520\n",
      "Epoch 447-Loss:     4.2891\n",
      "Epoch 448-Loss:     4.3922\n",
      "Epoch 449-Loss:     4.3115\n",
      "Epoch 450-Loss:     4.4306\n",
      "Epoch 451-Loss:     4.4162\n",
      "Epoch 452-Loss:     4.3380\n",
      "Epoch 453-Loss:     4.4487\n",
      "Epoch 454-Loss:     4.3992\n",
      "Epoch 455-Loss:     4.3968\n",
      "Epoch 456-Loss:     4.4474\n",
      "Epoch 457-Loss:     4.4466\n",
      "Epoch 458-Loss:     4.4577\n",
      "Epoch 459-Loss:     4.4117\n",
      "Epoch 460-Loss:     4.4553\n",
      "Epoch 461-Loss:     4.5148\n",
      "Epoch 462-Loss:     4.3226\n",
      "Epoch 463-Loss:     4.3242\n",
      "Epoch 464-Loss:     4.5330\n",
      "Epoch 465-Loss:     4.4143\n",
      "Epoch 466-Loss:     4.5799\n",
      "Epoch 467-Loss:     4.4855\n",
      "Epoch 468-Loss:     4.4634\n",
      "Epoch 469-Loss:     4.2684\n",
      "Epoch 470-Loss:     4.2848\n",
      "Epoch 471-Loss:     4.3188\n",
      "Epoch 472-Loss:     4.2951\n",
      "Epoch 473-Loss:     4.1670\n",
      "Epoch 474-Loss:     4.1778\n",
      "Epoch 475-Loss:     4.1351\n",
      "Epoch 476-Loss:     4.2418\n",
      "Epoch 477-Loss:     4.2975\n",
      "Epoch 478-Loss:     4.2344\n",
      "Epoch 479-Loss:     4.2496\n",
      "Epoch 480-Loss:     4.3081\n",
      "Epoch 481-Loss:     4.1727\n",
      "Epoch 482-Loss:     4.2725\n",
      "Epoch 483-Loss:     4.3846\n",
      "Epoch 484-Loss:     4.2870\n",
      "Epoch 485-Loss:     4.5464\n",
      "Epoch 486-Loss:     4.5072\n",
      "Epoch 487-Loss:     4.3906\n",
      "Epoch 488-Loss:     4.4256\n",
      "Epoch 489-Loss:     4.6018\n",
      "Epoch 490-Loss:     4.6129\n",
      "Epoch 491-Loss:     4.4948\n",
      "Epoch 492-Loss:     4.2614\n",
      "Epoch 493-Loss:     4.1236\n",
      "Epoch 494-Loss:     4.2305\n",
      "Epoch 495-Loss:     4.4171\n",
      "Epoch 496-Loss:     4.2361\n",
      "Epoch 497-Loss:     4.6236\n",
      "Epoch 498-Loss:     4.4105\n",
      "Epoch 499-Loss:     4.1517\n",
      "Epoch 500-Loss:     4.0258\n",
      "Epoch 501-Loss:     4.0550\n",
      "Epoch 502-Loss:     4.0630\n",
      "Epoch 503-Loss:     3.9474\n",
      "Epoch 504-Loss:     3.9078\n",
      "Epoch 505-Loss:     3.8963\n",
      "Epoch 506-Loss:     3.8742\n",
      "Epoch 507-Loss:     3.9723\n",
      "Epoch 508-Loss:     4.0584\n",
      "Epoch 509-Loss:     3.9910\n",
      "Epoch 510-Loss:     4.0116\n",
      "Epoch 511-Loss:     4.0073\n",
      "Epoch 512-Loss:     4.0835\n",
      "Epoch 513-Loss:     4.0329\n",
      "Epoch 514-Loss:     3.9028\n",
      "Epoch 515-Loss:     3.9483\n",
      "Epoch 516-Loss:     3.9887\n",
      "Epoch 517-Loss:     3.9613\n",
      "Epoch 518-Loss:     3.9931\n",
      "Epoch 519-Loss:     4.0113\n",
      "Epoch 520-Loss:     3.9769\n",
      "Epoch 521-Loss:     3.9671\n",
      "Epoch 522-Loss:     4.0026\n",
      "Epoch 523-Loss:     3.9651\n",
      "Epoch 524-Loss:     3.9868\n",
      "Epoch 525-Loss:     3.9517\n",
      "Epoch 526-Loss:     3.8593\n",
      "Epoch 527-Loss:     3.8678\n",
      "Epoch 528-Loss:     4.0324\n",
      "Epoch 529-Loss:     4.1392\n",
      "Epoch 530-Loss:     4.1594\n",
      "Epoch 531-Loss:     3.9785\n",
      "Epoch 532-Loss:     3.9822\n",
      "Epoch 533-Loss:     3.7548\n",
      "Epoch 534-Loss:     3.7970\n",
      "Epoch 535-Loss:     3.7630\n",
      "Epoch 536-Loss:     3.8017\n",
      "Epoch 537-Loss:     3.8096\n",
      "Epoch 538-Loss:     3.7629\n",
      "Epoch 539-Loss:     3.7622\n",
      "Epoch 540-Loss:     3.9577\n",
      "Epoch 541-Loss:     3.9816\n",
      "Epoch 542-Loss:     4.1263\n",
      "Epoch 543-Loss:     4.0122\n",
      "Epoch 544-Loss:     4.1559\n",
      "Epoch 545-Loss:     4.2434\n",
      "Epoch 546-Loss:     4.3307\n",
      "Epoch 547-Loss:     4.5167\n",
      "Epoch 548-Loss:     4.1905\n",
      "Epoch 549-Loss:     4.3234\n",
      "Epoch 550-Loss:     4.3468\n",
      "Epoch 551-Loss:     4.4597\n",
      "Epoch 552-Loss:     4.2163\n",
      "Epoch 553-Loss:     4.1175\n",
      "Epoch 554-Loss:     3.8471\n",
      "Epoch 555-Loss:     3.9170\n",
      "Epoch 556-Loss:     3.9143\n",
      "Epoch 557-Loss:     3.8679\n",
      "Epoch 558-Loss:     3.8470\n",
      "Epoch 559-Loss:     3.7123\n",
      "Epoch 560-Loss:     3.7206\n",
      "Epoch 561-Loss:     3.7160\n",
      "Epoch 562-Loss:     3.8196\n",
      "Epoch 563-Loss:     3.9664\n",
      "Epoch 564-Loss:     3.8645\n",
      "Epoch 565-Loss:     3.9983\n",
      "Epoch 566-Loss:     4.0796\n",
      "Epoch 567-Loss:     4.1742\n",
      "Epoch 568-Loss:     4.2237\n",
      "Epoch 569-Loss:     3.9713\n",
      "Epoch 570-Loss:     3.9793\n",
      "Epoch 571-Loss:     3.9643\n",
      "Epoch 572-Loss:     3.8823\n",
      "Epoch 573-Loss:     3.7874\n",
      "Epoch 574-Loss:     3.7121\n",
      "Epoch 575-Loss:     3.7970\n",
      "Epoch 576-Loss:     3.7832\n",
      "Epoch 577-Loss:     3.6965\n",
      "Epoch 578-Loss:     3.8474\n",
      "Epoch 579-Loss:     3.5727\n",
      "Epoch 580-Loss:     3.6292\n",
      "Epoch 581-Loss:     3.6636\n",
      "Epoch 582-Loss:     3.5589\n",
      "Epoch 583-Loss:     3.6763\n",
      "Epoch 584-Loss:     3.6277\n",
      "Epoch 585-Loss:     3.6549\n",
      "Epoch 586-Loss:     3.6272\n",
      "Epoch 587-Loss:     3.4638\n",
      "Epoch 588-Loss:     3.4863\n",
      "Epoch 589-Loss:     3.4380\n",
      "Epoch 590-Loss:     3.4085\n",
      "Epoch 591-Loss:     3.3672\n",
      "Epoch 592-Loss:     3.4198\n",
      "Epoch 593-Loss:     3.3952\n",
      "Epoch 594-Loss:     3.3647\n",
      "Epoch 595-Loss:     3.4457\n",
      "Epoch 596-Loss:     3.3494\n",
      "Epoch 597-Loss:     3.3787\n",
      "Epoch 598-Loss:     3.3816\n",
      "Epoch 599-Loss:     3.3313\n",
      "Epoch 600-Loss:     3.3631\n",
      "Epoch 601-Loss:     3.4865\n",
      "Epoch 602-Loss:     3.4911\n",
      "Epoch 603-Loss:     3.6373\n",
      "Epoch 604-Loss:     3.5846\n",
      "Epoch 605-Loss:     3.6190\n",
      "Epoch 606-Loss:     3.7891\n",
      "Epoch 607-Loss:     3.8889\n",
      "Epoch 608-Loss:     3.8665\n",
      "Epoch 609-Loss:     3.7107\n",
      "Epoch 610-Loss:     3.7716\n",
      "Epoch 611-Loss:     3.9541\n",
      "Epoch 612-Loss:     3.8938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 613-Loss:     3.8297\n",
      "Epoch 614-Loss:     3.8308\n",
      "Epoch 615-Loss:     3.8407\n",
      "Epoch 616-Loss:     3.9325\n",
      "Epoch 617-Loss:     3.9167\n",
      "Epoch 618-Loss:     3.9317\n",
      "Epoch 619-Loss:     4.0252\n",
      "Epoch 620-Loss:     4.0741\n",
      "Epoch 621-Loss:     3.9644\n",
      "Epoch 622-Loss:     3.8352\n",
      "Epoch 623-Loss:     3.8374\n",
      "Epoch 624-Loss:     3.8509\n",
      "Epoch 625-Loss:     3.7992\n",
      "Epoch 626-Loss:     3.7982\n",
      "Epoch 627-Loss:     3.8626\n",
      "Epoch 628-Loss:     3.8407\n",
      "Epoch 629-Loss:     3.9816\n",
      "Epoch 630-Loss:     3.9566\n",
      "Epoch 631-Loss:     3.7830\n",
      "Epoch 632-Loss:     3.7751\n",
      "Epoch 633-Loss:     3.6638\n",
      "Epoch 634-Loss:     3.5456\n",
      "Epoch 635-Loss:     3.7014\n",
      "Epoch 636-Loss:     3.6171\n",
      "Epoch 637-Loss:     3.6112\n",
      "Epoch 638-Loss:     3.7063\n",
      "Epoch 639-Loss:     3.8303\n",
      "Epoch 640-Loss:     3.7802\n",
      "Epoch 641-Loss:     3.7949\n",
      "Epoch 642-Loss:     3.6569\n",
      "Epoch 643-Loss:     3.7329\n",
      "Epoch 644-Loss:     3.5308\n",
      "Epoch 645-Loss:     3.7505\n",
      "Epoch 646-Loss:     3.5085\n",
      "Epoch 647-Loss:     3.4377\n",
      "Epoch 648-Loss:     3.4978\n",
      "Epoch 649-Loss:     3.5031\n",
      "Epoch 650-Loss:     3.6292\n",
      "Epoch 651-Loss:     3.6415\n",
      "Epoch 652-Loss:     3.5452\n",
      "Epoch 653-Loss:     3.6518\n",
      "Epoch 654-Loss:     3.5609\n",
      "Epoch 655-Loss:     3.4980\n",
      "Epoch 656-Loss:     3.4665\n",
      "Epoch 657-Loss:     3.3671\n",
      "Epoch 658-Loss:     3.3126\n",
      "Epoch 659-Loss:     3.3070\n",
      "Epoch 660-Loss:     3.4113\n",
      "Epoch 661-Loss:     3.3570\n",
      "Epoch 662-Loss:     3.3640\n",
      "Epoch 663-Loss:     3.3715\n",
      "Epoch 664-Loss:     3.4038\n",
      "Epoch 665-Loss:     3.3969\n",
      "Epoch 666-Loss:     3.3719\n",
      "Epoch 667-Loss:     3.3467\n",
      "Epoch 668-Loss:     3.2840\n",
      "Epoch 669-Loss:     3.3361\n",
      "Epoch 670-Loss:     3.3422\n",
      "Epoch 671-Loss:     3.2980\n",
      "Epoch 672-Loss:     3.3610\n",
      "Epoch 673-Loss:     3.3039\n",
      "Epoch 674-Loss:     3.4329\n",
      "Epoch 675-Loss:     3.3241\n",
      "Epoch 676-Loss:     3.3458\n",
      "Epoch 677-Loss:     3.2990\n",
      "Epoch 678-Loss:     3.3496\n",
      "Epoch 679-Loss:     3.4070\n",
      "Epoch 680-Loss:     3.4471\n",
      "Epoch 681-Loss:     3.6256\n",
      "Epoch 682-Loss:     3.6848\n",
      "Epoch 683-Loss:     3.5675\n",
      "Epoch 684-Loss:     3.4785\n",
      "Epoch 685-Loss:     3.5197\n",
      "Epoch 686-Loss:     3.5965\n",
      "Epoch 687-Loss:     3.4834\n",
      "Epoch 688-Loss:     3.3879\n",
      "Epoch 689-Loss:     3.5023\n",
      "Epoch 690-Loss:     3.4872\n",
      "Epoch 691-Loss:     3.4748\n",
      "Epoch 692-Loss:     3.4282\n",
      "Epoch 693-Loss:     3.3465\n",
      "Epoch 694-Loss:     3.3954\n",
      "Epoch 695-Loss:     3.2589\n",
      "Epoch 696-Loss:     3.3563\n",
      "Epoch 697-Loss:     3.5417\n",
      "Epoch 698-Loss:     3.6755\n",
      "Epoch 699-Loss:     3.9413\n",
      "Epoch 700-Loss:     3.7299\n",
      "Epoch 701-Loss:     3.7004\n",
      "Epoch 702-Loss:     3.6549\n",
      "Epoch 703-Loss:     3.6235\n",
      "Epoch 704-Loss:     3.7007\n",
      "Epoch 705-Loss:     3.5092\n",
      "Epoch 706-Loss:     3.4490\n",
      "Epoch 707-Loss:     3.4263\n",
      "Epoch 708-Loss:     3.6348\n",
      "Epoch 709-Loss:     3.5545\n",
      "Epoch 710-Loss:     3.4175\n",
      "Epoch 711-Loss:     3.3473\n",
      "Epoch 712-Loss:     3.3237\n",
      "Epoch 713-Loss:     3.4899\n",
      "Epoch 714-Loss:     3.5546\n",
      "Epoch 715-Loss:     3.5345\n",
      "Epoch 716-Loss:     3.5096\n",
      "Epoch 717-Loss:     3.4455\n",
      "Epoch 718-Loss:     3.3803\n",
      "Epoch 719-Loss:     3.4295\n",
      "Epoch 720-Loss:     3.5434\n",
      "Epoch 721-Loss:     3.5153\n",
      "Epoch 722-Loss:     3.5085\n",
      "Epoch 723-Loss:     3.5039\n",
      "Epoch 724-Loss:     3.3237\n",
      "Epoch 725-Loss:     3.3108\n",
      "Epoch 726-Loss:     3.3001\n",
      "Epoch 727-Loss:     3.3412\n",
      "Epoch 728-Loss:     3.3628\n",
      "Epoch 729-Loss:     3.3904\n",
      "Epoch 730-Loss:     3.3486\n",
      "Epoch 731-Loss:     3.2742\n",
      "Epoch 732-Loss:     3.2859\n",
      "Epoch 733-Loss:     3.3976\n",
      "Epoch 734-Loss:     3.3489\n",
      "Epoch 735-Loss:     3.3568\n",
      "Epoch 736-Loss:     3.4433\n",
      "Epoch 737-Loss:     3.5866\n",
      "Epoch 738-Loss:     3.5423\n",
      "Epoch 739-Loss:     3.5088\n",
      "Epoch 740-Loss:     3.4178\n",
      "Epoch 741-Loss:     3.4267\n",
      "Epoch 742-Loss:     3.3797\n",
      "Epoch 743-Loss:     3.4198\n",
      "Epoch 744-Loss:     3.5058\n",
      "Epoch 745-Loss:     3.5526\n",
      "Epoch 746-Loss:     3.6014\n",
      "Epoch 747-Loss:     3.6224\n",
      "Epoch 748-Loss:     3.6312\n",
      "Epoch 749-Loss:     3.5014\n",
      "Epoch 750-Loss:     3.4438\n",
      "Epoch 751-Loss:     3.4056\n",
      "Epoch 752-Loss:     3.4417\n",
      "Epoch 753-Loss:     3.4332\n",
      "Epoch 754-Loss:     3.6090\n",
      "Epoch 755-Loss:     3.6195\n",
      "Epoch 756-Loss:     3.8121\n",
      "Epoch 757-Loss:     3.9212\n",
      "Epoch 758-Loss:     3.5600\n",
      "Epoch 759-Loss:     3.6125\n",
      "Epoch 760-Loss:     3.7960\n",
      "Epoch 761-Loss:     4.0152\n",
      "Epoch 762-Loss:     3.9652\n",
      "Epoch 763-Loss:     3.5523\n",
      "Epoch 764-Loss:     3.2545\n",
      "Epoch 765-Loss:     3.2472\n",
      "Epoch 766-Loss:     3.1197\n",
      "Epoch 767-Loss:     3.1708\n",
      "Epoch 768-Loss:     3.2356\n",
      "Epoch 769-Loss:     3.2581\n",
      "Epoch 770-Loss:     3.2856\n",
      "Epoch 771-Loss:     3.3238\n",
      "Epoch 772-Loss:     3.3634\n",
      "Epoch 773-Loss:     3.3308\n",
      "Epoch 774-Loss:     3.3130\n",
      "Epoch 775-Loss:     3.2813\n",
      "Epoch 776-Loss:     3.1747\n",
      "Epoch 777-Loss:     3.1673\n",
      "Epoch 778-Loss:     3.1796\n",
      "Epoch 779-Loss:     3.2091\n",
      "Epoch 780-Loss:     3.1890\n",
      "Epoch 781-Loss:     3.2500\n",
      "Epoch 782-Loss:     3.1089\n",
      "Epoch 783-Loss:     3.0596\n",
      "Epoch 784-Loss:     3.1070\n",
      "Epoch 785-Loss:     3.1763\n",
      "Epoch 786-Loss:     3.2302\n",
      "Epoch 787-Loss:     3.3448\n",
      "Epoch 788-Loss:     3.5957\n",
      "Epoch 789-Loss:     3.5708\n",
      "Epoch 790-Loss:     3.4536\n",
      "Epoch 791-Loss:     3.3214\n",
      "Epoch 792-Loss:     3.2501\n",
      "Epoch 793-Loss:     3.3057\n",
      "Epoch 794-Loss:     3.3195\n",
      "Epoch 795-Loss:     3.3695\n",
      "Epoch 796-Loss:     3.4117\n",
      "Epoch 797-Loss:     3.4191\n",
      "Epoch 798-Loss:     3.4594\n",
      "Epoch 799-Loss:     3.5003\n",
      "Epoch 800-Loss:     3.4488\n",
      "Epoch 801-Loss:     3.1726\n",
      "Epoch 802-Loss:     3.0573\n",
      "Epoch 803-Loss:     3.0390\n",
      "Epoch 804-Loss:     3.1084\n",
      "Epoch 805-Loss:     3.1284\n",
      "Epoch 806-Loss:     3.1371\n",
      "Epoch 807-Loss:     3.1177\n",
      "Epoch 808-Loss:     3.0429\n",
      "Epoch 809-Loss:     3.0977\n",
      "Epoch 810-Loss:     3.0816\n",
      "Epoch 811-Loss:     3.0595\n",
      "Epoch 812-Loss:     3.1163\n",
      "Epoch 813-Loss:     3.0278\n",
      "Epoch 814-Loss:     3.0910\n",
      "Epoch 815-Loss:     3.0748\n",
      "Epoch 816-Loss:     3.0086\n",
      "Epoch 817-Loss:     3.0848\n",
      "Epoch 818-Loss:     3.1029\n",
      "Epoch 819-Loss:     3.1288\n",
      "Epoch 820-Loss:     3.1623\n",
      "Epoch 821-Loss:     3.0817\n",
      "Epoch 822-Loss:     3.1506\n",
      "Epoch 823-Loss:     3.1729\n",
      "Epoch 824-Loss:     3.2364\n",
      "Epoch 825-Loss:     3.2305\n",
      "Epoch 826-Loss:     3.1669\n",
      "Epoch 827-Loss:     3.2201\n",
      "Epoch 828-Loss:     3.2487\n",
      "Epoch 829-Loss:     3.2683\n",
      "Epoch 830-Loss:     3.2860\n",
      "Epoch 831-Loss:     3.1110\n",
      "Epoch 832-Loss:     3.1944\n",
      "Epoch 833-Loss:     3.1715\n",
      "Epoch 834-Loss:     3.1692\n",
      "Epoch 835-Loss:     3.1853\n",
      "Epoch 836-Loss:     3.0957\n",
      "Epoch 837-Loss:     3.2287\n",
      "Epoch 838-Loss:     3.3034\n",
      "Epoch 839-Loss:     3.3725\n",
      "Epoch 840-Loss:     3.2905\n",
      "Epoch 841-Loss:     3.2620\n",
      "Epoch 842-Loss:     3.5011\n",
      "Epoch 843-Loss:     3.4176\n",
      "Epoch 844-Loss:     3.3734\n",
      "Epoch 845-Loss:     3.2474\n",
      "Epoch 846-Loss:     3.3582\n",
      "Epoch 847-Loss:     3.2632\n",
      "Epoch 848-Loss:     3.2481\n",
      "Epoch 849-Loss:     3.1637\n",
      "Epoch 850-Loss:     3.0638\n",
      "Epoch 851-Loss:     3.0654\n",
      "Epoch 852-Loss:     3.1444\n",
      "Epoch 853-Loss:     3.0147\n",
      "Epoch 854-Loss:     3.0529\n",
      "Epoch 855-Loss:     3.1529\n",
      "Epoch 856-Loss:     3.1336\n",
      "Epoch 857-Loss:     3.3053\n",
      "Epoch 858-Loss:     3.3397\n",
      "Epoch 859-Loss:     3.2257\n",
      "Epoch 860-Loss:     3.3431\n",
      "Epoch 861-Loss:     3.3705\n",
      "Epoch 862-Loss:     3.4433\n",
      "Epoch 863-Loss:     3.6362\n",
      "Epoch 864-Loss:     3.4951\n",
      "Epoch 865-Loss:     3.4110\n",
      "Epoch 866-Loss:     3.3521\n",
      "Epoch 867-Loss:     3.3837\n",
      "Epoch 868-Loss:     3.4228\n",
      "Epoch 869-Loss:     3.3521\n",
      "Epoch 870-Loss:     3.2656\n",
      "Epoch 871-Loss:     3.5285\n",
      "Epoch 872-Loss:     3.4673\n",
      "Epoch 873-Loss:     3.2267\n",
      "Epoch 874-Loss:     3.2767\n",
      "Epoch 875-Loss:     3.3228\n",
      "Epoch 876-Loss:     3.2837\n",
      "Epoch 877-Loss:     3.2435\n",
      "Epoch 878-Loss:     3.2029\n",
      "Epoch 879-Loss:     3.1730\n",
      "Epoch 880-Loss:     3.3288\n",
      "Epoch 881-Loss:     3.3093\n",
      "Epoch 882-Loss:     3.3289\n",
      "Epoch 883-Loss:     3.5223\n",
      "Epoch 884-Loss:     3.6112\n",
      "Epoch 885-Loss:     3.5927\n",
      "Epoch 886-Loss:     3.4018\n",
      "Epoch 887-Loss:     3.3113\n",
      "Epoch 888-Loss:     3.1532\n",
      "Epoch 889-Loss:     3.1200\n",
      "Epoch 890-Loss:     3.1843\n",
      "Epoch 891-Loss:     3.0940\n",
      "Epoch 892-Loss:     3.0927\n",
      "Epoch 893-Loss:     3.1195\n",
      "Epoch 894-Loss:     3.1478\n",
      "Epoch 895-Loss:     3.2325\n",
      "Epoch 896-Loss:     3.3452\n",
      "Epoch 897-Loss:     3.4130\n",
      "Epoch 898-Loss:     3.2638\n",
      "Epoch 899-Loss:     3.2874\n",
      "Epoch 900-Loss:     3.2802\n",
      "Epoch 901-Loss:     3.3746\n",
      "Epoch 902-Loss:     3.1690\n",
      "Epoch 903-Loss:     3.1978\n",
      "Epoch 904-Loss:     3.4108\n",
      "Epoch 905-Loss:     3.1584\n",
      "Epoch 906-Loss:     3.2279\n",
      "Epoch 907-Loss:     3.1436\n",
      "Epoch 908-Loss:     3.1979\n",
      "Epoch 909-Loss:     3.1525\n",
      "Epoch 910-Loss:     3.1577\n",
      "Epoch 911-Loss:     3.1283\n",
      "Epoch 912-Loss:     3.3649\n",
      "Epoch 913-Loss:     3.3534\n",
      "Epoch 914-Loss:     3.3726\n",
      "Epoch 915-Loss:     3.3798\n",
      "Epoch 916-Loss:     3.3116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 917-Loss:     3.4164\n",
      "Epoch 918-Loss:     3.3006\n",
      "Epoch 919-Loss:     3.1339\n",
      "Epoch 920-Loss:     3.1967\n",
      "Epoch 921-Loss:     3.1695\n",
      "Epoch 922-Loss:     3.1589\n",
      "Epoch 923-Loss:     3.1823\n",
      "Epoch 924-Loss:     3.2678\n",
      "Epoch 925-Loss:     3.2389\n",
      "Epoch 926-Loss:     3.2798\n",
      "Epoch 927-Loss:     3.1488\n",
      "Epoch 928-Loss:     3.0857\n",
      "Epoch 929-Loss:     3.1542\n",
      "Epoch 930-Loss:     3.3447\n",
      "Epoch 931-Loss:     3.3497\n",
      "Epoch 932-Loss:     3.3306\n",
      "Epoch 933-Loss:     3.3309\n",
      "Epoch 934-Loss:     3.3754\n",
      "Epoch 935-Loss:     3.3648\n",
      "Epoch 936-Loss:     3.5041\n",
      "Epoch 937-Loss:     3.3488\n",
      "Epoch 938-Loss:     3.3630\n",
      "Epoch 939-Loss:     3.4105\n",
      "Epoch 940-Loss:     3.4261\n",
      "Epoch 941-Loss:     3.4840\n",
      "Epoch 942-Loss:     3.5003\n",
      "Epoch 943-Loss:     3.6021\n",
      "Epoch 944-Loss:     3.3964\n",
      "Epoch 945-Loss:     3.4329\n",
      "Epoch 946-Loss:     3.5294\n",
      "Epoch 947-Loss:     3.5388\n",
      "Epoch 948-Loss:     3.5417\n",
      "Epoch 949-Loss:     3.4307\n",
      "Epoch 950-Loss:     3.5198\n",
      "Epoch 951-Loss:     3.3841\n",
      "Epoch 952-Loss:     3.4142\n",
      "Epoch 953-Loss:     3.4962\n",
      "Epoch 954-Loss:     3.5606\n",
      "Epoch 955-Loss:     3.4965\n",
      "Epoch 956-Loss:     3.3826\n",
      "Epoch 957-Loss:     3.3907\n",
      "Epoch 958-Loss:     3.3728\n",
      "Epoch 959-Loss:     3.2719\n",
      "Epoch 960-Loss:     3.0777\n",
      "Epoch 961-Loss:     3.1018\n",
      "Epoch 962-Loss:     3.0092\n",
      "Epoch 963-Loss:     3.0187\n",
      "Epoch 964-Loss:     3.0206\n",
      "Epoch 965-Loss:     2.9661\n",
      "Epoch 966-Loss:     2.9387\n",
      "Epoch 967-Loss:     2.9003\n",
      "Epoch 968-Loss:     3.0916\n",
      "Epoch 969-Loss:     3.4767\n",
      "Epoch 970-Loss:     3.3878\n",
      "Epoch 971-Loss:     3.2162\n",
      "Epoch 972-Loss:     3.0925\n",
      "Epoch 973-Loss:     3.0538\n",
      "Epoch 974-Loss:     3.0781\n",
      "Epoch 975-Loss:     2.9726\n",
      "Epoch 976-Loss:     2.9471\n",
      "Epoch 977-Loss:     2.9089\n",
      "Epoch 978-Loss:     2.9454\n",
      "Epoch 979-Loss:     2.9781\n",
      "Epoch 980-Loss:     2.9152\n",
      "Epoch 981-Loss:     2.8625\n",
      "Epoch 982-Loss:     2.8761\n",
      "Epoch 983-Loss:     2.9874\n",
      "Epoch 984-Loss:     2.9771\n",
      "Epoch 985-Loss:     2.8645\n",
      "Epoch 986-Loss:     2.8972\n",
      "Epoch 987-Loss:     2.8888\n",
      "Epoch 988-Loss:     2.9584\n",
      "Epoch 989-Loss:     2.9875\n",
      "Epoch 990-Loss:     3.1092\n",
      "Epoch 991-Loss:     3.2582\n",
      "Epoch 992-Loss:     3.3611\n",
      "Epoch 993-Loss:     3.4324\n",
      "Epoch 994-Loss:     3.6808\n",
      "Epoch 995-Loss:     3.4826\n",
      "Epoch 996-Loss:     3.4722\n",
      "Epoch 997-Loss:     3.0210\n",
      "Epoch 998-Loss:     3.1058\n",
      "Epoch 999-Loss:     3.0320\n",
      "Epoch 1000-Loss:     3.0122\n",
      "Epoch 1001-Loss:     2.9429\n",
      "Epoch 1002-Loss:     2.9483\n",
      "Epoch 1003-Loss:     3.0492\n",
      "Epoch 1004-Loss:     3.0620\n",
      "Epoch 1005-Loss:     3.0840\n",
      "Epoch 1006-Loss:     3.0923\n",
      "Epoch 1007-Loss:     3.0712\n",
      "Epoch 1008-Loss:     3.0585\n",
      "Epoch 1009-Loss:     2.8718\n",
      "Epoch 1010-Loss:     2.9177\n",
      "Epoch 1011-Loss:     2.9801\n",
      "Epoch 1012-Loss:     2.9677\n",
      "Epoch 1013-Loss:     2.9064\n",
      "Epoch 1014-Loss:     2.8928\n",
      "Epoch 1015-Loss:     2.8321\n",
      "Epoch 1016-Loss:     2.8668\n",
      "Epoch 1017-Loss:     2.9039\n",
      "Epoch 1018-Loss:     2.9097\n",
      "Epoch 1019-Loss:     3.0378\n",
      "Epoch 1020-Loss:     2.9359\n",
      "Epoch 1021-Loss:     2.9431\n",
      "Epoch 1022-Loss:     2.9404\n",
      "Epoch 1023-Loss:     2.9000\n",
      "Epoch 1024-Loss:     2.9007\n",
      "Epoch 1025-Loss:     2.9040\n",
      "Epoch 1026-Loss:     3.1499\n",
      "Epoch 1027-Loss:     2.9796\n",
      "Epoch 1028-Loss:     3.0711\n",
      "Epoch 1029-Loss:     2.9987\n",
      "Epoch 1030-Loss:     3.0851\n",
      "Epoch 1031-Loss:     3.0580\n",
      "Epoch 1032-Loss:     3.0847\n",
      "Epoch 1033-Loss:     3.0049\n",
      "Epoch 1034-Loss:     3.0085\n",
      "Epoch 1035-Loss:     2.9629\n",
      "Epoch 1036-Loss:     3.0154\n",
      "Epoch 1037-Loss:     3.0402\n",
      "Epoch 1038-Loss:     3.0998\n",
      "Epoch 1039-Loss:     3.0815\n",
      "Epoch 1040-Loss:     3.1607\n",
      "Epoch 1041-Loss:     3.2036\n",
      "Epoch 1042-Loss:     3.3480\n",
      "Epoch 1043-Loss:     3.3470\n",
      "Epoch 1044-Loss:     3.2975\n",
      "Epoch 1045-Loss:     3.2952\n",
      "Epoch 1046-Loss:     3.3323\n",
      "Epoch 1047-Loss:     3.3592\n",
      "Epoch 1048-Loss:     3.2662\n",
      "Epoch 1049-Loss:     3.2383\n",
      "Epoch 1050-Loss:     3.2813\n",
      "Epoch 1051-Loss:     3.2519\n",
      "Epoch 1052-Loss:     3.3396\n",
      "Epoch 1053-Loss:     3.3456\n",
      "Epoch 1054-Loss:     3.3582\n",
      "Epoch 1055-Loss:     3.2143\n",
      "Epoch 1056-Loss:     3.2549\n",
      "Epoch 1057-Loss:     3.3205\n",
      "Epoch 1058-Loss:     3.2036\n",
      "Epoch 1059-Loss:     3.0926\n",
      "Epoch 1060-Loss:     3.1228\n",
      "Epoch 1061-Loss:     3.2453\n",
      "Epoch 1062-Loss:     3.5070\n",
      "Epoch 1063-Loss:     3.2743\n",
      "Epoch 1064-Loss:     3.2188\n",
      "Epoch 1065-Loss:     3.1869\n",
      "Epoch 1066-Loss:     3.1745\n",
      "Epoch 1067-Loss:     3.2182\n",
      "Epoch 1068-Loss:     3.2194\n",
      "Epoch 1069-Loss:     3.0861\n",
      "Epoch 1070-Loss:     2.9996\n",
      "Epoch 1071-Loss:     3.0372\n",
      "Epoch 1072-Loss:     3.0068\n",
      "Epoch 1073-Loss:     3.1227\n",
      "Epoch 1074-Loss:     3.1618\n",
      "Epoch 1075-Loss:     3.1685\n",
      "Epoch 1076-Loss:     3.1676\n",
      "Epoch 1077-Loss:     3.1805\n",
      "Epoch 1078-Loss:     3.2055\n",
      "Epoch 1079-Loss:     3.1305\n",
      "Epoch 1080-Loss:     3.1325\n",
      "Epoch 1081-Loss:     3.0557\n",
      "Epoch 1082-Loss:     3.0168\n",
      "Epoch 1083-Loss:     2.9511\n",
      "Epoch 1084-Loss:     2.8179\n",
      "Epoch 1085-Loss:     2.7531\n",
      "Epoch 1086-Loss:     2.8916\n",
      "Epoch 1087-Loss:     2.8347\n",
      "Epoch 1088-Loss:     2.7823\n",
      "Epoch 1089-Loss:     2.8071\n",
      "Epoch 1090-Loss:     2.8289\n",
      "Epoch 1091-Loss:     2.8022\n",
      "Epoch 1092-Loss:     2.8105\n",
      "Epoch 1093-Loss:     2.7881\n",
      "Epoch 1094-Loss:     2.7703\n",
      "Epoch 1095-Loss:     2.8148\n",
      "Epoch 1096-Loss:     2.9085\n",
      "Epoch 1097-Loss:     2.9541\n",
      "Epoch 1098-Loss:     2.9027\n",
      "Epoch 1099-Loss:     2.7369\n",
      "Epoch 1100-Loss:     2.7624\n",
      "Epoch 1101-Loss:     2.7869\n",
      "Epoch 1102-Loss:     2.8202\n",
      "Epoch 1103-Loss:     2.9654\n",
      "Epoch 1104-Loss:     2.8318\n",
      "Epoch 1105-Loss:     3.0544\n",
      "Epoch 1106-Loss:     3.0992\n",
      "Epoch 1107-Loss:     3.0279\n",
      "Epoch 1108-Loss:     3.1076\n",
      "Epoch 1109-Loss:     3.1383\n",
      "Epoch 1110-Loss:     3.0945\n",
      "Epoch 1111-Loss:     3.0117\n",
      "Epoch 1112-Loss:     3.0399\n",
      "Epoch 1113-Loss:     2.9938\n",
      "Epoch 1114-Loss:     3.0304\n",
      "Epoch 1115-Loss:     3.0027\n",
      "Epoch 1116-Loss:     3.1180\n",
      "Epoch 1117-Loss:     3.0762\n",
      "Epoch 1118-Loss:     3.0262\n",
      "Epoch 1119-Loss:     2.8886\n",
      "Epoch 1120-Loss:     2.8058\n",
      "Epoch 1121-Loss:     2.8241\n",
      "Epoch 1122-Loss:     2.8943\n",
      "Epoch 1123-Loss:     2.8930\n",
      "Epoch 1124-Loss:     2.8978\n",
      "Epoch 1125-Loss:     2.9006\n",
      "Epoch 1126-Loss:     2.8490\n",
      "Epoch 1127-Loss:     2.9246\n",
      "Epoch 1128-Loss:     2.9157\n",
      "Epoch 1129-Loss:     2.9370\n",
      "Epoch 1130-Loss:     2.9385\n",
      "Epoch 1131-Loss:     2.7984\n",
      "Epoch 1132-Loss:     2.8587\n",
      "Epoch 1133-Loss:     2.8606\n",
      "Epoch 1134-Loss:     2.8086\n",
      "Epoch 1135-Loss:     2.8938\n",
      "Epoch 1136-Loss:     2.9698\n",
      "Epoch 1137-Loss:     3.0290\n",
      "Epoch 1138-Loss:     3.0631\n",
      "Epoch 1139-Loss:     3.0730\n",
      "Epoch 1140-Loss:     3.0156\n",
      "Epoch 1141-Loss:     2.9129\n",
      "Epoch 1142-Loss:     2.8146\n",
      "Epoch 1143-Loss:     2.9335\n",
      "Epoch 1144-Loss:     2.9414\n",
      "Epoch 1145-Loss:     2.9661\n",
      "Epoch 1146-Loss:     2.9229\n",
      "Epoch 1147-Loss:     2.8372\n",
      "Epoch 1148-Loss:     2.9242\n",
      "Epoch 1149-Loss:     2.8847\n",
      "Epoch 1150-Loss:     2.8608\n",
      "Epoch 1151-Loss:     2.8568\n",
      "Epoch 1152-Loss:     2.7789\n",
      "Epoch 1153-Loss:     2.8321\n",
      "Epoch 1154-Loss:     2.8148\n",
      "Epoch 1155-Loss:     2.8905\n",
      "Epoch 1156-Loss:     2.7841\n",
      "Epoch 1157-Loss:     2.8193\n",
      "Epoch 1158-Loss:     2.8136\n",
      "Epoch 1159-Loss:     2.9999\n",
      "Epoch 1160-Loss:     2.9581\n",
      "Epoch 1161-Loss:     2.8983\n",
      "Epoch 1162-Loss:     2.9204\n",
      "Epoch 1163-Loss:     3.0663\n",
      "Epoch 1164-Loss:     3.2281\n",
      "Epoch 1165-Loss:     3.1449\n",
      "Epoch 1166-Loss:     3.1471\n",
      "Epoch 1167-Loss:     2.9642\n",
      "Epoch 1168-Loss:     3.0496\n",
      "Epoch 1169-Loss:     2.9642\n",
      "Epoch 1170-Loss:     2.9594\n",
      "Epoch 1171-Loss:     2.9267\n",
      "Epoch 1172-Loss:     3.0381\n",
      "Epoch 1173-Loss:     3.0553\n",
      "Epoch 1174-Loss:     3.0785\n",
      "Epoch 1175-Loss:     3.0106\n",
      "Epoch 1176-Loss:     3.0107\n",
      "Epoch 1177-Loss:     2.8590\n",
      "Epoch 1178-Loss:     2.8648\n",
      "Epoch 1179-Loss:     2.8074\n",
      "Epoch 1180-Loss:     2.8346\n",
      "Epoch 1181-Loss:     2.8274\n",
      "Epoch 1182-Loss:     2.8624\n",
      "Epoch 1183-Loss:     2.8333\n",
      "Epoch 1184-Loss:     2.8651\n",
      "Epoch 1185-Loss:     2.9235\n",
      "Epoch 1186-Loss:     2.9273\n",
      "Epoch 1187-Loss:     2.9827\n",
      "Epoch 1188-Loss:     3.0308\n",
      "Epoch 1189-Loss:     2.9966\n",
      "Epoch 1190-Loss:     3.0170\n",
      "Epoch 1191-Loss:     2.9617\n",
      "Epoch 1192-Loss:     2.8594\n",
      "Epoch 1193-Loss:     2.8635\n",
      "Epoch 1194-Loss:     2.9063\n",
      "Epoch 1195-Loss:     3.0354\n",
      "Epoch 1196-Loss:     2.9681\n",
      "Epoch 1197-Loss:     2.9899\n",
      "Epoch 1198-Loss:     3.0234\n",
      "Epoch 1199-Loss:     3.0074\n",
      "Epoch 1200-Loss:     2.9517\n",
      "Epoch 1201-Loss:     3.0326\n",
      "Epoch 1202-Loss:     3.0558\n",
      "Epoch 1203-Loss:     3.0221\n",
      "Epoch 1204-Loss:     3.1254\n",
      "Epoch 1205-Loss:     3.1026\n",
      "Epoch 1206-Loss:     3.2334\n",
      "Epoch 1207-Loss:     3.4683\n",
      "Epoch 1208-Loss:     3.5337\n",
      "Epoch 1209-Loss:     3.4578\n",
      "Epoch 1210-Loss:     3.2863\n",
      "Epoch 1211-Loss:     3.1106\n",
      "Epoch 1212-Loss:     2.9563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1213-Loss:     2.9139\n",
      "Epoch 1214-Loss:     2.9480\n",
      "Epoch 1215-Loss:     2.8455\n",
      "Epoch 1216-Loss:     2.8371\n",
      "Epoch 1217-Loss:     2.7483\n",
      "Epoch 1218-Loss:     2.7974\n",
      "Epoch 1219-Loss:     2.8443\n",
      "Epoch 1220-Loss:     2.8157\n",
      "Epoch 1221-Loss:     2.8035\n",
      "Epoch 1222-Loss:     2.8255\n",
      "Epoch 1223-Loss:     2.8246\n",
      "Epoch 1224-Loss:     2.8933\n",
      "Epoch 1225-Loss:     2.9017\n",
      "Epoch 1226-Loss:     2.8124\n",
      "Epoch 1227-Loss:     2.7727\n",
      "Epoch 1228-Loss:     2.8944\n",
      "Epoch 1229-Loss:     2.8684\n",
      "Epoch 1230-Loss:     2.8631\n",
      "Epoch 1231-Loss:     2.8082\n",
      "Epoch 1232-Loss:     2.6811\n",
      "Epoch 1233-Loss:     2.7064\n",
      "Epoch 1234-Loss:     2.7181\n",
      "Epoch 1235-Loss:     2.7522\n",
      "Epoch 1236-Loss:     2.7591\n",
      "Epoch 1237-Loss:     2.7563\n",
      "Epoch 1238-Loss:     2.7366\n",
      "Epoch 1239-Loss:     2.6997\n",
      "Epoch 1240-Loss:     2.7089\n",
      "Epoch 1241-Loss:     2.6004\n",
      "Epoch 1242-Loss:     2.6377\n",
      "Epoch 1243-Loss:     2.6123\n",
      "Epoch 1244-Loss:     2.5513\n",
      "Epoch 1245-Loss:     2.6595\n",
      "Epoch 1246-Loss:     2.6407\n",
      "Epoch 1247-Loss:     2.6613\n",
      "Epoch 1248-Loss:     2.6237\n",
      "Epoch 1249-Loss:     2.6271\n",
      "Epoch 1250-Loss:     2.6122\n",
      "Epoch 1251-Loss:     2.6154\n",
      "Epoch 1252-Loss:     2.6261\n",
      "Epoch 1253-Loss:     2.6473\n",
      "Epoch 1254-Loss:     2.7657\n",
      "Epoch 1255-Loss:     2.6943\n",
      "Epoch 1256-Loss:     2.7260\n",
      "Epoch 1257-Loss:     2.7709\n",
      "Epoch 1258-Loss:     2.7682\n",
      "Epoch 1259-Loss:     2.7982\n",
      "Epoch 1260-Loss:     2.8031\n",
      "Epoch 1261-Loss:     2.7533\n",
      "Epoch 1262-Loss:     2.7973\n",
      "Epoch 1263-Loss:     2.8863\n",
      "Epoch 1264-Loss:     2.8731\n",
      "Epoch 1265-Loss:     2.7481\n",
      "Epoch 1266-Loss:     2.9581\n",
      "Epoch 1267-Loss:     3.0392\n",
      "Epoch 1268-Loss:     3.0561\n",
      "Epoch 1269-Loss:     2.9280\n",
      "Epoch 1270-Loss:     2.8285\n",
      "Epoch 1271-Loss:     2.7993\n",
      "Epoch 1272-Loss:     2.7435\n",
      "Epoch 1273-Loss:     2.7071\n",
      "Epoch 1274-Loss:     2.6541\n",
      "Epoch 1275-Loss:     2.6400\n",
      "Epoch 1276-Loss:     2.6671\n",
      "Epoch 1277-Loss:     2.7341\n",
      "Epoch 1278-Loss:     2.7751\n",
      "Epoch 1279-Loss:     2.7579\n",
      "Epoch 1280-Loss:     2.7602\n",
      "Epoch 1281-Loss:     2.7801\n",
      "Epoch 1282-Loss:     2.8045\n",
      "Epoch 1283-Loss:     2.7724\n",
      "Epoch 1284-Loss:     2.7910\n",
      "Epoch 1285-Loss:     2.7404\n",
      "Epoch 1286-Loss:     2.7453\n",
      "Epoch 1287-Loss:     2.8581\n",
      "Epoch 1288-Loss:     2.8867\n",
      "Epoch 1289-Loss:     2.8217\n",
      "Epoch 1290-Loss:     2.7389\n",
      "Epoch 1291-Loss:     2.7597\n",
      "Epoch 1292-Loss:     2.8554\n",
      "Epoch 1293-Loss:     2.9678\n",
      "Epoch 1294-Loss:     3.0995\n",
      "Epoch 1295-Loss:     3.1123\n",
      "Epoch 1296-Loss:     3.0167\n",
      "Epoch 1297-Loss:     3.2487\n",
      "Epoch 1298-Loss:     3.1365\n",
      "Epoch 1299-Loss:     2.9057\n",
      "Epoch 1300-Loss:     3.1857\n",
      "Epoch 1301-Loss:     3.2456\n",
      "Epoch 1302-Loss:     3.0414\n",
      "Epoch 1303-Loss:     3.1829\n",
      "Epoch 1304-Loss:     2.9873\n",
      "Epoch 1305-Loss:     2.9926\n",
      "Epoch 1306-Loss:     2.9721\n",
      "Epoch 1307-Loss:     2.8618\n",
      "Epoch 1308-Loss:     2.8039\n",
      "Epoch 1309-Loss:     2.9609\n",
      "Epoch 1310-Loss:     2.9569\n",
      "Epoch 1311-Loss:     3.0327\n",
      "Epoch 1312-Loss:     3.1965\n",
      "Epoch 1313-Loss:     3.1217\n",
      "Epoch 1314-Loss:     3.1445\n",
      "Epoch 1315-Loss:     3.0566\n",
      "Epoch 1316-Loss:     2.8514\n",
      "Epoch 1317-Loss:     2.9281\n",
      "Epoch 1318-Loss:     2.9134\n",
      "Epoch 1319-Loss:     2.8750\n",
      "Epoch 1320-Loss:     2.7571\n",
      "Epoch 1321-Loss:     2.7637\n",
      "Epoch 1322-Loss:     2.8731\n",
      "Epoch 1323-Loss:     2.8885\n",
      "Epoch 1324-Loss:     2.8469\n",
      "Epoch 1325-Loss:     2.8829\n",
      "Epoch 1326-Loss:     3.0062\n",
      "Epoch 1327-Loss:     2.8745\n",
      "Epoch 1328-Loss:     2.8056\n",
      "Epoch 1329-Loss:     2.7390\n",
      "Epoch 1330-Loss:     2.7317\n",
      "Epoch 1331-Loss:     2.6937\n",
      "Epoch 1332-Loss:     2.7677\n",
      "Epoch 1333-Loss:     2.6528\n",
      "Epoch 1334-Loss:     2.6897\n",
      "Epoch 1335-Loss:     2.9175\n",
      "Epoch 1336-Loss:     2.8639\n",
      "Epoch 1337-Loss:     2.8738\n",
      "Epoch 1338-Loss:     2.8737\n",
      "Epoch 1339-Loss:     2.7934\n",
      "Epoch 1340-Loss:     2.8144\n",
      "Epoch 1341-Loss:     2.9743\n",
      "Epoch 1342-Loss:     3.0581\n",
      "Epoch 1343-Loss:     2.9710\n",
      "Epoch 1344-Loss:     2.8768\n",
      "Epoch 1345-Loss:     2.7872\n",
      "Epoch 1346-Loss:     2.7117\n",
      "Epoch 1347-Loss:     2.8092\n",
      "Epoch 1348-Loss:     2.8688\n",
      "Epoch 1349-Loss:     2.8085\n",
      "Epoch 1350-Loss:     2.8582\n",
      "Epoch 1351-Loss:     2.8561\n",
      "Epoch 1352-Loss:     2.8719\n",
      "Epoch 1353-Loss:     2.9285\n",
      "Epoch 1354-Loss:     3.0412\n",
      "Epoch 1355-Loss:     3.3196\n",
      "Epoch 1356-Loss:     3.2181\n",
      "Epoch 1357-Loss:     3.3372\n",
      "Epoch 1358-Loss:     3.3489\n",
      "Epoch 1359-Loss:     3.1854\n",
      "Epoch 1360-Loss:     2.8774\n",
      "Epoch 1361-Loss:     2.8260\n",
      "Epoch 1362-Loss:     3.0060\n",
      "Epoch 1363-Loss:     3.0101\n",
      "Epoch 1364-Loss:     3.2382\n",
      "Epoch 1365-Loss:     3.1023\n",
      "Epoch 1366-Loss:     2.9815\n",
      "Epoch 1367-Loss:     2.9503\n",
      "Epoch 1368-Loss:     2.8820\n",
      "Epoch 1369-Loss:     2.8538\n",
      "Epoch 1370-Loss:     2.7997\n",
      "Epoch 1371-Loss:     2.8273\n",
      "Epoch 1372-Loss:     2.7479\n",
      "Epoch 1373-Loss:     2.6338\n",
      "Epoch 1374-Loss:     2.6490\n",
      "Epoch 1375-Loss:     2.5913\n",
      "Epoch 1376-Loss:     2.6556\n",
      "Epoch 1377-Loss:     2.6739\n",
      "Epoch 1378-Loss:     2.6456\n",
      "Epoch 1379-Loss:     2.7267\n",
      "Epoch 1380-Loss:     2.9391\n",
      "Epoch 1381-Loss:     2.7584\n",
      "Epoch 1382-Loss:     3.0740\n",
      "Epoch 1383-Loss:     2.8538\n",
      "Epoch 1384-Loss:     2.9064\n",
      "Epoch 1385-Loss:     3.1295\n",
      "Epoch 1386-Loss:     3.0374\n",
      "Epoch 1387-Loss:     2.8578\n",
      "Epoch 1388-Loss:     2.8074\n",
      "Epoch 1389-Loss:     2.7575\n",
      "Epoch 1390-Loss:     2.7712\n",
      "Epoch 1391-Loss:     2.6820\n",
      "Epoch 1392-Loss:     2.7371\n",
      "Epoch 1393-Loss:     2.8428\n",
      "Epoch 1394-Loss:     2.7890\n",
      "Epoch 1395-Loss:     2.7203\n",
      "Epoch 1396-Loss:     2.7286\n",
      "Epoch 1397-Loss:     2.7717\n",
      "Epoch 1398-Loss:     2.6757\n",
      "Epoch 1399-Loss:     2.7005\n",
      "Epoch 1400-Loss:     2.6205\n",
      "Epoch 1401-Loss:     2.6262\n",
      "Epoch 1402-Loss:     2.6057\n",
      "Epoch 1403-Loss:     2.6348\n",
      "Epoch 1404-Loss:     2.6578\n",
      "Epoch 1405-Loss:     2.6967\n",
      "Epoch 1406-Loss:     2.8123\n",
      "Epoch 1407-Loss:     2.8728\n",
      "Epoch 1408-Loss:     3.0230\n",
      "Epoch 1409-Loss:     3.0335\n",
      "Epoch 1410-Loss:     3.0264\n",
      "Epoch 1411-Loss:     2.9563\n",
      "Epoch 1412-Loss:     2.8398\n",
      "Epoch 1413-Loss:     2.8574\n",
      "Epoch 1414-Loss:     2.8879\n",
      "Epoch 1415-Loss:     2.8008\n",
      "Epoch 1416-Loss:     2.8605\n",
      "Epoch 1417-Loss:     2.7412\n",
      "Epoch 1418-Loss:     2.6395\n",
      "Epoch 1419-Loss:     2.7618\n",
      "Epoch 1420-Loss:     2.7693\n",
      "Epoch 1421-Loss:     2.8930\n",
      "Epoch 1422-Loss:     3.0002\n",
      "Epoch 1423-Loss:     3.2026\n",
      "Epoch 1424-Loss:     3.0142\n",
      "Epoch 1425-Loss:     3.0370\n",
      "Epoch 1426-Loss:     3.0907\n",
      "Epoch 1427-Loss:     3.0234\n",
      "Epoch 1428-Loss:     2.8689\n",
      "Epoch 1429-Loss:     2.8683\n",
      "Epoch 1430-Loss:     2.8763\n",
      "Epoch 1431-Loss:     2.9882\n",
      "Epoch 1432-Loss:     3.0635\n",
      "Epoch 1433-Loss:     3.0548\n",
      "Epoch 1434-Loss:     3.1362\n",
      "Epoch 1435-Loss:     3.0334\n",
      "Epoch 1436-Loss:     3.0458\n",
      "Epoch 1437-Loss:     3.0626\n",
      "Epoch 1438-Loss:     3.0121\n",
      "Epoch 1439-Loss:     2.9436\n",
      "Epoch 1440-Loss:     2.7974\n",
      "Epoch 1441-Loss:     2.7834\n",
      "Epoch 1442-Loss:     2.7502\n",
      "Epoch 1443-Loss:     2.7658\n",
      "Epoch 1444-Loss:     2.7402\n",
      "Epoch 1445-Loss:     2.7055\n",
      "Epoch 1446-Loss:     2.6803\n",
      "Epoch 1447-Loss:     2.6720\n",
      "Epoch 1448-Loss:     2.7210\n",
      "Epoch 1449-Loss:     2.6713\n",
      "Epoch 1450-Loss:     2.6785\n",
      "Epoch 1451-Loss:     2.7134\n",
      "Epoch 1452-Loss:     2.7832\n",
      "Epoch 1453-Loss:     2.7204\n",
      "Epoch 1454-Loss:     2.7183\n",
      "Epoch 1455-Loss:     2.7744\n",
      "Epoch 1456-Loss:     2.7463\n",
      "Epoch 1457-Loss:     2.6660\n",
      "Epoch 1458-Loss:     2.6448\n",
      "Epoch 1459-Loss:     2.6126\n",
      "Epoch 1460-Loss:     2.6834\n",
      "Epoch 1461-Loss:     2.7316\n",
      "Epoch 1462-Loss:     2.9081\n",
      "Epoch 1463-Loss:     3.0800\n",
      "Epoch 1464-Loss:     3.0430\n",
      "Epoch 1465-Loss:     2.8274\n",
      "Epoch 1466-Loss:     2.8060\n",
      "Epoch 1467-Loss:     2.9019\n",
      "Epoch 1468-Loss:     2.9293\n",
      "Epoch 1469-Loss:     2.6996\n",
      "Epoch 1470-Loss:     2.6560\n",
      "Epoch 1471-Loss:     2.6623\n",
      "Epoch 1472-Loss:     2.6819\n",
      "Epoch 1473-Loss:     2.5985\n",
      "Epoch 1474-Loss:     2.6685\n",
      "Epoch 1475-Loss:     2.6788\n",
      "Epoch 1476-Loss:     2.6381\n",
      "Epoch 1477-Loss:     2.6830\n",
      "Epoch 1478-Loss:     2.6645\n",
      "Epoch 1479-Loss:     2.7255\n",
      "Epoch 1480-Loss:     2.7649\n",
      "Epoch 1481-Loss:     2.8287\n",
      "Epoch 1482-Loss:     2.8135\n",
      "Epoch 1483-Loss:     2.7817\n",
      "Epoch 1484-Loss:     2.7049\n",
      "Epoch 1485-Loss:     2.6707\n",
      "Epoch 1486-Loss:     2.6826\n",
      "Epoch 1487-Loss:     2.6262\n",
      "Epoch 1488-Loss:     2.6372\n",
      "Epoch 1489-Loss:     2.6176\n",
      "Epoch 1490-Loss:     2.6521\n",
      "Epoch 1491-Loss:     2.5823\n",
      "Epoch 1492-Loss:     2.6279\n",
      "Epoch 1493-Loss:     2.6018\n",
      "Epoch 1494-Loss:     2.8265\n",
      "Epoch 1495-Loss:     2.7810\n",
      "Epoch 1496-Loss:     2.7230\n",
      "Epoch 1497-Loss:     2.7052\n",
      "Epoch 1498-Loss:     2.6577\n",
      "Epoch 1499-Loss:     2.7378\n",
      "Epoch 1500-Loss:     2.7297\n",
      "Epoch 1501-Loss:     2.7571\n",
      "Epoch 1502-Loss:     2.8931\n",
      "Epoch 1503-Loss:     3.0127\n",
      "Epoch 1504-Loss:     2.9293\n",
      "Epoch 1505-Loss:     3.0521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1506-Loss:     2.8925\n",
      "Epoch 1507-Loss:     2.9467\n",
      "Epoch 1508-Loss:     2.8789\n",
      "Epoch 1509-Loss:     2.8282\n",
      "Epoch 1510-Loss:     2.6665\n",
      "Epoch 1511-Loss:     2.6169\n",
      "Epoch 1512-Loss:     2.6585\n",
      "Epoch 1513-Loss:     2.6842\n",
      "Epoch 1514-Loss:     2.7160\n",
      "Epoch 1515-Loss:     2.7571\n",
      "Epoch 1516-Loss:     2.7518\n",
      "Epoch 1517-Loss:     2.6719\n",
      "Epoch 1518-Loss:     2.7078\n",
      "Epoch 1519-Loss:     2.6769\n",
      "Epoch 1520-Loss:     2.6005\n",
      "Epoch 1521-Loss:     2.5966\n",
      "Epoch 1522-Loss:     2.6545\n",
      "Epoch 1523-Loss:     2.6315\n",
      "Epoch 1524-Loss:     2.7389\n",
      "Epoch 1525-Loss:     2.7292\n",
      "Epoch 1526-Loss:     2.8232\n",
      "Epoch 1527-Loss:     2.8326\n",
      "Epoch 1528-Loss:     2.8216\n",
      "Epoch 1529-Loss:     2.8684\n",
      "Epoch 1530-Loss:     2.7870\n",
      "Epoch 1531-Loss:     2.9363\n",
      "Epoch 1532-Loss:     2.7345\n",
      "Epoch 1533-Loss:     2.8083\n",
      "Epoch 1534-Loss:     2.6695\n",
      "Epoch 1535-Loss:     2.5352\n",
      "Epoch 1536-Loss:     2.5162\n",
      "Epoch 1537-Loss:     2.4768\n",
      "Epoch 1538-Loss:     2.5561\n",
      "Epoch 1539-Loss:     2.5723\n",
      "Epoch 1540-Loss:     2.5993\n",
      "Epoch 1541-Loss:     2.6012\n",
      "Epoch 1542-Loss:     2.5620\n",
      "Epoch 1543-Loss:     2.7138\n",
      "Epoch 1544-Loss:     2.6500\n",
      "Epoch 1545-Loss:     2.6334\n",
      "Epoch 1546-Loss:     2.5960\n",
      "Epoch 1547-Loss:     2.5384\n",
      "Epoch 1548-Loss:     2.6008\n",
      "Epoch 1549-Loss:     2.6341\n",
      "Epoch 1550-Loss:     2.5926\n",
      "Epoch 1551-Loss:     2.7619\n",
      "Epoch 1552-Loss:     2.8265\n",
      "Epoch 1553-Loss:     2.8010\n",
      "Epoch 1554-Loss:     2.7493\n",
      "Epoch 1555-Loss:     2.9478\n",
      "Epoch 1556-Loss:     3.0698\n",
      "Epoch 1557-Loss:     2.8641\n",
      "Epoch 1558-Loss:     2.8452\n",
      "Epoch 1559-Loss:     2.7947\n",
      "Epoch 1560-Loss:     2.6725\n",
      "Epoch 1561-Loss:     2.7406\n",
      "Epoch 1562-Loss:     2.6897\n",
      "Epoch 1563-Loss:     2.6794\n",
      "Epoch 1564-Loss:     2.7063\n",
      "Epoch 1565-Loss:     2.7053\n",
      "Epoch 1566-Loss:     2.7664\n",
      "Epoch 1567-Loss:     2.8759\n",
      "Epoch 1568-Loss:     2.8977\n",
      "Epoch 1569-Loss:     2.7420\n",
      "Epoch 1570-Loss:     2.9158\n",
      "Epoch 1571-Loss:     2.7585\n",
      "Epoch 1572-Loss:     2.7043\n",
      "Epoch 1573-Loss:     2.6917\n",
      "Epoch 1574-Loss:     2.7331\n",
      "Epoch 1575-Loss:     2.6549\n",
      "Epoch 1576-Loss:     2.6603\n",
      "Epoch 1577-Loss:     2.6767\n",
      "Epoch 1578-Loss:     2.7126\n",
      "Epoch 1579-Loss:     2.7094\n",
      "Epoch 1580-Loss:     2.7280\n",
      "Epoch 1581-Loss:     2.8168\n",
      "Epoch 1582-Loss:     2.7470\n",
      "Epoch 1583-Loss:     2.7480\n",
      "Epoch 1584-Loss:     2.8054\n",
      "Epoch 1585-Loss:     2.9118\n",
      "Epoch 1586-Loss:     2.8707\n",
      "Epoch 1587-Loss:     2.7382\n",
      "Epoch 1588-Loss:     2.6298\n",
      "Epoch 1589-Loss:     2.6525\n",
      "Epoch 1590-Loss:     2.6192\n",
      "Epoch 1591-Loss:     2.6854\n",
      "Epoch 1592-Loss:     2.7668\n",
      "Epoch 1593-Loss:     2.7636\n",
      "Epoch 1594-Loss:     2.8389\n",
      "Epoch 1595-Loss:     2.8612\n",
      "Epoch 1596-Loss:     2.8629\n",
      "Epoch 1597-Loss:     2.8466\n",
      "Epoch 1598-Loss:     2.8228\n",
      "Epoch 1599-Loss:     2.7802\n",
      "Epoch 1600-Loss:     2.6327\n",
      "Epoch 1601-Loss:     2.6729\n",
      "Epoch 1602-Loss:     2.7141\n",
      "Epoch 1603-Loss:     2.6497\n",
      "Epoch 1604-Loss:     2.6148\n",
      "Epoch 1605-Loss:     2.5990\n",
      "Epoch 1606-Loss:     2.5960\n",
      "Epoch 1607-Loss:     2.6455\n",
      "Epoch 1608-Loss:     2.6863\n",
      "Epoch 1609-Loss:     2.7556\n",
      "Epoch 1610-Loss:     2.7587\n",
      "Epoch 1611-Loss:     2.7810\n",
      "Epoch 1612-Loss:     2.7809\n",
      "Epoch 1613-Loss:     2.7463\n",
      "Epoch 1614-Loss:     2.6325\n",
      "Epoch 1615-Loss:     2.5917\n",
      "Epoch 1616-Loss:     2.5648\n",
      "Epoch 1617-Loss:     2.6136\n",
      "Epoch 1618-Loss:     2.6229\n",
      "Epoch 1619-Loss:     2.6860\n",
      "Epoch 1620-Loss:     2.7042\n",
      "Epoch 1621-Loss:     2.7527\n",
      "Epoch 1622-Loss:     2.7924\n",
      "Epoch 1623-Loss:     2.8140\n",
      "Epoch 1624-Loss:     2.7054\n",
      "Epoch 1625-Loss:     2.6973\n",
      "Epoch 1626-Loss:     2.6490\n",
      "Epoch 1627-Loss:     2.7213\n",
      "Epoch 1628-Loss:     2.7143\n",
      "Epoch 1629-Loss:     2.6743\n",
      "Epoch 1630-Loss:     2.7673\n",
      "Epoch 1631-Loss:     2.8337\n",
      "Epoch 1632-Loss:     2.7863\n",
      "Epoch 1633-Loss:     3.0231\n",
      "Epoch 1634-Loss:     2.8210\n",
      "Epoch 1635-Loss:     2.7126\n",
      "Epoch 1636-Loss:     2.8196\n",
      "Epoch 1637-Loss:     2.7175\n",
      "Epoch 1638-Loss:     2.6797\n",
      "Epoch 1639-Loss:     2.7209\n",
      "Epoch 1640-Loss:     2.8376\n",
      "Epoch 1641-Loss:     2.8248\n",
      "Epoch 1642-Loss:     2.8603\n",
      "Epoch 1643-Loss:     2.7535\n",
      "Epoch 1644-Loss:     2.5945\n",
      "Epoch 1645-Loss:     2.6920\n",
      "Epoch 1646-Loss:     2.7345\n",
      "Epoch 1647-Loss:     2.7991\n",
      "Epoch 1648-Loss:     2.8027\n",
      "Epoch 1649-Loss:     2.8679\n",
      "Epoch 1650-Loss:     2.8078\n",
      "Epoch 1651-Loss:     2.7297\n",
      "Epoch 1652-Loss:     2.7033\n",
      "Epoch 1653-Loss:     2.7918\n",
      "Epoch 1654-Loss:     2.7210\n",
      "Epoch 1655-Loss:     2.8147\n",
      "Epoch 1656-Loss:     2.8084\n",
      "Epoch 1657-Loss:     2.8539\n",
      "Epoch 1658-Loss:     2.7568\n",
      "Epoch 1659-Loss:     2.7101\n",
      "Epoch 1660-Loss:     2.6850\n",
      "Epoch 1661-Loss:     2.5912\n",
      "Epoch 1662-Loss:     2.6479\n",
      "Epoch 1663-Loss:     2.6149\n",
      "Epoch 1664-Loss:     2.6221\n",
      "Epoch 1665-Loss:     2.6658\n",
      "Epoch 1666-Loss:     2.6815\n",
      "Epoch 1667-Loss:     2.6779\n",
      "Epoch 1668-Loss:     2.7238\n",
      "Epoch 1669-Loss:     2.6841\n",
      "Epoch 1670-Loss:     2.7248\n",
      "Epoch 1671-Loss:     2.7508\n",
      "Epoch 1672-Loss:     2.6859\n",
      "Epoch 1673-Loss:     2.7114\n",
      "Epoch 1674-Loss:     2.6900\n",
      "Epoch 1675-Loss:     2.7306\n",
      "Epoch 1676-Loss:     2.7686\n",
      "Epoch 1677-Loss:     2.7457\n",
      "Epoch 1678-Loss:     2.7177\n",
      "Epoch 1679-Loss:     2.6539\n",
      "Epoch 1680-Loss:     2.7196\n",
      "Epoch 1681-Loss:     2.6353\n",
      "Epoch 1682-Loss:     2.6078\n",
      "Epoch 1683-Loss:     2.7217\n",
      "Epoch 1684-Loss:     2.7085\n",
      "Epoch 1685-Loss:     2.7777\n",
      "Epoch 1686-Loss:     2.8563\n",
      "Epoch 1687-Loss:     2.8430\n",
      "Epoch 1688-Loss:     2.8645\n",
      "Epoch 1689-Loss:     2.7325\n",
      "Epoch 1690-Loss:     2.7127\n",
      "Epoch 1691-Loss:     2.7605\n",
      "Epoch 1692-Loss:     2.7207\n",
      "Epoch 1693-Loss:     2.8141\n",
      "Epoch 1694-Loss:     2.6912\n",
      "Epoch 1695-Loss:     2.7319\n",
      "Epoch 1696-Loss:     2.8515\n",
      "Epoch 1697-Loss:     2.7986\n",
      "Epoch 1698-Loss:     2.7899\n",
      "Epoch 1699-Loss:     2.8321\n",
      "Epoch 1700-Loss:     2.7417\n",
      "Epoch 1701-Loss:     2.7631\n",
      "Epoch 1702-Loss:     2.6357\n",
      "Epoch 1703-Loss:     2.8110\n",
      "Epoch 1704-Loss:     2.6391\n",
      "Epoch 1705-Loss:     2.6683\n",
      "Epoch 1706-Loss:     2.7288\n",
      "Epoch 1707-Loss:     2.6642\n",
      "Epoch 1708-Loss:     2.9001\n",
      "Epoch 1709-Loss:     2.7014\n",
      "Epoch 1710-Loss:     2.7183\n",
      "Epoch 1711-Loss:     2.7579\n",
      "Epoch 1712-Loss:     2.8890\n",
      "Epoch 1713-Loss:     2.8988\n",
      "Epoch 1714-Loss:     2.9840\n",
      "Epoch 1715-Loss:     2.9710\n",
      "Epoch 1716-Loss:     2.9124\n",
      "Epoch 1717-Loss:     3.0068\n",
      "Epoch 1718-Loss:     2.9852\n",
      "Epoch 1719-Loss:     2.8669\n",
      "Epoch 1720-Loss:     2.8283\n",
      "Epoch 1721-Loss:     2.8413\n",
      "Epoch 1722-Loss:     2.8179\n",
      "Epoch 1723-Loss:     2.8447\n",
      "Epoch 1724-Loss:     2.8921\n",
      "Epoch 1725-Loss:     2.9927\n",
      "Epoch 1726-Loss:     3.1919\n",
      "Epoch 1727-Loss:     3.1306\n",
      "Epoch 1728-Loss:     3.0083\n",
      "Epoch 1729-Loss:     2.9809\n",
      "Epoch 1730-Loss:     3.1108\n",
      "Epoch 1731-Loss:     3.0555\n",
      "Epoch 1732-Loss:     3.0028\n",
      "Epoch 1733-Loss:     2.9794\n",
      "Epoch 1734-Loss:     2.8425\n",
      "Epoch 1735-Loss:     2.7724\n",
      "Epoch 1736-Loss:     2.8984\n",
      "Epoch 1737-Loss:     2.8422\n",
      "Epoch 1738-Loss:     2.8550\n",
      "Epoch 1739-Loss:     2.9577\n",
      "Epoch 1740-Loss:     2.9864\n",
      "Epoch 1741-Loss:     2.9546\n",
      "Epoch 1742-Loss:     3.0052\n",
      "Epoch 1743-Loss:     3.0139\n",
      "Epoch 1744-Loss:     2.9305\n",
      "Epoch 1745-Loss:     2.8353\n",
      "Epoch 1746-Loss:     2.9887\n",
      "Epoch 1747-Loss:     2.9869\n",
      "Epoch 1748-Loss:     3.0222\n",
      "Epoch 1749-Loss:     2.8888\n",
      "Epoch 1750-Loss:     2.9392\n",
      "Epoch 1751-Loss:     2.9680\n",
      "Epoch 1752-Loss:     2.9895\n",
      "Epoch 1753-Loss:     2.8961\n",
      "Epoch 1754-Loss:     2.8028\n",
      "Epoch 1755-Loss:     2.8417\n",
      "Epoch 1756-Loss:     2.8100\n",
      "Epoch 1757-Loss:     2.7434\n",
      "Epoch 1758-Loss:     2.7112\n",
      "Epoch 1759-Loss:     2.6002\n",
      "Epoch 1760-Loss:     2.6403\n",
      "Epoch 1761-Loss:     2.6991\n",
      "Epoch 1762-Loss:     2.6777\n",
      "Epoch 1763-Loss:     2.6252\n",
      "Epoch 1764-Loss:     2.6500\n",
      "Epoch 1765-Loss:     2.5529\n",
      "Epoch 1766-Loss:     2.5303\n",
      "Epoch 1767-Loss:     2.6024\n",
      "Epoch 1768-Loss:     2.5858\n",
      "Epoch 1769-Loss:     2.6592\n",
      "Epoch 1770-Loss:     2.7312\n",
      "Epoch 1771-Loss:     2.7089\n",
      "Epoch 1772-Loss:     2.7412\n",
      "Epoch 1773-Loss:     2.7410\n",
      "Epoch 1774-Loss:     2.7687\n",
      "Epoch 1775-Loss:     2.7790\n",
      "Epoch 1776-Loss:     2.7154\n",
      "Epoch 1777-Loss:     2.7859\n",
      "Epoch 1778-Loss:     2.7518\n",
      "Epoch 1779-Loss:     2.7069\n",
      "Epoch 1780-Loss:     2.7115\n",
      "Epoch 1781-Loss:     2.7114\n",
      "Epoch 1782-Loss:     2.6614\n",
      "Epoch 1783-Loss:     2.8389\n",
      "Epoch 1784-Loss:     2.7489\n",
      "Epoch 1785-Loss:     2.7807\n",
      "Epoch 1786-Loss:     2.8052\n",
      "Epoch 1787-Loss:     2.8848\n",
      "Epoch 1788-Loss:     2.9472\n",
      "Epoch 1789-Loss:     2.8547\n",
      "Epoch 1790-Loss:     2.8137\n",
      "Epoch 1791-Loss:     2.8965\n",
      "Epoch 1792-Loss:     2.8394\n",
      "Epoch 1793-Loss:     2.8693\n",
      "Epoch 1794-Loss:     2.8289\n",
      "Epoch 1795-Loss:     2.7359\n",
      "Epoch 1796-Loss:     2.6628\n",
      "Epoch 1797-Loss:     2.6444\n",
      "Epoch 1798-Loss:     2.6456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1799-Loss:     2.6479\n",
      "Epoch 1800-Loss:     2.6632\n",
      "Epoch 1801-Loss:     2.6946\n",
      "Epoch 1802-Loss:     2.6910\n",
      "Epoch 1803-Loss:     2.6463\n",
      "Epoch 1804-Loss:     2.6962\n",
      "Epoch 1805-Loss:     2.7138\n",
      "Epoch 1806-Loss:     2.7142\n",
      "Epoch 1807-Loss:     2.6547\n",
      "Epoch 1808-Loss:     2.6605\n",
      "Epoch 1809-Loss:     2.6597\n",
      "Epoch 1810-Loss:     2.6766\n",
      "Epoch 1811-Loss:     2.5475\n",
      "Epoch 1812-Loss:     2.5865\n",
      "Epoch 1813-Loss:     2.6413\n",
      "Epoch 1814-Loss:     2.7126\n",
      "Epoch 1815-Loss:     2.9140\n",
      "Epoch 1816-Loss:     2.8818\n",
      "Epoch 1817-Loss:     2.7345\n",
      "Epoch 1818-Loss:     2.7706\n",
      "Epoch 1819-Loss:     2.6949\n",
      "Epoch 1820-Loss:     2.8317\n",
      "Epoch 1821-Loss:     2.8032\n",
      "Epoch 1822-Loss:     2.6657\n",
      "Epoch 1823-Loss:     2.6194\n",
      "Epoch 1824-Loss:     2.6853\n",
      "Epoch 1825-Loss:     2.7498\n",
      "Epoch 1826-Loss:     2.7055\n",
      "Epoch 1827-Loss:     2.8512\n",
      "Epoch 1828-Loss:     2.8973\n",
      "Epoch 1829-Loss:     3.0027\n",
      "Epoch 1830-Loss:     2.8561\n",
      "Epoch 1831-Loss:     2.8469\n",
      "Epoch 1832-Loss:     2.7810\n",
      "Epoch 1833-Loss:     2.8090\n",
      "Epoch 1834-Loss:     2.7479\n",
      "Epoch 1835-Loss:     2.7330\n",
      "Epoch 1836-Loss:     2.7346\n",
      "Epoch 1837-Loss:     2.6631\n",
      "Epoch 1838-Loss:     2.6239\n",
      "Epoch 1839-Loss:     2.6266\n",
      "Epoch 1840-Loss:     2.5469\n",
      "Epoch 1841-Loss:     2.6101\n",
      "Epoch 1842-Loss:     2.6050\n",
      "Epoch 1843-Loss:     2.6088\n",
      "Epoch 1844-Loss:     2.6623\n",
      "Epoch 1845-Loss:     2.6609\n",
      "Epoch 1846-Loss:     2.7765\n",
      "Epoch 1847-Loss:     2.7672\n",
      "Epoch 1848-Loss:     2.7778\n",
      "Epoch 1849-Loss:     2.8259\n",
      "Epoch 1850-Loss:     2.8243\n",
      "Epoch 1851-Loss:     2.7880\n",
      "Epoch 1852-Loss:     2.6945\n",
      "Epoch 1853-Loss:     2.7005\n",
      "Epoch 1854-Loss:     2.6716\n",
      "Epoch 1855-Loss:     2.7252\n",
      "Epoch 1856-Loss:     2.7386\n",
      "Epoch 1857-Loss:     2.6793\n",
      "Epoch 1858-Loss:     2.6893\n",
      "Epoch 1859-Loss:     2.6541\n",
      "Epoch 1860-Loss:     2.6562\n",
      "Epoch 1861-Loss:     2.6951\n",
      "Epoch 1862-Loss:     2.6760\n",
      "Epoch 1863-Loss:     2.6925\n",
      "Epoch 1864-Loss:     2.7392\n",
      "Epoch 1865-Loss:     2.7535\n",
      "Epoch 1866-Loss:     2.6124\n",
      "Epoch 1867-Loss:     2.7163\n",
      "Epoch 1868-Loss:     2.6786\n",
      "Epoch 1869-Loss:     2.6681\n",
      "Epoch 1870-Loss:     2.6548\n",
      "Epoch 1871-Loss:     2.6789\n",
      "Epoch 1872-Loss:     2.6712\n",
      "Epoch 1873-Loss:     2.7521\n",
      "Epoch 1874-Loss:     2.7127\n",
      "Epoch 1875-Loss:     2.6989\n",
      "Epoch 1876-Loss:     2.7153\n",
      "Epoch 1877-Loss:     2.5996\n",
      "Epoch 1878-Loss:     2.5913\n",
      "Epoch 1879-Loss:     2.6961\n",
      "Epoch 1880-Loss:     2.7120\n",
      "Epoch 1881-Loss:     2.6952\n",
      "Epoch 1882-Loss:     2.7949\n",
      "Epoch 1883-Loss:     2.8739\n",
      "Epoch 1884-Loss:     2.8932\n",
      "Epoch 1885-Loss:     2.8708\n",
      "Epoch 1886-Loss:     2.8069\n",
      "Epoch 1887-Loss:     2.7307\n",
      "Epoch 1888-Loss:     2.6943\n",
      "Epoch 1889-Loss:     2.8018\n",
      "Epoch 1890-Loss:     2.9209\n",
      "Epoch 1891-Loss:     2.7395\n",
      "Epoch 1892-Loss:     2.7823\n",
      "Epoch 1893-Loss:     2.8370\n",
      "Epoch 1894-Loss:     2.8883\n",
      "Epoch 1895-Loss:     2.9175\n",
      "Epoch 1896-Loss:     2.9448\n",
      "Epoch 1897-Loss:     2.9835\n",
      "Epoch 1898-Loss:     2.9409\n",
      "Epoch 1899-Loss:     2.9837\n",
      "Epoch 1900-Loss:     3.0495\n",
      "Epoch 1901-Loss:     2.8846\n",
      "Epoch 1902-Loss:     2.8277\n",
      "Epoch 1903-Loss:     2.9627\n",
      "Epoch 1904-Loss:     2.9327\n",
      "Epoch 1905-Loss:     2.9466\n",
      "Epoch 1906-Loss:     2.9559\n",
      "Epoch 1907-Loss:     2.9105\n",
      "Epoch 1908-Loss:     2.9956\n",
      "Epoch 1909-Loss:     2.9911\n",
      "Epoch 1910-Loss:     2.9549\n",
      "Epoch 1911-Loss:     2.9481\n",
      "Epoch 1912-Loss:     2.9176\n",
      "Epoch 1913-Loss:     2.8421\n",
      "Epoch 1914-Loss:     2.7073\n",
      "Epoch 1915-Loss:     2.7410\n",
      "Epoch 1916-Loss:     2.6880\n",
      "Epoch 1917-Loss:     2.7401\n",
      "Epoch 1918-Loss:     2.7719\n",
      "Epoch 1919-Loss:     3.1424\n",
      "Epoch 1920-Loss:     2.9063\n",
      "Epoch 1921-Loss:     2.8827\n",
      "Epoch 1922-Loss:     2.8444\n",
      "Epoch 1923-Loss:     2.8342\n",
      "Epoch 1924-Loss:     2.7912\n",
      "Epoch 1925-Loss:     2.7204\n",
      "Epoch 1926-Loss:     2.7498\n",
      "Epoch 1927-Loss:     2.7869\n",
      "Epoch 1928-Loss:     2.7947\n",
      "Epoch 1929-Loss:     2.8398\n",
      "Epoch 1930-Loss:     2.8864\n",
      "Epoch 1931-Loss:     2.8589\n",
      "Epoch 1932-Loss:     2.8431\n",
      "Epoch 1933-Loss:     2.8944\n",
      "Epoch 1934-Loss:     2.8735\n",
      "Epoch 1935-Loss:     2.7834\n",
      "Epoch 1936-Loss:     2.8657\n",
      "Epoch 1937-Loss:     2.9015\n",
      "Epoch 1938-Loss:     2.8817\n",
      "Epoch 1939-Loss:     2.9061\n",
      "Epoch 1940-Loss:     2.8996\n",
      "Epoch 1941-Loss:     2.9193\n",
      "Epoch 1942-Loss:     3.0179\n",
      "Epoch 1943-Loss:     3.1595\n",
      "Epoch 1944-Loss:     3.1123\n",
      "Epoch 1945-Loss:     2.9679\n",
      "Epoch 1946-Loss:     2.8416\n",
      "Epoch 1947-Loss:     2.7891\n",
      "Epoch 1948-Loss:     2.8734\n",
      "Epoch 1949-Loss:     2.7911\n",
      "Epoch 1950-Loss:     2.7990\n",
      "Epoch 1951-Loss:     2.8261\n",
      "Epoch 1952-Loss:     2.7306\n",
      "Epoch 1953-Loss:     2.8025\n",
      "Epoch 1954-Loss:     2.7680\n",
      "Epoch 1955-Loss:     2.7288\n",
      "Epoch 1956-Loss:     2.7923\n",
      "Epoch 1957-Loss:     2.7414\n",
      "Epoch 1958-Loss:     2.7301\n",
      "Epoch 1959-Loss:     2.7673\n",
      "Epoch 1960-Loss:     2.8251\n",
      "Epoch 1961-Loss:     2.8670\n",
      "Epoch 1962-Loss:     2.7977\n",
      "Epoch 1963-Loss:     2.7369\n",
      "Epoch 1964-Loss:     2.7792\n",
      "Epoch 1965-Loss:     2.8179\n",
      "Epoch 1966-Loss:     2.8924\n",
      "Epoch 1967-Loss:     2.8749\n",
      "Epoch 1968-Loss:     2.9794\n",
      "Epoch 1969-Loss:     2.9991\n",
      "Epoch 1970-Loss:     2.8351\n",
      "Epoch 1971-Loss:     2.8313\n",
      "Epoch 1972-Loss:     2.9436\n",
      "Epoch 1973-Loss:     2.8633\n",
      "Epoch 1974-Loss:     2.7025\n",
      "Epoch 1975-Loss:     2.6130\n",
      "Epoch 1976-Loss:     2.6191\n",
      "Epoch 1977-Loss:     2.6837\n",
      "Epoch 1978-Loss:     2.6784\n",
      "Epoch 1979-Loss:     2.6947\n",
      "Epoch 1980-Loss:     2.6139\n",
      "Epoch 1981-Loss:     2.6648\n",
      "Epoch 1982-Loss:     2.7593\n",
      "Epoch 1983-Loss:     2.9681\n",
      "Epoch 1984-Loss:     2.9251\n",
      "Epoch 1985-Loss:     3.2264\n",
      "Epoch 1986-Loss:     3.0860\n",
      "Epoch 1987-Loss:     2.8220\n",
      "Epoch 1988-Loss:     2.8675\n",
      "Epoch 1989-Loss:     2.7801\n",
      "Epoch 1990-Loss:     2.7725\n",
      "Epoch 1991-Loss:     2.7609\n",
      "Epoch 1992-Loss:     2.7495\n",
      "Epoch 1993-Loss:     2.7800\n",
      "Epoch 1994-Loss:     2.7589\n",
      "Epoch 1995-Loss:     2.7356\n",
      "Epoch 1996-Loss:     2.7643\n",
      "Epoch 1997-Loss:     2.8246\n",
      "Epoch 1998-Loss:     3.1000\n",
      "Epoch 1999-Loss:     3.0177\n",
      "Epoch 2000-Loss:     3.0494\n",
      "Testing Accuracy: 0.6349310278892517\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "# changed lr: 0.1, 0.01, 0.001, 0.0001\\\n",
    "# changed epochs: 100, 1000, 10000, 100000\n",
    "# best result from lr: 0.01, ep: 1000\n",
    "learning_rate = 0.0005\n",
    "epochs = 2000\n",
    "\n",
    "# network Parameters\n",
    "n_classes = 14\n",
    "dropout = 0.8\n",
    "\n",
    "seed = 123\n",
    "\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 9, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 32, 64])),\n",
    "    'wc3': tf.Variable(tf.random_normal([5, 64, 128])),\n",
    "    'wd1': tf.Variable(tf.random_normal([10*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bc3': tf.Variable(tf.random_normal([128])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "# changed relu to sigmoid\n",
    "# (best) result from reLu \n",
    "def conv1d(x, W, b, s):\n",
    "    x = tf.nn.conv1d(x, W, s, padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Conv Layer 1\n",
    "    conv1 = conv1d(x, weights['wc1'], biases['bc1'], 5)\n",
    "\n",
    "    # Conv Layer 2\n",
    "    conv2 = conv1d(conv1, weights['wc2'], biases['bc2'], 5)\n",
    "    \n",
    "    #Conv Layer 3\n",
    "    # conv3 = conv1d(conv2, weights['wc3'], biases['bc3'], 1)\n",
    "    \n",
    "\n",
    "    # Fully connected layer \n",
    "    # Changed relu to sigmoid\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.sigmoid(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction \n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 250, 9])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(optimizer, feed_dict={\n",
    "            x: f_train,\n",
    "            y: l_train.eval(),\n",
    "            keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "        loss = sess.run(cost, feed_dict={\n",
    "            x: f_train,\n",
    "            y: l_train.eval(),\n",
    "            keep_prob: 1.})\n",
    "\n",
    "        print('Epoch {:>2}-'\n",
    "            'Loss: {:>10.4f}'.format(\n",
    "            epoch + 1,\n",
    "            loss))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: f_test,\n",
    "        y: l_test.eval(),\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(activation_function, conv_layers, learning_rate, epochs, dropout, n_filters, seed):\n",
    "    # network Parameters\n",
    "    n_classes = 14\n",
    "\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "    if conv_layers == 2:\n",
    "        weights = {\n",
    "            'wc1': tf.Variable(tf.random_normal([n_filters, 9, 32])),\n",
    "            'wc2': tf.Variable(tf.random_normal([n_filters, 32, 64])),\n",
    "            'wc3': tf.Variable(tf.random_normal([n_filters, 64, 128])),\n",
    "            'wd1': tf.Variable(tf.random_normal([10*64, 1024])),\n",
    "            'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "    elif conv_layers == 3:\n",
    "        weights = {\n",
    "            'wc1': tf.Variable(tf.random_normal([n_filters, 9, 32])),\n",
    "            'wc2': tf.Variable(tf.random_normal([n_filters, 32, 64])),\n",
    "            'wc3': tf.Variable(tf.random_normal([n_filters, 64, 128])),\n",
    "            'wd1': tf.Variable(tf.random_normal([10*128, 1024])),\n",
    "            'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.random_normal([32])),\n",
    "        'bc2': tf.Variable(tf.random_normal([64])),\n",
    "        'bc3': tf.Variable(tf.random_normal([128])),\n",
    "        'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    # changed relu to sigmoid\n",
    "    # (best) result from reLu \n",
    "    def conv1d(x, W, b, s):\n",
    "        x = tf.nn.conv1d(x, W, s, padding='SAME')\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "    if conv_layers == 2:\n",
    "        def conv_net(x, weights, biases, dropout, activation_function):\n",
    "            # Conv Layer 1\n",
    "            conv1 = conv1d(x, weights['wc1'], biases['bc1'], 5)\n",
    "\n",
    "            # Conv Layer 2\n",
    "            conv2 = conv1d(conv1, weights['wc2'], biases['bc2'], 5)\n",
    "    \n",
    "            #Conv Layer 3\n",
    "            # conv3 = conv1d(conv2, weights['wc3'], biases['bc3'], 1)\n",
    "    \n",
    "\n",
    "            # Fully connected layer \n",
    "            # Changed relu to sigmoid\n",
    "            fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "            fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "            fc1 = activation_function(fc1)\n",
    "            fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "            # Output Layer - class prediction \n",
    "            out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "            return out\n",
    "    elif conv_layers == 3:\n",
    "        def conv_net(x, weights, biases, dropout, activation_function):\n",
    "            # Conv Layer 1\n",
    "            conv1 = conv1d(x, weights['wc1'], biases['bc1'], 5)\n",
    "\n",
    "            # Conv Layer 2\n",
    "            conv2 = conv1d(conv1, weights['wc2'], biases['bc2'], 5)\n",
    "    \n",
    "            #Conv Layer 3\n",
    "            conv3 = conv1d(conv2, weights['wc3'], biases['bc3'], 1)\n",
    "    \n",
    "\n",
    "            # Fully connected layer \n",
    "            # Changed relu to sigmoid\n",
    "            fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "            fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "            fc1 = activation_function(fc1)\n",
    "            fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "            # Output Layer - class prediction \n",
    "            out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "            return out\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 250, 9])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Model\n",
    "    logits = conv_net(x, weights, biases, keep_prob, activation_function)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: f_train,\n",
    "                y: l_train.eval(),\n",
    "                keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: f_train,\n",
    "                y: l_train.eval(),\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print('Epoch {:>2} - '\n",
    "                    'Loss: {:>10.4f}'.format(\n",
    "                    epoch + 1,\n",
    "                    loss))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "        test_acc = sess.run(accuracy, feed_dict={\n",
    "            x: f_test,\n",
    "            y: l_test.eval(),\n",
    "            keep_prob: 1.})\n",
    "        print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 - Loss:    45.6232\n",
      "Epoch 101 - Loss:     8.5770\n",
      "Epoch 201 - Loss:     6.2645\n",
      "Epoch 301 - Loss:     4.8543\n",
      "Epoch 401 - Loss:     4.1534\n",
      "Epoch 501 - Loss:     3.8986\n",
      "Epoch 601 - Loss:     3.4219\n",
      "Epoch 701 - Loss:     3.5755\n",
      "Epoch 801 - Loss:     3.4659\n",
      "Epoch 901 - Loss:     3.6292\n",
      "Epoch 1001 - Loss:     3.4280\n",
      "Epoch 1101 - Loss:     3.3295\n",
      "Epoch 1201 - Loss:     3.2437\n",
      "Epoch 1301 - Loss:     2.9707\n",
      "Epoch 1401 - Loss:     2.9898\n",
      "Epoch 1501 - Loss:     3.0429\n",
      "Epoch 1601 - Loss:     2.8173\n",
      "Epoch 1701 - Loss:     2.8700\n",
      "Epoch 1801 - Loss:     2.8705\n",
      "Epoch 1901 - Loss:     2.8649\n"
     ]
    }
   ],
   "source": [
    "neural_net(tf.nn.sigmoid, 2, 0.0005, 2000, 0.8, 5, 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net(tf.nn.sigmoid, 2, 0.0005, 2000, 0.8, 5, 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
