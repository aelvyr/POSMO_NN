{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#col_array = ['label', 'label2' 'label_id', 'speed', 'heading_x', 'heading_y', 'heading_z', 'acceleration_x', 'acceleration_y', 'acceleration_z', 'gyroscope_x', 'gyroscope_y', 'gyroscope_z', 'magnetic_field_x', 'magnetic_field_y', 'magnetic_field_z', 'pressure_hPa', 'log_timestamp']\n",
    "col_array = ['label_id', 'acceleration_x', 'acceleration_y', 'acceleration_z', 'gyroscope_x', 'gyroscope_y', 'gyroscope_z', 'magnetic_field_x', 'magnetic_field_y', 'magnetic_field_z']\n",
    "path = \"../posmo/experiments/data/posmo_segments/v8/\" \n",
    "def read_values(csv, col_array, path=\"../posmo/experiments/data/posmo_segments/v8/\"):\n",
    "    return pd.read_csv(path + csv).loc[:, col_array] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = []\n",
    "for fname in glob.glob(\"../posmo/experiments/data/posmo_segments/v8/*.csv\"):\n",
    "    files.append(fname)\n",
    "for file in files:\n",
    "    if file in glob.glob(\"../posmo/experiments/data/posmo_segments/v8/Other*\"):\n",
    "        files.remove(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbike1 = read_values(\"Bike-20180425T600 PM-aelvyr@gmail.com-8DD36F40-3778-454F-A8D8-E330C6F39830.csv\", col_array)\\nbike2 = read_values(\"Bike-20180426T518 PM-aelvyr@gmail.com-33A201D1-213B-4B6B-A555-84A9CF015E87.csv\", col_array)\\nbike3 = read_values(\"Bike-20180426T527 PM-aelvyr@gmail.com-8623C55E-8B27-4D36-93C5-7F91F53CFF6C.csv\", col_array)\\nbike4 = read_values(\"Bike-20180427T149 PM-aelvyr@gmail.com-1222C4CE-8523-45AD-A22B-030C80EEEDDC.csv\", col_array)\\nbike5 = read_values(\"Bike-20180427T1218 AM-aelvyr@gmail.com-09521095-A9D3-4662-A914-7DCBC1422947.csv\", col_array)\\nbus1 = read_values(\"Bus-20180423T0929-djordje@datamap.io-F068A314-DC4D-4FB0-9D97-E79929993E41.csv\", col_array)\\nbus2 = read_values(\"Bus-20180423T2150-djordje@datamap.io-DEB56810-DB1E-4E14-A144-130B52E0F405.csv\", col_array)\\ncar1 = read_values(\"Car-20180420T0000-djordje@datamap.io-B2876225-A37C-4A72-B045-790E5437B114.csv\", col_array)\\ncar2 = read_values(\"Car-20180420T0912-djordje@datamap.io-37B35625-AAF7-4217-9535-38D6BCF3FF93.csv\", col_array)\\ncar3 = read_values(\"Car-20180420T1328-roger@datamap.io-A3DB32B6-A6AC-4BD9-88EF-8AFFCFCCEDC1.csv\", col_array)\\nflat1 = read_values(\"Other-Flat-20180424T1445-roger@datamap.io-1A6D2916-7C62-48D4-8CB9-9CBBA9FD7E20.csv\", col_array)\\nflat2 = read_values(\"Other-flat-reversed-20180424T1538-roger@datamap.io-7419A222-F45B-415A-A3DF-08691E85B99A.csv\", col_array)\\nstanding = read_values(\"Other-InPocket-20180424T1446-roger@datamap.io-6DA03A16-DE1F-4508-BFC8-E8403F621954.csv\", col_array)\\nscooter = read_values(\"Other-Scooter-20180422T2049-roger@datamap.io-0C33D54C-1B7B-4C45-92D3-2F89A7A82C16.csv\", col_array)\\ntram1 = read_values(\"Tram-20180426T1825-roger@datamap.io-7C72A146-5733-446A-8418-5126D1BB74DE.csv\", col_array)\\ntram2 = read_values(\"Tram-20180427T1806-roger@datamap.io-9D3D94ED-4E6B-4382-87A0-0F9A3BA4AA82.csv\", col_array)\\ntrain = read_values(\"Train-20180420T1731-roger@datamap.io-E15B0F2F-910B-49CC-A96D-D54276492C97.csv\", col_array)\\nwalk1 = read_values(\"Walk-20180418T1004-roger@datamap.io-D7CA4A5B-0BDC-4E28-9BF4-3DD46B6F91AC.csv\", col_array)\\nwalk2 = read_values(\"Walk-20180418T1507-roger@datamap.io-F3AD9D9D-6B43-4AD1-885B-EAFF3E531A25.csv\", col_array)\\nwalk3 = read_values(\"Walk-20180418T1753-roger@datamap.io-AE01B243-595E-4F19-BAE3-8068038A1AE0.csv\", col_array)\\nwalk4 = read_values(\"Walk-20180419T0011-roger@datamap.io-50A0AB4B-4DB1-4231-8BA6-179DF3EFA381.csv\", col_array)\\nwalk5 = read_values(\"Walk-20180419T1025-roger@datamap.io-9F5D6118-143B-4625-8CD2-A94C5E7E62A0.csv\", col_array)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bike1 = read_values(\"Bike-20180425T600 PM-aelvyr@gmail.com-8DD36F40-3778-454F-A8D8-E330C6F39830.csv\", col_array)\n",
    "bike2 = read_values(\"Bike-20180426T518 PM-aelvyr@gmail.com-33A201D1-213B-4B6B-A555-84A9CF015E87.csv\", col_array)\n",
    "bike3 = read_values(\"Bike-20180426T527 PM-aelvyr@gmail.com-8623C55E-8B27-4D36-93C5-7F91F53CFF6C.csv\", col_array)\n",
    "bike4 = read_values(\"Bike-20180427T149 PM-aelvyr@gmail.com-1222C4CE-8523-45AD-A22B-030C80EEEDDC.csv\", col_array)\n",
    "bike5 = read_values(\"Bike-20180427T1218 AM-aelvyr@gmail.com-09521095-A9D3-4662-A914-7DCBC1422947.csv\", col_array)\n",
    "bus1 = read_values(\"Bus-20180423T0929-djordje@datamap.io-F068A314-DC4D-4FB0-9D97-E79929993E41.csv\", col_array)\n",
    "bus2 = read_values(\"Bus-20180423T2150-djordje@datamap.io-DEB56810-DB1E-4E14-A144-130B52E0F405.csv\", col_array)\n",
    "car1 = read_values(\"Car-20180420T0000-djordje@datamap.io-B2876225-A37C-4A72-B045-790E5437B114.csv\", col_array)\n",
    "car2 = read_values(\"Car-20180420T0912-djordje@datamap.io-37B35625-AAF7-4217-9535-38D6BCF3FF93.csv\", col_array)\n",
    "car3 = read_values(\"Car-20180420T1328-roger@datamap.io-A3DB32B6-A6AC-4BD9-88EF-8AFFCFCCEDC1.csv\", col_array)\n",
    "flat1 = read_values(\"Other-Flat-20180424T1445-roger@datamap.io-1A6D2916-7C62-48D4-8CB9-9CBBA9FD7E20.csv\", col_array)\n",
    "flat2 = read_values(\"Other-flat-reversed-20180424T1538-roger@datamap.io-7419A222-F45B-415A-A3DF-08691E85B99A.csv\", col_array)\n",
    "standing = read_values(\"Other-InPocket-20180424T1446-roger@datamap.io-6DA03A16-DE1F-4508-BFC8-E8403F621954.csv\", col_array)\n",
    "scooter = read_values(\"Other-Scooter-20180422T2049-roger@datamap.io-0C33D54C-1B7B-4C45-92D3-2F89A7A82C16.csv\", col_array)\n",
    "tram1 = read_values(\"Tram-20180426T1825-roger@datamap.io-7C72A146-5733-446A-8418-5126D1BB74DE.csv\", col_array)\n",
    "tram2 = read_values(\"Tram-20180427T1806-roger@datamap.io-9D3D94ED-4E6B-4382-87A0-0F9A3BA4AA82.csv\", col_array)\n",
    "train = read_values(\"Train-20180420T1731-roger@datamap.io-E15B0F2F-910B-49CC-A96D-D54276492C97.csv\", col_array)\n",
    "walk1 = read_values(\"Walk-20180418T1004-roger@datamap.io-D7CA4A5B-0BDC-4E28-9BF4-3DD46B6F91AC.csv\", col_array)\n",
    "walk2 = read_values(\"Walk-20180418T1507-roger@datamap.io-F3AD9D9D-6B43-4AD1-885B-EAFF3E531A25.csv\", col_array)\n",
    "walk3 = read_values(\"Walk-20180418T1753-roger@datamap.io-AE01B243-595E-4F19-BAE3-8068038A1AE0.csv\", col_array)\n",
    "walk4 = read_values(\"Walk-20180419T0011-roger@datamap.io-50A0AB4B-4DB1-4231-8BA6-179DF3EFA381.csv\", col_array)\n",
    "walk5 = read_values(\"Walk-20180419T1025-roger@datamap.io-9F5D6118-143B-4625-8CD2-A94C5E7E62A0.csv\", col_array)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'Bike': 0, 'Bus': 1, 'Car': 2, 'Tram': 3, 'Train': 4, 'Walk': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df, f_train, l_train, f_test, l_test, seq_len=250):\n",
    "    df_np = df.as_matrix()\n",
    "    df_np = df_np[:(df_np.shape[0] - df_np.shape[0] % seq_len), :]\n",
    "    df_np = df_np.reshape(int(df_np.shape[0]/seq_len), seq_len, df_np.shape[1])\n",
    "    for i in range(df_np.shape[0]):\n",
    "        label = df_np[i, 0, 0]\n",
    "        feature = np.expand_dims(df_np[i, :, 1:10], axis=0)\n",
    "        if i % 10 == 0:\n",
    "            l_test = np.append(l_test, label)\n",
    "            f_test = np.concatenate([f_test, feature], axis=0)\n",
    "        else:\n",
    "            l_train = np.append(l_train, label)\n",
    "            f_train = np.concatenate([f_train, feature], axis=0)\n",
    "    #feature = df_np[:, :, 1:18]\n",
    "    #len = feature.shape[0]\n",
    "    #n = int((len - len % 10) / 10)\n",
    "    #feature_test = df_np[:n, :, 1:18]\n",
    "    #feature_train = df_np[n:, :, 1:18]\n",
    "    #f_test = np.concatenate([f_test, feature_test], axis=0)\n",
    "    #f_train = np.concatenate([f_train, feature_train], axis=0)\n",
    "    return f_train, l_train, f_test, l_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_shapes():\n",
    "    return f_train.shape, l_train.shape, f_test.shape, l_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = np.empty([1, 250, 9])\n",
    "l_train = np.array([])\n",
    "f_test = np.empty([1, 250, 9])\n",
    "l_test = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    df = read_values(file, col_array, path=\"\")\n",
    "    f_train, l_train, f_test, l_test = feature_extraction(df, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "f_train, l_train, f_test, l_test = feature_extraction(bike1, f_train, l_train, f_test, l_test)\n",
    "f_train, l_train, f_test, l_test = feature_extraction(bike2, f_train, l_train, f_test, l_test)\n",
    "f_train, l_train, f_test, l_test = feature_extraction(bike3, f_train, l_train, f_test, l_test)\n",
    "f_train, l_train, f_test, l_test = feature_extraction(bike4, f_train, l_train, f_test, l_test)\n",
    "f_train, l_train, f_test, l_test = feature_extraction(bike5, f_train, l_train, f_test, l_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = f_train[1:, :, :]\n",
    "f_test = f_test[1:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8802, 250, 9), (8802,), (1008, 250, 9), (1008,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_train, l_train, f_test, l_test = feature_extraction(bus1, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(bus2, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((573, 250, 9), (573,), (68, 250, 9), (68,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_train, l_train, f_test, l_test = feature_extraction(car1, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(car2, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(car3, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1070, 250, 9), (1070,), (124, 250, 9), (124,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_train, l_train, f_test, l_test = feature_extraction(tram1, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(tram2, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1165, 250, 9), (1165,), (136, 250, 9), (136,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_train, l_train, f_test, l_test = feature_extraction(train, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1260, 250, 9), (1260,), (147, 250, 9), (147,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_train, l_train, f_test, l_test = feature_extraction(walk1, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(walk2, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(walk3, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(walk4, f_train, l_train, f_test, l_test)\n",
    "#f_train, l_train, f_test, l_test = feature_extraction(walk5, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2202, 250, 9), (2202,), (254, 250, 9), (254,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_train = tf.one_hot(l_train, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test = tf.one_hot(l_test, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1-Loss:    20.9706\n",
      "Epoch  2-Loss:    20.3817\n",
      "Epoch  3-Loss:    12.1017\n",
      "Epoch  4-Loss:    14.4091\n",
      "Epoch  5-Loss:     6.7563\n",
      "Epoch  6-Loss:     8.5554\n",
      "Epoch  7-Loss:    13.9862\n",
      "Epoch  8-Loss:     6.8687\n",
      "Epoch  9-Loss:     7.5689\n",
      "Epoch 10-Loss:     9.4599\n",
      "Epoch 11-Loss:    13.0111\n",
      "Epoch 12-Loss:     7.3081\n",
      "Epoch 13-Loss:     7.1247\n",
      "Epoch 14-Loss:     9.1775\n",
      "Epoch 15-Loss:     4.9056\n",
      "Epoch 16-Loss:     9.1195\n",
      "Epoch 17-Loss:     3.7984\n",
      "Epoch 18-Loss:     8.4467\n",
      "Epoch 19-Loss:     4.5952\n",
      "Epoch 20-Loss:     8.1685\n",
      "Epoch 21-Loss:     4.5658\n",
      "Epoch 22-Loss:     7.4332\n",
      "Epoch 23-Loss:     4.7776\n",
      "Epoch 24-Loss:     6.4708\n",
      "Epoch 25-Loss:     5.1449\n",
      "Epoch 26-Loss:     5.1210\n",
      "Epoch 27-Loss:     6.3451\n",
      "Epoch 28-Loss:     5.6209\n",
      "Epoch 29-Loss:     8.5449\n",
      "Epoch 30-Loss:     6.4711\n",
      "Epoch 31-Loss:     5.8846\n",
      "Epoch 32-Loss:     5.4379\n",
      "Epoch 33-Loss:     6.6937\n",
      "Epoch 34-Loss:     6.2181\n",
      "Epoch 35-Loss:    10.3936\n",
      "Epoch 36-Loss:     3.4177\n",
      "Epoch 37-Loss:     7.2254\n",
      "Epoch 38-Loss:     4.6662\n",
      "Epoch 39-Loss:     7.1907\n",
      "Epoch 40-Loss:     5.5284\n",
      "Epoch 41-Loss:     7.3629\n",
      "Epoch 42-Loss:     5.7927\n",
      "Epoch 43-Loss:     7.9826\n",
      "Epoch 44-Loss:     4.6851\n",
      "Epoch 45-Loss:     8.1876\n",
      "Epoch 46-Loss:     3.0158\n",
      "Epoch 47-Loss:     9.5071\n",
      "Epoch 48-Loss:     4.8192\n",
      "Epoch 49-Loss:     6.9603\n",
      "Epoch 50-Loss:     4.9082\n",
      "Epoch 51-Loss:     6.9916\n",
      "Epoch 52-Loss:     4.7327\n",
      "Epoch 53-Loss:     6.8388\n",
      "Epoch 54-Loss:     5.8984\n",
      "Epoch 55-Loss:     6.3531\n",
      "Epoch 56-Loss:     5.4025\n",
      "Epoch 57-Loss:     6.4630\n",
      "Epoch 58-Loss:     5.6251\n",
      "Epoch 59-Loss:     6.8908\n",
      "Epoch 60-Loss:     5.4950\n",
      "Epoch 61-Loss:     6.7606\n",
      "Epoch 62-Loss:     4.5412\n",
      "Epoch 63-Loss:     7.7613\n",
      "Epoch 64-Loss:     5.4562\n",
      "Epoch 65-Loss:     6.5298\n",
      "Epoch 66-Loss:     6.1668\n",
      "Epoch 67-Loss:     4.8073\n",
      "Epoch 68-Loss:     7.5781\n",
      "Epoch 69-Loss:     4.0111\n",
      "Epoch 70-Loss:     8.8336\n",
      "Epoch 71-Loss:     3.9721\n",
      "Epoch 72-Loss:     8.1827\n",
      "Epoch 73-Loss:     3.7436\n",
      "Epoch 74-Loss:     7.5871\n",
      "Epoch 75-Loss:     4.5401\n",
      "Epoch 76-Loss:     7.3001\n",
      "Epoch 77-Loss:     4.6435\n",
      "Epoch 78-Loss:     6.7570\n",
      "Epoch 79-Loss:     4.8045\n",
      "Epoch 80-Loss:     7.0829\n",
      "Epoch 81-Loss:     4.6066\n",
      "Epoch 82-Loss:     6.7489\n",
      "Epoch 83-Loss:     5.0377\n",
      "Epoch 84-Loss:     6.4672\n",
      "Epoch 85-Loss:     5.0295\n",
      "Epoch 86-Loss:     6.6971\n",
      "Epoch 87-Loss:     4.8027\n",
      "Epoch 88-Loss:     6.5445\n",
      "Epoch 89-Loss:     5.2475\n",
      "Epoch 90-Loss:     7.5081\n",
      "Epoch 91-Loss:     4.1922\n",
      "Epoch 92-Loss:     8.1759\n",
      "Epoch 93-Loss:     4.3091\n",
      "Epoch 94-Loss:     7.2808\n",
      "Epoch 95-Loss:     4.5542\n",
      "Epoch 96-Loss:     6.7034\n",
      "Epoch 97-Loss:     4.9264\n",
      "Epoch 98-Loss:     6.2699\n",
      "Epoch 99-Loss:     5.4179\n",
      "Epoch 100-Loss:     6.4757\n",
      "Epoch 101-Loss:     5.4614\n",
      "Epoch 102-Loss:     6.2238\n",
      "Epoch 103-Loss:     4.9853\n",
      "Epoch 104-Loss:     6.6765\n",
      "Epoch 105-Loss:     5.1030\n",
      "Epoch 106-Loss:     6.4576\n",
      "Epoch 107-Loss:     5.3210\n",
      "Epoch 108-Loss:     6.3902\n",
      "Epoch 109-Loss:     5.4113\n",
      "Epoch 110-Loss:     6.1215\n",
      "Epoch 111-Loss:     5.4682\n",
      "Epoch 112-Loss:     6.2642\n",
      "Epoch 113-Loss:     5.6010\n",
      "Epoch 114-Loss:     6.2497\n",
      "Epoch 115-Loss:     5.1373\n",
      "Epoch 116-Loss:     5.9540\n",
      "Epoch 117-Loss:     4.9123\n",
      "Epoch 118-Loss:     6.9557\n",
      "Epoch 119-Loss:     5.6724\n",
      "Epoch 120-Loss:     5.7783\n",
      "Epoch 121-Loss:     6.6477\n",
      "Epoch 122-Loss:     4.4855\n",
      "Epoch 123-Loss:     7.2025\n",
      "Epoch 124-Loss:     3.9933\n",
      "Epoch 125-Loss:     6.9448\n",
      "Epoch 126-Loss:     5.6174\n",
      "Epoch 127-Loss:     5.7352\n",
      "Epoch 128-Loss:     6.3540\n",
      "Epoch 129-Loss:     5.3602\n",
      "Epoch 130-Loss:     6.4256\n",
      "Epoch 131-Loss:     5.3511\n",
      "Epoch 132-Loss:     6.1269\n",
      "Epoch 133-Loss:     5.4513\n",
      "Epoch 134-Loss:     6.2511\n",
      "Epoch 135-Loss:     3.2730\n",
      "Epoch 136-Loss:     8.3265\n",
      "Epoch 137-Loss:     5.5937\n",
      "Epoch 138-Loss:    11.4272\n",
      "Epoch 139-Loss:     2.9172\n",
      "Epoch 140-Loss:     8.1627\n",
      "Epoch 141-Loss:     3.6988\n",
      "Epoch 142-Loss:     7.6102\n",
      "Epoch 143-Loss:     4.2157\n",
      "Epoch 144-Loss:     7.3159\n",
      "Epoch 145-Loss:     3.5643\n",
      "Epoch 146-Loss:     8.2161\n",
      "Epoch 147-Loss:     4.4923\n",
      "Epoch 148-Loss:     7.4324\n",
      "Epoch 149-Loss:     4.1039\n",
      "Epoch 150-Loss:     7.1780\n",
      "Epoch 151-Loss:     4.1654\n",
      "Epoch 152-Loss:     7.7664\n",
      "Epoch 153-Loss:     3.5095\n",
      "Epoch 154-Loss:     8.2308\n",
      "Epoch 155-Loss:     3.7673\n",
      "Epoch 156-Loss:     7.4733\n",
      "Epoch 157-Loss:     3.8092\n",
      "Epoch 158-Loss:     7.5783\n",
      "Epoch 159-Loss:     3.4255\n",
      "Epoch 160-Loss:     8.1413\n",
      "Epoch 161-Loss:     3.5379\n",
      "Epoch 162-Loss:     7.7386\n",
      "Epoch 163-Loss:     3.7034\n",
      "Epoch 164-Loss:     7.6141\n",
      "Epoch 165-Loss:     3.9354\n",
      "Epoch 166-Loss:     7.1799\n",
      "Epoch 167-Loss:     3.7347\n",
      "Epoch 168-Loss:     7.9448\n",
      "Epoch 169-Loss:     3.5537\n",
      "Epoch 170-Loss:     7.9895\n",
      "Epoch 171-Loss:     3.6496\n",
      "Epoch 172-Loss:     7.4488\n",
      "Epoch 173-Loss:     2.6908\n",
      "Epoch 174-Loss:     8.3874\n",
      "Epoch 175-Loss:     3.3011\n",
      "Epoch 176-Loss:     7.9910\n",
      "Epoch 177-Loss:     3.5455\n",
      "Epoch 178-Loss:     7.7445\n",
      "Epoch 179-Loss:     3.8193\n",
      "Epoch 180-Loss:     7.4131\n",
      "Epoch 181-Loss:     3.6806\n",
      "Epoch 182-Loss:     7.9073\n",
      "Epoch 183-Loss:     3.5584\n",
      "Epoch 184-Loss:     7.7194\n",
      "Epoch 185-Loss:     3.7745\n",
      "Epoch 186-Loss:     7.5321\n",
      "Epoch 187-Loss:     3.6746\n",
      "Epoch 188-Loss:     7.9359\n",
      "Epoch 189-Loss:     3.5838\n",
      "Epoch 190-Loss:     7.7028\n",
      "Epoch 191-Loss:     3.7019\n",
      "Epoch 192-Loss:     7.5369\n",
      "Epoch 193-Loss:     3.9543\n",
      "Epoch 194-Loss:     7.3244\n",
      "Epoch 195-Loss:     3.7877\n",
      "Epoch 196-Loss:     7.5238\n",
      "Epoch 197-Loss:     3.6819\n",
      "Epoch 198-Loss:     8.0664\n",
      "Epoch 199-Loss:     3.6587\n",
      "Epoch 200-Loss:     7.6116\n",
      "Epoch 201-Loss:     3.7153\n",
      "Epoch 202-Loss:     7.7554\n",
      "Epoch 203-Loss:     3.7356\n",
      "Epoch 204-Loss:     7.5973\n",
      "Epoch 205-Loss:     3.8036\n",
      "Epoch 206-Loss:     7.4690\n",
      "Epoch 207-Loss:     3.9191\n",
      "Epoch 208-Loss:     7.5100\n",
      "Epoch 209-Loss:     4.0432\n",
      "Epoch 210-Loss:     7.2205\n",
      "Epoch 211-Loss:     3.8919\n",
      "Epoch 212-Loss:     7.3643\n",
      "Epoch 213-Loss:     3.9267\n",
      "Epoch 214-Loss:     7.3620\n",
      "Epoch 215-Loss:     3.9076\n",
      "Epoch 216-Loss:     7.2171\n",
      "Epoch 217-Loss:     4.0702\n",
      "Epoch 218-Loss:     7.2275\n",
      "Epoch 219-Loss:     4.0032\n",
      "Epoch 220-Loss:     7.0963\n",
      "Epoch 221-Loss:     4.1931\n",
      "Epoch 222-Loss:     6.8408\n",
      "Epoch 223-Loss:     4.3867\n",
      "Epoch 224-Loss:     6.7972\n",
      "Epoch 225-Loss:     4.2920\n",
      "Epoch 226-Loss:     6.8859\n",
      "Epoch 227-Loss:     4.1176\n",
      "Epoch 228-Loss:     6.9967\n",
      "Epoch 229-Loss:     4.0079\n",
      "Epoch 230-Loss:     7.1347\n",
      "Epoch 231-Loss:     3.9586\n",
      "Epoch 232-Loss:     7.3314\n",
      "Epoch 233-Loss:     3.7425\n",
      "Epoch 234-Loss:     7.5406\n",
      "Epoch 235-Loss:     3.8145\n",
      "Epoch 236-Loss:     7.4593\n",
      "Epoch 237-Loss:     3.7607\n",
      "Epoch 238-Loss:     7.4298\n",
      "Epoch 239-Loss:     3.6528\n",
      "Epoch 240-Loss:     7.7223\n",
      "Epoch 241-Loss:     3.6675\n",
      "Epoch 242-Loss:     7.6205\n",
      "Epoch 243-Loss:     3.6766\n",
      "Epoch 244-Loss:     7.6940\n",
      "Epoch 245-Loss:     3.7889\n",
      "Epoch 246-Loss:     7.5363\n",
      "Epoch 247-Loss:     3.7507\n",
      "Epoch 248-Loss:     7.3376\n",
      "Epoch 249-Loss:     3.7939\n",
      "Epoch 250-Loss:     7.4405\n",
      "Epoch 251-Loss:     3.7386\n",
      "Epoch 252-Loss:     7.3616\n",
      "Epoch 253-Loss:     3.8314\n",
      "Epoch 254-Loss:     7.4720\n",
      "Epoch 255-Loss:     3.6873\n",
      "Epoch 256-Loss:     7.5139\n",
      "Epoch 257-Loss:     3.5700\n",
      "Epoch 258-Loss:     7.6210\n",
      "Epoch 259-Loss:     3.5291\n",
      "Epoch 260-Loss:     7.7473\n",
      "Epoch 261-Loss:     3.5448\n",
      "Epoch 262-Loss:     7.6608\n",
      "Epoch 263-Loss:     3.7724\n",
      "Epoch 264-Loss:     7.2398\n",
      "Epoch 265-Loss:     3.8122\n",
      "Epoch 266-Loss:     7.4309\n",
      "Epoch 267-Loss:     3.6500\n",
      "Epoch 268-Loss:     7.5511\n",
      "Epoch 269-Loss:     3.5019\n",
      "Epoch 270-Loss:     7.7760\n",
      "Epoch 271-Loss:     3.5313\n",
      "Epoch 272-Loss:     7.8570\n",
      "Epoch 273-Loss:     3.5304\n",
      "Epoch 274-Loss:     7.7077\n",
      "Epoch 275-Loss:     3.5644\n",
      "Epoch 276-Loss:     7.7092\n",
      "Epoch 277-Loss:     3.6304\n",
      "Epoch 278-Loss:     7.8475\n",
      "Epoch 279-Loss:     3.5106\n",
      "Epoch 280-Loss:     7.8033\n",
      "Epoch 281-Loss:     3.3533\n",
      "Epoch 282-Loss:     8.1861\n",
      "Epoch 283-Loss:     3.2622\n",
      "Epoch 284-Loss:     8.2255\n",
      "Epoch 285-Loss:     3.3717\n",
      "Epoch 286-Loss:     7.8278\n",
      "Epoch 287-Loss:     3.4012\n",
      "Epoch 288-Loss:     8.0444\n",
      "Epoch 289-Loss:     3.3108\n",
      "Epoch 290-Loss:     7.9351\n",
      "Epoch 291-Loss:     3.3076\n",
      "Epoch 292-Loss:     8.1402\n",
      "Epoch 293-Loss:     3.2718\n",
      "Epoch 294-Loss:     8.1705\n",
      "Epoch 295-Loss:     3.2758\n",
      "Epoch 296-Loss:     8.1523\n",
      "Epoch 297-Loss:     3.1901\n",
      "Epoch 298-Loss:     8.3246\n",
      "Epoch 299-Loss:     3.2757\n",
      "Epoch 300-Loss:     8.2339\n",
      "Epoch 301-Loss:     3.1818\n",
      "Epoch 302-Loss:     8.1711\n",
      "Epoch 303-Loss:     3.2937\n",
      "Epoch 304-Loss:     8.1092\n",
      "Epoch 305-Loss:     3.2073\n",
      "Epoch 306-Loss:     8.3516\n",
      "Epoch 307-Loss:     3.2344\n",
      "Epoch 308-Loss:     8.3510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309-Loss:     3.2454\n",
      "Epoch 310-Loss:     8.1834\n",
      "Epoch 311-Loss:     3.3257\n",
      "Epoch 312-Loss:     8.2142\n",
      "Epoch 313-Loss:     3.2495\n",
      "Epoch 314-Loss:     8.2065\n",
      "Epoch 315-Loss:     3.2782\n",
      "Epoch 316-Loss:     8.0423\n",
      "Epoch 317-Loss:     3.2511\n",
      "Epoch 318-Loss:     8.2388\n",
      "Epoch 319-Loss:     3.1825\n",
      "Epoch 320-Loss:     8.1886\n",
      "Epoch 321-Loss:     3.2483\n",
      "Epoch 322-Loss:     8.1049\n",
      "Epoch 323-Loss:     3.2633\n",
      "Epoch 324-Loss:     8.2681\n",
      "Epoch 325-Loss:     3.1630\n",
      "Epoch 326-Loss:     8.2788\n",
      "Epoch 327-Loss:     3.2646\n",
      "Epoch 328-Loss:     8.2766\n",
      "Epoch 329-Loss:     3.1489\n",
      "Epoch 330-Loss:     8.1798\n",
      "Epoch 331-Loss:     3.1635\n",
      "Epoch 332-Loss:     8.3684\n",
      "Epoch 333-Loss:     3.1044\n",
      "Epoch 334-Loss:     8.3241\n",
      "Epoch 335-Loss:     3.1552\n",
      "Epoch 336-Loss:     8.2344\n",
      "Epoch 337-Loss:     3.1165\n",
      "Epoch 338-Loss:     8.4527\n",
      "Epoch 339-Loss:     3.1390\n",
      "Epoch 340-Loss:     8.2673\n",
      "Epoch 341-Loss:     3.0774\n",
      "Epoch 342-Loss:     8.2046\n",
      "Epoch 343-Loss:     3.0990\n",
      "Epoch 344-Loss:     8.3857\n",
      "Epoch 345-Loss:     3.0825\n",
      "Epoch 346-Loss:     8.4306\n",
      "Epoch 347-Loss:     2.9875\n",
      "Epoch 348-Loss:     8.5445\n",
      "Epoch 349-Loss:     3.0713\n",
      "Epoch 350-Loss:     8.1844\n",
      "Epoch 351-Loss:     3.0624\n",
      "Epoch 352-Loss:     8.4834\n",
      "Epoch 353-Loss:     3.0750\n",
      "Epoch 354-Loss:     8.0428\n",
      "Epoch 355-Loss:     3.2183\n",
      "Epoch 356-Loss:     8.4104\n",
      "Epoch 357-Loss:     3.0459\n",
      "Epoch 358-Loss:     8.4975\n",
      "Epoch 359-Loss:     3.0510\n",
      "Epoch 360-Loss:     8.8991\n",
      "Epoch 361-Loss:     3.0535\n",
      "Epoch 362-Loss:     8.4109\n",
      "Epoch 363-Loss:     3.0103\n",
      "Epoch 364-Loss:     8.5785\n",
      "Epoch 365-Loss:     3.0231\n",
      "Epoch 366-Loss:     8.5400\n",
      "Epoch 367-Loss:     3.0638\n",
      "Epoch 368-Loss:     8.4278\n",
      "Epoch 369-Loss:     2.9755\n",
      "Epoch 370-Loss:     8.5634\n",
      "Epoch 371-Loss:     2.9526\n",
      "Epoch 372-Loss:     8.4525\n",
      "Epoch 373-Loss:     2.9852\n",
      "Epoch 374-Loss:     8.6781\n",
      "Epoch 375-Loss:     3.0204\n",
      "Epoch 376-Loss:     8.7191\n",
      "Epoch 377-Loss:     2.9609\n",
      "Epoch 378-Loss:     8.5068\n",
      "Epoch 379-Loss:     2.9036\n",
      "Epoch 380-Loss:     8.7032\n",
      "Epoch 381-Loss:     2.9658\n",
      "Epoch 382-Loss:     8.7101\n",
      "Epoch 383-Loss:     3.0047\n",
      "Epoch 384-Loss:     8.3141\n",
      "Epoch 385-Loss:     3.0052\n",
      "Epoch 386-Loss:     8.7078\n",
      "Epoch 387-Loss:     2.9066\n",
      "Epoch 388-Loss:     8.6090\n",
      "Epoch 389-Loss:     3.0995\n",
      "Epoch 390-Loss:     8.3502\n",
      "Epoch 391-Loss:     3.0203\n",
      "Epoch 392-Loss:     8.6248\n",
      "Epoch 393-Loss:     2.8979\n",
      "Epoch 394-Loss:     8.7407\n",
      "Epoch 395-Loss:     2.8251\n",
      "Epoch 396-Loss:     8.6576\n",
      "Epoch 397-Loss:     2.9766\n",
      "Epoch 398-Loss:     8.7969\n",
      "Epoch 399-Loss:     2.8886\n",
      "Epoch 400-Loss:     8.5815\n",
      "Epoch 401-Loss:     2.9022\n",
      "Epoch 402-Loss:     8.6445\n",
      "Epoch 403-Loss:     2.8869\n",
      "Epoch 404-Loss:     8.8447\n",
      "Epoch 405-Loss:     2.8823\n",
      "Epoch 406-Loss:     8.8685\n",
      "Epoch 407-Loss:     2.8845\n",
      "Epoch 408-Loss:     8.5870\n",
      "Epoch 409-Loss:     2.8754\n",
      "Epoch 410-Loss:     8.7818\n",
      "Epoch 411-Loss:     2.8865\n",
      "Epoch 412-Loss:     8.7028\n",
      "Epoch 413-Loss:     2.9053\n",
      "Epoch 414-Loss:     8.5788\n",
      "Epoch 415-Loss:     2.8840\n",
      "Epoch 416-Loss:     8.8340\n",
      "Epoch 417-Loss:     2.8610\n",
      "Epoch 418-Loss:     8.5250\n",
      "Epoch 419-Loss:     2.9001\n",
      "Epoch 420-Loss:     8.6616\n",
      "Epoch 421-Loss:     2.8300\n",
      "Epoch 422-Loss:     8.6916\n",
      "Epoch 423-Loss:     2.9118\n",
      "Epoch 424-Loss:     8.6468\n",
      "Epoch 425-Loss:     2.9241\n",
      "Epoch 426-Loss:     8.8521\n",
      "Epoch 427-Loss:     2.8988\n",
      "Epoch 428-Loss:     8.6198\n",
      "Epoch 429-Loss:     2.8750\n",
      "Epoch 430-Loss:     8.6740\n",
      "Epoch 431-Loss:     2.8179\n",
      "Epoch 432-Loss:     8.8236\n",
      "Epoch 433-Loss:     2.8647\n",
      "Epoch 434-Loss:     8.9606\n",
      "Epoch 435-Loss:     2.8216\n",
      "Epoch 436-Loss:     8.7986\n",
      "Epoch 437-Loss:     2.8710\n",
      "Epoch 438-Loss:     8.5882\n",
      "Epoch 439-Loss:     2.8402\n",
      "Epoch 440-Loss:     8.8291\n",
      "Epoch 441-Loss:     2.8341\n",
      "Epoch 442-Loss:     8.8667\n",
      "Epoch 443-Loss:     2.7810\n",
      "Epoch 444-Loss:     8.8979\n",
      "Epoch 445-Loss:     2.8718\n",
      "Epoch 446-Loss:     8.9595\n",
      "Epoch 447-Loss:     2.7618\n",
      "Epoch 448-Loss:     8.8727\n",
      "Epoch 449-Loss:     2.8161\n",
      "Epoch 450-Loss:     8.6507\n",
      "Epoch 451-Loss:     2.8542\n",
      "Epoch 452-Loss:     8.7681\n",
      "Epoch 453-Loss:     2.8277\n",
      "Epoch 454-Loss:     8.7467\n",
      "Epoch 455-Loss:     2.8088\n",
      "Epoch 456-Loss:     9.0078\n",
      "Epoch 457-Loss:     2.7219\n",
      "Epoch 458-Loss:     9.1328\n",
      "Epoch 459-Loss:     2.8212\n",
      "Epoch 460-Loss:     9.1282\n",
      "Epoch 461-Loss:     2.8336\n",
      "Epoch 462-Loss:     8.6571\n",
      "Epoch 463-Loss:     2.8016\n",
      "Epoch 464-Loss:     8.8231\n",
      "Epoch 465-Loss:     2.7482\n",
      "Epoch 466-Loss:     8.8766\n",
      "Epoch 467-Loss:     2.7069\n",
      "Epoch 468-Loss:     8.9620\n",
      "Epoch 469-Loss:     2.7191\n",
      "Epoch 470-Loss:     9.1308\n",
      "Epoch 471-Loss:     2.8108\n",
      "Epoch 472-Loss:     8.7081\n",
      "Epoch 473-Loss:     2.7976\n",
      "Epoch 474-Loss:     8.9870\n",
      "Epoch 475-Loss:     2.7869\n",
      "Epoch 476-Loss:     8.7667\n",
      "Epoch 477-Loss:     2.8344\n",
      "Epoch 478-Loss:     8.9204\n",
      "Epoch 479-Loss:     2.7425\n",
      "Epoch 480-Loss:     8.8566\n",
      "Epoch 481-Loss:     2.7988\n",
      "Epoch 482-Loss:     8.8979\n",
      "Epoch 483-Loss:     2.7834\n",
      "Epoch 484-Loss:     9.1817\n",
      "Epoch 485-Loss:     2.6700\n",
      "Epoch 486-Loss:     8.9214\n",
      "Epoch 487-Loss:     2.7962\n",
      "Epoch 488-Loss:     9.0360\n",
      "Epoch 489-Loss:     2.7364\n",
      "Epoch 490-Loss:     8.9614\n",
      "Epoch 491-Loss:     2.7271\n",
      "Epoch 492-Loss:     9.1173\n",
      "Epoch 493-Loss:     2.7833\n",
      "Epoch 494-Loss:     9.1979\n",
      "Epoch 495-Loss:     2.7782\n",
      "Epoch 496-Loss:     8.6888\n",
      "Epoch 497-Loss:     2.7267\n",
      "Epoch 498-Loss:     9.2459\n",
      "Epoch 499-Loss:     2.6787\n",
      "Epoch 500-Loss:     8.9367\n",
      "Epoch 501-Loss:     2.7362\n",
      "Epoch 502-Loss:     9.3758\n",
      "Epoch 503-Loss:     2.6617\n",
      "Epoch 504-Loss:     8.9708\n",
      "Epoch 505-Loss:     2.7500\n",
      "Epoch 506-Loss:     9.0731\n",
      "Epoch 507-Loss:     2.6789\n",
      "Epoch 508-Loss:     8.9673\n",
      "Epoch 509-Loss:     2.6488\n",
      "Epoch 510-Loss:     9.2499\n",
      "Epoch 511-Loss:     2.6678\n",
      "Epoch 512-Loss:     8.9772\n",
      "Epoch 513-Loss:     2.7428\n",
      "Epoch 514-Loss:     9.0159\n",
      "Epoch 515-Loss:     2.6766\n",
      "Epoch 516-Loss:     9.0140\n",
      "Epoch 517-Loss:     2.6986\n",
      "Epoch 518-Loss:     9.3900\n",
      "Epoch 519-Loss:     2.7975\n",
      "Epoch 520-Loss:     8.9867\n",
      "Epoch 521-Loss:     2.7054\n",
      "Epoch 522-Loss:     8.9666\n",
      "Epoch 523-Loss:     2.6965\n",
      "Epoch 524-Loss:     9.2047\n",
      "Epoch 525-Loss:     2.6930\n",
      "Epoch 526-Loss:     9.1462\n",
      "Epoch 527-Loss:     2.6813\n",
      "Epoch 528-Loss:     9.2876\n",
      "Epoch 529-Loss:     2.7163\n",
      "Epoch 530-Loss:     9.2074\n",
      "Epoch 531-Loss:     2.6726\n",
      "Epoch 532-Loss:     9.2005\n",
      "Epoch 533-Loss:     2.6768\n",
      "Epoch 534-Loss:     9.3014\n",
      "Epoch 535-Loss:     2.6874\n",
      "Epoch 536-Loss:     9.0416\n",
      "Epoch 537-Loss:     2.7050\n",
      "Epoch 538-Loss:     8.9925\n",
      "Epoch 539-Loss:     2.6698\n",
      "Epoch 540-Loss:     9.0167\n",
      "Epoch 541-Loss:     2.6688\n",
      "Epoch 542-Loss:     9.2914\n",
      "Epoch 543-Loss:     2.6623\n",
      "Epoch 544-Loss:     9.1128\n",
      "Epoch 545-Loss:     2.7405\n",
      "Epoch 546-Loss:     9.3719\n",
      "Epoch 547-Loss:     2.6713\n",
      "Epoch 548-Loss:     8.8734\n",
      "Epoch 549-Loss:     2.7195\n",
      "Epoch 550-Loss:     9.2436\n",
      "Epoch 551-Loss:     2.6739\n",
      "Epoch 552-Loss:     9.0626\n",
      "Epoch 553-Loss:     2.6355\n",
      "Epoch 554-Loss:     9.1121\n",
      "Epoch 555-Loss:     2.6960\n",
      "Epoch 556-Loss:     9.0140\n",
      "Epoch 557-Loss:     2.6865\n",
      "Epoch 558-Loss:     9.3474\n",
      "Epoch 559-Loss:     2.6488\n",
      "Epoch 560-Loss:     9.5122\n",
      "Epoch 561-Loss:     2.6593\n",
      "Epoch 562-Loss:     9.2846\n",
      "Epoch 563-Loss:     2.6534\n",
      "Epoch 564-Loss:     9.1332\n",
      "Epoch 565-Loss:     2.6666\n",
      "Epoch 566-Loss:     9.5679\n",
      "Epoch 567-Loss:     2.6328\n",
      "Epoch 568-Loss:     9.0108\n",
      "Epoch 569-Loss:     2.6466\n",
      "Epoch 570-Loss:     9.3082\n",
      "Epoch 571-Loss:     2.6302\n",
      "Epoch 572-Loss:     9.4277\n",
      "Epoch 573-Loss:     2.6375\n",
      "Epoch 574-Loss:     9.1113\n",
      "Epoch 575-Loss:     2.6920\n",
      "Epoch 576-Loss:     9.2461\n",
      "Epoch 577-Loss:     2.5798\n",
      "Epoch 578-Loss:     9.2379\n",
      "Epoch 579-Loss:     2.7353\n",
      "Epoch 580-Loss:     9.7387\n",
      "Epoch 581-Loss:     2.6475\n",
      "Epoch 582-Loss:     8.9793\n",
      "Epoch 583-Loss:     2.7361\n",
      "Epoch 584-Loss:     9.2819\n",
      "Epoch 585-Loss:     2.6140\n",
      "Epoch 586-Loss:     9.1817\n",
      "Epoch 587-Loss:     2.6153\n",
      "Epoch 588-Loss:     9.1994\n",
      "Epoch 589-Loss:     2.6129\n",
      "Epoch 590-Loss:     9.3804\n",
      "Epoch 591-Loss:     2.6900\n",
      "Epoch 592-Loss:     9.1081\n",
      "Epoch 593-Loss:     2.6532\n",
      "Epoch 594-Loss:     9.5066\n",
      "Epoch 595-Loss:     2.6027\n",
      "Epoch 596-Loss:     9.1842\n",
      "Epoch 597-Loss:     2.7352\n",
      "Epoch 598-Loss:     9.5669\n",
      "Epoch 599-Loss:     2.5572\n",
      "Epoch 600-Loss:     9.0370\n",
      "Epoch 601-Loss:     2.7270\n",
      "Epoch 602-Loss:     9.4978\n",
      "Epoch 603-Loss:     2.5734\n",
      "Epoch 604-Loss:     9.1121\n",
      "Epoch 605-Loss:     2.7041\n",
      "Epoch 606-Loss:     9.1617\n",
      "Epoch 607-Loss:     2.5149\n",
      "Epoch 608-Loss:     9.5016\n",
      "Epoch 609-Loss:     2.6599\n",
      "Epoch 610-Loss:     9.4513\n",
      "Epoch 611-Loss:     2.6464\n",
      "Epoch 612-Loss:     9.1951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 613-Loss:     2.6285\n",
      "Epoch 614-Loss:     9.4466\n",
      "Epoch 615-Loss:     2.6657\n",
      "Epoch 616-Loss:     9.3745\n",
      "Epoch 617-Loss:     2.6245\n",
      "Epoch 618-Loss:     9.1611\n",
      "Epoch 619-Loss:     2.7110\n",
      "Epoch 620-Loss:     9.4817\n",
      "Epoch 621-Loss:     2.5448\n",
      "Epoch 622-Loss:     9.1739\n",
      "Epoch 623-Loss:     2.6220\n",
      "Epoch 624-Loss:     9.5660\n",
      "Epoch 625-Loss:     2.6650\n",
      "Epoch 626-Loss:     9.2463\n",
      "Epoch 627-Loss:     2.6354\n",
      "Epoch 628-Loss:     9.3002\n",
      "Epoch 629-Loss:     2.7082\n",
      "Epoch 630-Loss:     9.3561\n",
      "Epoch 631-Loss:     2.5026\n",
      "Epoch 632-Loss:     9.3034\n",
      "Epoch 633-Loss:     2.6931\n",
      "Epoch 634-Loss:     9.3915\n",
      "Epoch 635-Loss:     2.6198\n",
      "Epoch 636-Loss:     9.3158\n",
      "Epoch 637-Loss:     2.5767\n",
      "Epoch 638-Loss:     9.3190\n",
      "Epoch 639-Loss:     2.6334\n",
      "Epoch 640-Loss:     9.6211\n",
      "Epoch 641-Loss:     2.6294\n",
      "Epoch 642-Loss:     9.4007\n",
      "Epoch 643-Loss:     2.5241\n",
      "Epoch 644-Loss:     9.1524\n",
      "Epoch 645-Loss:     2.7810\n",
      "Epoch 646-Loss:     9.6988\n",
      "Epoch 647-Loss:     2.5509\n",
      "Epoch 648-Loss:     9.2074\n",
      "Epoch 649-Loss:     2.5734\n",
      "Epoch 650-Loss:     9.5994\n",
      "Epoch 651-Loss:     2.5859\n",
      "Epoch 652-Loss:     9.3586\n",
      "Epoch 653-Loss:     2.5998\n",
      "Epoch 654-Loss:     9.5985\n",
      "Epoch 655-Loss:     2.5901\n",
      "Epoch 656-Loss:     9.1623\n",
      "Epoch 657-Loss:     2.5958\n",
      "Epoch 658-Loss:     9.6694\n",
      "Epoch 659-Loss:     2.5981\n",
      "Epoch 660-Loss:     9.1258\n",
      "Epoch 661-Loss:     2.6041\n",
      "Epoch 662-Loss:     9.5206\n",
      "Epoch 663-Loss:     2.6258\n",
      "Epoch 664-Loss:     9.4503\n",
      "Epoch 665-Loss:     2.5413\n",
      "Epoch 666-Loss:     9.4846\n",
      "Epoch 667-Loss:     2.6303\n",
      "Epoch 668-Loss:     9.5849\n",
      "Epoch 669-Loss:     2.5805\n",
      "Epoch 670-Loss:     9.2308\n",
      "Epoch 671-Loss:     2.6580\n",
      "Epoch 672-Loss:     9.5434\n",
      "Epoch 673-Loss:     2.6071\n",
      "Epoch 674-Loss:     9.4759\n",
      "Epoch 675-Loss:     2.5682\n",
      "Epoch 676-Loss:     9.4446\n",
      "Epoch 677-Loss:     2.5995\n",
      "Epoch 678-Loss:     9.5066\n",
      "Epoch 679-Loss:     2.6057\n",
      "Epoch 680-Loss:     9.5835\n",
      "Epoch 681-Loss:     2.4867\n",
      "Epoch 682-Loss:     9.0444\n",
      "Epoch 683-Loss:     2.7997\n",
      "Epoch 684-Loss:     9.7601\n",
      "Epoch 685-Loss:     2.4741\n",
      "Epoch 686-Loss:     9.1103\n",
      "Epoch 687-Loss:     2.8147\n",
      "Epoch 688-Loss:    10.0538\n",
      "Epoch 689-Loss:     2.5046\n",
      "Epoch 690-Loss:     9.1248\n",
      "Epoch 691-Loss:     2.7985\n",
      "Epoch 692-Loss:     9.9065\n",
      "Epoch 693-Loss:     2.4619\n",
      "Epoch 694-Loss:     8.9896\n",
      "Epoch 695-Loss:     2.8106\n",
      "Epoch 696-Loss:    10.2154\n",
      "Epoch 697-Loss:     2.5708\n",
      "Epoch 698-Loss:     8.9504\n",
      "Epoch 699-Loss:     2.8397\n",
      "Epoch 700-Loss:     9.7386\n",
      "Epoch 701-Loss:     2.5045\n",
      "Epoch 702-Loss:     9.3113\n",
      "Epoch 703-Loss:     2.6914\n",
      "Epoch 704-Loss:     9.7185\n",
      "Epoch 705-Loss:     2.5119\n",
      "Epoch 706-Loss:     9.4371\n",
      "Epoch 707-Loss:     2.6723\n",
      "Epoch 708-Loss:     9.4200\n",
      "Epoch 709-Loss:     2.5744\n",
      "Epoch 710-Loss:     9.6186\n",
      "Epoch 711-Loss:     2.5806\n",
      "Epoch 712-Loss:     9.6448\n",
      "Epoch 713-Loss:     2.6127\n",
      "Epoch 714-Loss:     9.3217\n",
      "Epoch 715-Loss:     2.6354\n",
      "Epoch 716-Loss:     9.7276\n",
      "Epoch 717-Loss:     2.5438\n",
      "Epoch 718-Loss:     9.3987\n",
      "Epoch 719-Loss:     2.6836\n",
      "Epoch 720-Loss:     9.5761\n",
      "Epoch 721-Loss:     2.5631\n",
      "Epoch 722-Loss:     9.5169\n",
      "Epoch 723-Loss:     2.5329\n",
      "Epoch 724-Loss:     9.5614\n",
      "Epoch 725-Loss:     2.5048\n",
      "Epoch 726-Loss:     9.2593\n",
      "Epoch 727-Loss:     2.5851\n",
      "Epoch 728-Loss:     9.6987\n",
      "Epoch 729-Loss:     2.5307\n",
      "Epoch 730-Loss:     9.5513\n",
      "Epoch 731-Loss:     2.5978\n",
      "Epoch 732-Loss:     9.5454\n",
      "Epoch 733-Loss:     2.5495\n",
      "Epoch 734-Loss:     9.4945\n",
      "Epoch 735-Loss:     2.6061\n",
      "Epoch 736-Loss:     9.5330\n",
      "Epoch 737-Loss:     2.5781\n",
      "Epoch 738-Loss:     9.2949\n",
      "Epoch 739-Loss:     2.5830\n",
      "Epoch 740-Loss:     9.4859\n",
      "Epoch 741-Loss:     2.5956\n",
      "Epoch 742-Loss:     9.5531\n",
      "Epoch 743-Loss:     2.5112\n",
      "Epoch 744-Loss:     9.4172\n",
      "Epoch 745-Loss:     2.6418\n",
      "Epoch 746-Loss:     9.4816\n",
      "Epoch 747-Loss:     2.5031\n",
      "Epoch 748-Loss:     9.5877\n",
      "Epoch 749-Loss:     2.6369\n",
      "Epoch 750-Loss:     9.7679\n",
      "Epoch 751-Loss:     2.5264\n",
      "Epoch 752-Loss:     9.3822\n",
      "Epoch 753-Loss:     2.5498\n",
      "Epoch 754-Loss:     9.6356\n",
      "Epoch 755-Loss:     2.5679\n",
      "Epoch 756-Loss:     9.6320\n",
      "Epoch 757-Loss:     2.5415\n",
      "Epoch 758-Loss:     9.7023\n",
      "Epoch 759-Loss:     2.6172\n",
      "Epoch 760-Loss:     9.4089\n",
      "Epoch 761-Loss:     2.6102\n",
      "Epoch 762-Loss:     9.6484\n",
      "Epoch 763-Loss:     2.5378\n",
      "Epoch 764-Loss:     9.3221\n",
      "Epoch 765-Loss:     2.6370\n",
      "Epoch 766-Loss:     9.7933\n",
      "Epoch 767-Loss:     2.6096\n",
      "Epoch 768-Loss:     9.4357\n",
      "Epoch 769-Loss:     2.5634\n",
      "Epoch 770-Loss:     9.4732\n",
      "Epoch 771-Loss:     2.5675\n",
      "Epoch 772-Loss:     9.4879\n",
      "Epoch 773-Loss:     2.6041\n",
      "Epoch 774-Loss:     9.6689\n",
      "Epoch 775-Loss:     2.5832\n",
      "Epoch 776-Loss:     9.6072\n",
      "Epoch 777-Loss:     2.5879\n",
      "Epoch 778-Loss:     9.6985\n",
      "Epoch 779-Loss:     2.5247\n",
      "Epoch 780-Loss:     9.4977\n",
      "Epoch 781-Loss:     2.6099\n",
      "Epoch 782-Loss:     9.8345\n",
      "Epoch 783-Loss:     2.4765\n",
      "Epoch 784-Loss:     9.0528\n",
      "Epoch 785-Loss:     2.7745\n",
      "Epoch 786-Loss:    10.1260\n",
      "Epoch 787-Loss:     2.4580\n",
      "Epoch 788-Loss:     9.1199\n",
      "Epoch 789-Loss:     2.8573\n",
      "Epoch 790-Loss:     9.9129\n",
      "Epoch 791-Loss:     2.4465\n",
      "Epoch 792-Loss:     9.3916\n",
      "Epoch 793-Loss:     2.7289\n",
      "Epoch 794-Loss:     9.7643\n",
      "Epoch 795-Loss:     2.4757\n",
      "Epoch 796-Loss:     9.5220\n",
      "Epoch 797-Loss:     2.6344\n",
      "Epoch 798-Loss:     9.5467\n",
      "Epoch 799-Loss:     2.5023\n",
      "Epoch 800-Loss:     9.7169\n",
      "Epoch 801-Loss:     2.6093\n",
      "Epoch 802-Loss:     9.5405\n",
      "Epoch 803-Loss:     2.4864\n",
      "Epoch 804-Loss:     9.4877\n",
      "Epoch 805-Loss:     2.6770\n",
      "Epoch 806-Loss:     9.9532\n",
      "Epoch 807-Loss:     2.4211\n",
      "Epoch 808-Loss:     9.3725\n",
      "Epoch 809-Loss:     2.6796\n",
      "Epoch 810-Loss:     9.5817\n",
      "Epoch 811-Loss:     2.4562\n",
      "Epoch 812-Loss:     9.9061\n",
      "Epoch 813-Loss:     2.6329\n",
      "Epoch 814-Loss:     9.5960\n",
      "Epoch 815-Loss:     2.4197\n",
      "Epoch 816-Loss:     9.3630\n",
      "Epoch 817-Loss:     2.6654\n",
      "Epoch 818-Loss:     9.7526\n",
      "Epoch 819-Loss:     2.4924\n",
      "Epoch 820-Loss:     9.3618\n",
      "Epoch 821-Loss:     2.6807\n",
      "Epoch 822-Loss:     9.9243\n",
      "Epoch 823-Loss:     2.4504\n",
      "Epoch 824-Loss:     9.2914\n",
      "Epoch 825-Loss:     2.8631\n",
      "Epoch 826-Loss:    10.0959\n",
      "Epoch 827-Loss:     2.4893\n",
      "Epoch 828-Loss:     9.1215\n",
      "Epoch 829-Loss:     2.6655\n",
      "Epoch 830-Loss:     9.9377\n",
      "Epoch 831-Loss:     2.4407\n",
      "Epoch 832-Loss:     9.3216\n",
      "Epoch 833-Loss:     2.9569\n",
      "Epoch 834-Loss:    10.1135\n",
      "Epoch 835-Loss:     2.3713\n",
      "Epoch 836-Loss:     9.1052\n",
      "Epoch 837-Loss:     2.8312\n",
      "Epoch 838-Loss:    10.0172\n",
      "Epoch 839-Loss:     2.4430\n",
      "Epoch 840-Loss:     9.2826\n",
      "Epoch 841-Loss:     2.9065\n",
      "Epoch 842-Loss:    10.2682\n",
      "Epoch 843-Loss:     2.4193\n",
      "Epoch 844-Loss:     9.0357\n",
      "Epoch 845-Loss:     3.0416\n",
      "Epoch 846-Loss:    10.3109\n",
      "Epoch 847-Loss:     2.4671\n",
      "Epoch 848-Loss:     8.9452\n",
      "Epoch 849-Loss:     3.0724\n",
      "Epoch 850-Loss:    10.3706\n",
      "Epoch 851-Loss:     2.4367\n",
      "Epoch 852-Loss:     9.0531\n",
      "Epoch 853-Loss:     2.9817\n",
      "Epoch 854-Loss:    10.2453\n",
      "Epoch 855-Loss:     2.4498\n",
      "Epoch 856-Loss:     9.0248\n",
      "Epoch 857-Loss:     3.0578\n",
      "Epoch 858-Loss:    10.4911\n",
      "Epoch 859-Loss:     2.4981\n",
      "Epoch 860-Loss:     8.9231\n",
      "Epoch 861-Loss:     3.1824\n",
      "Epoch 862-Loss:    10.3777\n",
      "Epoch 863-Loss:     2.4270\n",
      "Epoch 864-Loss:     8.6815\n",
      "Epoch 865-Loss:     3.3221\n",
      "Epoch 866-Loss:    10.7800\n",
      "Epoch 867-Loss:     2.5384\n",
      "Epoch 868-Loss:     8.4244\n",
      "Epoch 869-Loss:     3.4922\n",
      "Epoch 870-Loss:    10.6776\n",
      "Epoch 871-Loss:     2.5206\n",
      "Epoch 872-Loss:     8.5244\n",
      "Epoch 873-Loss:     3.4552\n",
      "Epoch 874-Loss:    10.7756\n",
      "Epoch 875-Loss:     2.5358\n",
      "Epoch 876-Loss:     8.3027\n",
      "Epoch 877-Loss:     3.7810\n",
      "Epoch 878-Loss:    11.1378\n",
      "Epoch 879-Loss:     2.6754\n",
      "Epoch 880-Loss:     8.1453\n",
      "Epoch 881-Loss:     4.1136\n",
      "Epoch 882-Loss:    11.3322\n",
      "Epoch 883-Loss:     2.7447\n",
      "Epoch 884-Loss:     7.8656\n",
      "Epoch 885-Loss:     3.9677\n",
      "Epoch 886-Loss:    11.0198\n",
      "Epoch 887-Loss:     2.6119\n",
      "Epoch 888-Loss:     8.3697\n",
      "Epoch 889-Loss:     4.0149\n",
      "Epoch 890-Loss:    11.1960\n",
      "Epoch 891-Loss:     2.6755\n",
      "Epoch 892-Loss:     8.0515\n",
      "Epoch 893-Loss:     4.2868\n",
      "Epoch 894-Loss:    11.4665\n",
      "Epoch 895-Loss:     2.7794\n",
      "Epoch 896-Loss:     7.6848\n",
      "Epoch 897-Loss:     4.0635\n",
      "Epoch 898-Loss:    11.3057\n",
      "Epoch 899-Loss:     2.7136\n",
      "Epoch 900-Loss:     7.8875\n",
      "Epoch 901-Loss:     4.2987\n",
      "Epoch 902-Loss:    11.5708\n",
      "Epoch 903-Loss:     2.8437\n",
      "Epoch 904-Loss:     7.7886\n",
      "Epoch 905-Loss:     4.3712\n",
      "Epoch 906-Loss:    11.4738\n",
      "Epoch 907-Loss:     2.8771\n",
      "Epoch 908-Loss:     7.5629\n",
      "Epoch 909-Loss:     4.3550\n",
      "Epoch 910-Loss:    11.4224\n",
      "Epoch 911-Loss:     2.7655\n",
      "Epoch 912-Loss:     7.8414\n",
      "Epoch 913-Loss:     4.4580\n",
      "Epoch 914-Loss:    11.6734\n",
      "Epoch 915-Loss:     2.8855\n",
      "Epoch 916-Loss:     7.4362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 917-Loss:     4.1880\n",
      "Epoch 918-Loss:    11.5106\n",
      "Epoch 919-Loss:     2.8313\n",
      "Epoch 920-Loss:     7.6361\n",
      "Epoch 921-Loss:     4.3010\n",
      "Epoch 922-Loss:    11.4338\n",
      "Epoch 923-Loss:     2.7081\n",
      "Epoch 924-Loss:     7.8774\n",
      "Epoch 925-Loss:     4.2881\n",
      "Epoch 926-Loss:    11.4552\n",
      "Epoch 927-Loss:     2.7337\n",
      "Epoch 928-Loss:     7.8420\n",
      "Epoch 929-Loss:     4.5136\n",
      "Epoch 930-Loss:    11.6547\n",
      "Epoch 931-Loss:     2.8472\n",
      "Epoch 932-Loss:     7.6744\n",
      "Epoch 933-Loss:     4.5684\n",
      "Epoch 934-Loss:    11.8375\n",
      "Epoch 935-Loss:     2.9514\n",
      "Epoch 936-Loss:     7.3217\n",
      "Epoch 937-Loss:     4.7193\n",
      "Epoch 938-Loss:    11.7482\n",
      "Epoch 939-Loss:     2.9246\n",
      "Epoch 940-Loss:     7.2903\n",
      "Epoch 941-Loss:     4.5266\n",
      "Epoch 942-Loss:    11.8446\n",
      "Epoch 943-Loss:     2.9113\n",
      "Epoch 944-Loss:     7.4329\n",
      "Epoch 945-Loss:     4.4041\n",
      "Epoch 946-Loss:    11.7115\n",
      "Epoch 947-Loss:     2.8414\n",
      "Epoch 948-Loss:     7.5792\n",
      "Epoch 949-Loss:     4.6885\n",
      "Epoch 950-Loss:    11.9611\n",
      "Epoch 951-Loss:     2.9824\n",
      "Epoch 952-Loss:     7.3114\n",
      "Epoch 953-Loss:     4.6220\n",
      "Epoch 954-Loss:    11.8768\n",
      "Epoch 955-Loss:     2.9430\n",
      "Epoch 956-Loss:     7.2852\n",
      "Epoch 957-Loss:     4.8165\n",
      "Epoch 958-Loss:    11.9508\n",
      "Epoch 959-Loss:     2.9810\n",
      "Epoch 960-Loss:     7.2436\n",
      "Epoch 961-Loss:     4.5631\n",
      "Epoch 962-Loss:    11.9127\n",
      "Epoch 963-Loss:     2.9353\n",
      "Epoch 964-Loss:     7.3047\n",
      "Epoch 965-Loss:     4.9072\n",
      "Epoch 966-Loss:    11.9947\n",
      "Epoch 967-Loss:     2.9422\n",
      "Epoch 968-Loss:     7.4095\n",
      "Epoch 969-Loss:     4.8423\n",
      "Epoch 970-Loss:    12.0909\n",
      "Epoch 971-Loss:     3.0358\n",
      "Epoch 972-Loss:     7.1836\n",
      "Epoch 973-Loss:     4.9667\n",
      "Epoch 974-Loss:    12.1132\n",
      "Epoch 975-Loss:     3.0402\n",
      "Epoch 976-Loss:     7.0951\n",
      "Epoch 977-Loss:     4.8599\n",
      "Epoch 978-Loss:    12.0967\n",
      "Epoch 979-Loss:     2.9902\n",
      "Epoch 980-Loss:     7.2030\n",
      "Epoch 981-Loss:     4.9610\n",
      "Epoch 982-Loss:    12.3097\n",
      "Epoch 983-Loss:     3.1457\n",
      "Epoch 984-Loss:     6.7713\n",
      "Epoch 985-Loss:     4.4976\n",
      "Epoch 986-Loss:    11.9269\n",
      "Epoch 987-Loss:     2.9060\n",
      "Epoch 988-Loss:     7.2350\n",
      "Epoch 989-Loss:     4.9438\n",
      "Epoch 990-Loss:    12.2703\n",
      "Epoch 991-Loss:     3.1037\n",
      "Epoch 992-Loss:     6.9526\n",
      "Epoch 993-Loss:     4.7104\n",
      "Epoch 994-Loss:    12.2241\n",
      "Epoch 995-Loss:     3.0731\n",
      "Epoch 996-Loss:     6.7776\n",
      "Epoch 997-Loss:     4.6710\n",
      "Epoch 998-Loss:    12.1719\n",
      "Epoch 999-Loss:     3.0815\n",
      "Epoch 1000-Loss:     6.9511\n",
      "Testing Accuracy: 0.6279762387275696\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "# changed lr: 0.1, 0.01, 0.001, 0.0001\\\n",
    "# changed epochs: 100, 1000, 10000, 100000\n",
    "# best result from lr: 0.01, ep: 1000\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "# network Parameters\n",
    "n_classes = 14\n",
    "dropout = 0.8\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 9, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 32, 64])),\n",
    "    'wc3': tf.Variable(tf.random_normal([5, 64, 128])),\n",
    "    'wd1': tf.Variable(tf.random_normal([10*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bc3': tf.Variable(tf.random_normal([128])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "# changed relu to sigmoid\n",
    "# (best) result from reLu \n",
    "def conv1d(x, W, b, s):\n",
    "    x = tf.nn.conv1d(x, W, s, padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Conv Layer 1\n",
    "    conv1 = conv1d(x, weights['wc1'], biases['bc1'], 5)\n",
    "\n",
    "    # Conv Layer 2\n",
    "    conv2 = conv1d(conv1, weights['wc2'], biases['bc2'], 5)\n",
    "    \n",
    "    #Conv Layer 3\n",
    "    # conv3 = conv1d(conv2, weights['wc3'], biases['bc3'], 1)\n",
    "    \n",
    "\n",
    "    # Fully connected layer \n",
    "    # Changed relu to sigmoid\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.sigmoid(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction \n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 250, 9])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf. global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(optimizer, feed_dict={\n",
    "            x: f_train,\n",
    "            y: l_train.eval(),\n",
    "            keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "        loss = sess.run(cost, feed_dict={\n",
    "            x: f_train,\n",
    "            y: l_train.eval(),\n",
    "            keep_prob: 1.})\n",
    "\n",
    "        print('Epoch {:>2}-'\n",
    "            'Loss: {:>10.4f}'.format(\n",
    "            epoch + 1,\n",
    "            loss))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: f_test,\n",
    "        y: l_test.eval(),\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
