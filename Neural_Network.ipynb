{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#col_array = ['label', 'label2' 'label_id', 'speed', 'heading_x', 'heading_y', 'heading_z', 'acceleration_x', 'acceleration_y', 'acceleration_z', 'gyroscope_x', 'gyroscope_y', 'gyroscope_z', 'magnetic_field_x', 'magnetic_field_y', 'magnetic_field_z', 'pressure_hPa', 'log_timestamp']\n",
    "col_array = ['label_id', 'acceleration_x', 'acceleration_y', 'acceleration_z', 'gyroscope_x', 'gyroscope_y', 'gyroscope_z', 'magnetic_field_x', 'magnetic_field_y', 'magnetic_field_z']\n",
    "path = \"../posmo/experiments/data/posmo_segments/v8/\" \n",
    "def read_values(csv, col_array, path=\"../posmo/experiments/data/posmo_segments/v8/\"):\n",
    "    return pd.read_csv(path + csv).loc[:, col_array] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = []\n",
    "for fname in glob.glob(\"../posmo/experiments/data/posmo_segments/v8/*.csv\"):\n",
    "    files.append(fname)\n",
    "for file in files:\n",
    "    if file in glob.glob(\"../posmo/experiments/data/posmo_segments/v8/Other*\"):\n",
    "        files.remove(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df, f_train, l_train, f_test, l_test, seq_len=250):\n",
    "    df_np = df.as_matrix()\n",
    "    df_np = df_np[:(df_np.shape[0] - df_np.shape[0] % seq_len), :]\n",
    "    df_np = df_np.reshape(int(df_np.shape[0]/seq_len), seq_len, df_np.shape[1])\n",
    "    for i in range(df_np.shape[0]):\n",
    "        label = df_np[i, 0, 0]\n",
    "        feature = np.expand_dims(df_np[i, :, 1:10], axis=0)\n",
    "        if i % 10 == 0:\n",
    "            l_test = np.append(l_test, label)\n",
    "            f_test = np.concatenate([f_test, feature], axis=0)\n",
    "        else:\n",
    "            l_train = np.append(l_train, label)\n",
    "            f_train = np.concatenate([f_train, feature], axis=0)\n",
    "    #feature = df_np[:, :, 1:18]\n",
    "    #len = feature.shape[0]\n",
    "    #n = int((len - len % 10) / 10)\n",
    "    #feature_test = df_np[:n, :, 1:18]\n",
    "    #feature_train = df_np[n:, :, 1:18]\n",
    "    #f_test = np.concatenate([f_test, feature_test], axis=0)\n",
    "    #f_train = np.concatenate([f_train, feature_train], axis=0)\n",
    "    return f_train, l_train, f_test, l_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_shapes():\n",
    "    return f_train.shape, l_train.shape, f_test.shape, l_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = np.empty([1, 250, 9])\n",
    "l_train = np.array([])\n",
    "f_test = np.empty([1, 250, 9])\n",
    "l_test = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    df = read_values(file, col_array, path=\"\")\n",
    "    f_train, l_train, f_test, l_test = feature_extraction(df, f_train, l_train, f_test, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = f_train[1:, :, :]\n",
    "f_test = f_test[1:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13255, 250, 9), (13255,), (1523, 250, 9), (1523,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_train_one_hot = tf.one_hot(l_train, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test_one_hot = tf.one_hot(l_test, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(activation_function, conv_layers, learning_rate, epochs, dropout, n_filters, seed):\n",
    "    # network Parameters\n",
    "    n_classes = 14\n",
    "\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "    if conv_layers == 2:\n",
    "        weights = {\n",
    "            'wc1': tf.Variable(tf.random_normal([n_filters, 9, 32])),\n",
    "            'wc2': tf.Variable(tf.random_normal([n_filters, 32, 64])),\n",
    "            'wc3': tf.Variable(tf.random_normal([n_filters, 64, 128])),\n",
    "            'wd1': tf.Variable(tf.random_normal([10*64, 1024])),\n",
    "            'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "    elif conv_layers == 3:\n",
    "        weights = {\n",
    "            'wc1': tf.Variable(tf.random_normal([n_filters, 9, 32])),\n",
    "            'wc2': tf.Variable(tf.random_normal([n_filters, 32, 64])),\n",
    "            'wc3': tf.Variable(tf.random_normal([n_filters, 64, 128])),\n",
    "            'wd1': tf.Variable(tf.random_normal([10*128, 1024])),\n",
    "            'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.random_normal([32])),\n",
    "        'bc2': tf.Variable(tf.random_normal([64])),\n",
    "        'bc3': tf.Variable(tf.random_normal([128])),\n",
    "        'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    # changed relu to sigmoid\n",
    "    # (best) result from reLu \n",
    "    def conv1d(x, W, b, s):\n",
    "        x = tf.nn.conv1d(x, W, s, padding='SAME')\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "    if conv_layers == 2:\n",
    "        def conv_net(x, weights, biases, dropout, activation_function):\n",
    "            # Conv Layer 1\n",
    "            conv1 = conv1d(x, weights['wc1'], biases['bc1'], 5)\n",
    "\n",
    "            # Conv Layer 2\n",
    "            conv2 = conv1d(conv1, weights['wc2'], biases['bc2'], 5)\n",
    "    \n",
    "            #Conv Layer 3\n",
    "            # conv3 = conv1d(conv2, weights['wc3'], biases['bc3'], 1)\n",
    "    \n",
    "\n",
    "            # Fully connected layer \n",
    "            # Changed relu to sigmoid\n",
    "            fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "            fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "            fc1 = activation_function(fc1)\n",
    "            fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "            # Output Layer - class prediction \n",
    "            out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "            return out\n",
    "    elif conv_layers == 3:\n",
    "        def conv_net(x, weights, biases, dropout, activation_function):\n",
    "            # Conv Layer 1\n",
    "            conv1 = conv1d(x, weights['wc1'], biases['bc1'], 5)\n",
    "\n",
    "            # Conv Layer 2\n",
    "            conv2 = conv1d(conv1, weights['wc2'], biases['bc2'], 5)\n",
    "    \n",
    "            #Conv Layer 3\n",
    "            conv3 = conv1d(conv2, weights['wc3'], biases['bc3'], 1)\n",
    "    \n",
    "\n",
    "            # Fully connected layer \n",
    "            # Changed relu to sigmoid\n",
    "            fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "            fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "            fc1 = activation_function(fc1)\n",
    "            fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "            # Output Layer - class prediction \n",
    "            out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "            return out\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 250, 9])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Model\n",
    "    logits = conv_net(x, weights, biases, keep_prob, activation_function)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: f_train,\n",
    "                y: l_train_one_hot.eval(),\n",
    "                keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: f_train,\n",
    "                y: l_train_one_hot.eval(),\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print('Epoch {:>2} - '\n",
    "                    'Loss: {:>10.4f}'.format(\n",
    "                    epoch + 1,\n",
    "                    loss))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "        test_acc = sess.run(accuracy, feed_dict={\n",
    "            x: f_test,\n",
    "            y: l_test_one_hot.eval(),\n",
    "            keep_prob: 1.})\n",
    "        print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_3d_2d(features):\n",
    "    nsamples, nx, ny = features.shape\n",
    "    return features.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.933026920551543\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(transform_3d_2d(f_train), l_train)\n",
    "pred = clf.predict(transform_3d_2d(f_test))\n",
    "test_acc = metrics.accuracy_score(l_test, pred)\n",
    "print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 - Loss:    43.7542\n",
      "Epoch 101 - Loss:    10.6798\n",
      "Epoch 201 - Loss:     6.4271\n",
      "Epoch 301 - Loss:     5.5761\n",
      "Epoch 401 - Loss:     4.6144\n",
      "Epoch 501 - Loss:     4.3379\n",
      "Epoch 601 - Loss:     3.9611\n",
      "Epoch 701 - Loss:     3.7260\n",
      "Epoch 801 - Loss:     3.5431\n",
      "Epoch 901 - Loss:     3.8287\n",
      "Epoch 1001 - Loss:     3.3900\n",
      "Epoch 1101 - Loss:     3.1861\n",
      "Epoch 1201 - Loss:     3.5413\n",
      "Epoch 1301 - Loss:     3.1882\n",
      "Epoch 1401 - Loss:     2.9227\n",
      "Epoch 1501 - Loss:     2.9351\n",
      "Epoch 1601 - Loss:     2.9868\n",
      "Epoch 1701 - Loss:     3.0374\n",
      "Epoch 1801 - Loss:     2.9358\n",
      "Epoch 1901 - Loss:     2.7700\n",
      "Testing Accuracy: 0.6605384349822998\n"
     ]
    }
   ],
   "source": [
    "neural_net(tf.nn.sigmoid, 2, 0.0005, 2000, 0.8, 5, 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 - Loss:    14.7844\n",
      "Epoch 101 - Loss:    10.3681\n",
      "Epoch 201 - Loss:     9.9865\n",
      "Epoch 301 - Loss:     8.3634\n",
      "Epoch 401 - Loss:     7.1704\n",
      "Epoch 501 - Loss:     6.4957\n",
      "Epoch 601 - Loss:     6.1401\n",
      "Epoch 701 - Loss:     5.7386\n",
      "Epoch 801 - Loss:     5.0314\n",
      "Epoch 901 - Loss:     5.0131\n",
      "Epoch 1001 - Loss:     4.4731\n",
      "Epoch 1101 - Loss:     4.4357\n",
      "Epoch 1201 - Loss:     3.7790\n",
      "Epoch 1301 - Loss:     3.8026\n",
      "Epoch 1401 - Loss:     3.9143\n",
      "Epoch 1501 - Loss:     3.8617\n",
      "Epoch 1601 - Loss:     3.6310\n",
      "Epoch 1701 - Loss:     3.3495\n",
      "Epoch 1801 - Loss:     3.3783\n",
      "Epoch 1901 - Loss:     3.2074\n",
      "Testing Accuracy: 0.6250820159912109\n"
     ]
    }
   ],
   "source": [
    "neural_net(tf.nn.sigmoid, 2, 0.0005, 2000, 0.8, 5, 123)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
