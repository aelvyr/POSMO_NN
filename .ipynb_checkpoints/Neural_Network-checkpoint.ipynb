{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_array = ['label_id', 'acceleration_x', 'acceleration_y', 'acceleration_z', 'gyroscope_x', 'gyroscope_y', 'gyroscope_z', 'magnetic_field_x', 'magnetic_field_y', 'magnetic_field_z']\n",
    "path = \"../posmo_clasifier/data/iOS/v8/\" \n",
    "def read_values(csv, col_array):\n",
    "    return pd.read_csv(csv).loc[:, col_array] \n",
    "def feature_extraction(df, f_train, l_train, f_test, l_test, seq_len=250):\n",
    "    df_np = df.as_matrix()\n",
    "    df_np = df_np[:(df_np.shape[0] - df_np.shape[0] % seq_len), :]\n",
    "    df_np = df_np.reshape(int(df_np.shape[0]/seq_len), seq_len, df_np.shape[1])\n",
    "    for i in range(df_np.shape[0]):\n",
    "        label = df_np[i, 0, 0]\n",
    "        feature = np.expand_dims(df_np[i, :, 1:10], axis=0)\n",
    "        if i % 10 == 0:\n",
    "            l_test = np.append(l_test, label)\n",
    "            f_test = np.concatenate([f_test, feature], axis=0)\n",
    "        else:\n",
    "            l_train = np.append(l_train, label)\n",
    "            f_train = np.concatenate([f_train, feature], axis=0)\n",
    "    return f_train, l_train, f_test, l_test\n",
    "def return_shapes():\n",
    "    return f_train.shape, l_train.shape, f_test.shape, l_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(col_array, inp_len, path, seq_len):\n",
    "    files = []\n",
    "    for fname in glob.glob(path + \"*.csv\"):\n",
    "        files.append(fname)\n",
    "    for file in files:\n",
    "        if file in glob.glob(path + \"Other*\"):\n",
    "            files.remove(file) \n",
    "    f_train = np.empty([1, seq_len, inp_len])\n",
    "    l_train = np.array([])\n",
    "    f_test = np.empty([1, seq_len, inp_len])\n",
    "    l_test = np.array([])\n",
    "    for file in files:\n",
    "        df = read_values(file, col_array)\n",
    "        f_train, l_train, f_test, l_test = feature_extraction(df, f_train, l_train, f_test, l_test, seq_len)\n",
    "    f_train = f_train[1:, :, :]\n",
    "    f_test = f_test[1:, :, :]\n",
    "    l_train_one_hot = tf.one_hot(l_train, 14)\n",
    "    l_test_one_hot = tf.one_hot(l_test, 14)\n",
    "    print(return_shapes())\n",
    "    return f_train, f_test, l_train, l_test, l_train_one_hot, l_test_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(activation_function, conv_layers, learning_rate, epochs, dropout, n_filters, seed):\n",
    "        \n",
    "        # network Parameters  \n",
    "        n_classes = 14\n",
    "    \n",
    "        tf.set_random_seed(seed)\n",
    "    \n",
    "        if conv_layers == 2:\n",
    "            weights = {\n",
    "                'wc1': tf.Variable(tf.random_normal([n_filters, 9, 32])),\n",
    "                'wc2': tf.Variable(tf.random_normal([n_filters, 32, 64])),\n",
    "                'wc3': tf.Variable(tf.random_normal([n_filters, 64, 128])),\n",
    "                'wd1': tf.Variable(tf.random_normal([10*64, 1024])),\n",
    "                'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "        elif conv_layers == 3:\n",
    "            weights = {\n",
    "                'wc1': tf.Variable(tf.random_normal([n_filters, 9, 32])),\n",
    "                'wc2': tf.Variable(tf.random_normal([n_filters, 32, 64])),\n",
    "                'wc3': tf.Variable(tf.random_normal([n_filters, 64, 128])),\n",
    "                'wd1': tf.Variable(tf.random_normal([10*128, 1024])),\n",
    "                'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "        biases = {\n",
    "            'bc1': tf.Variable(tf.random_normal([32])),\n",
    "            'bc2': tf.Variable(tf.random_normal([64])),\n",
    "            'bc3': tf.Variable(tf.random_normal([128])),\n",
    "            'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "            'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "        # changed relu to sigmoid\n",
    "        # (best) result from reLu \n",
    "        def conv1d(x, W, b, s):\n",
    "            x = tf.nn.conv1d(x, W, s, padding='SAME')\n",
    "            x = tf.nn.bias_add(x, b)\n",
    "            return tf.nn.relu(x)\n",
    "\n",
    "        if conv_layers == 2:\n",
    "            def conv_net(x, weights, biases, dropout, activation_function):\n",
    "                # Conv Layer 1\n",
    "                conv1 = conv1d(x, weights['wc1'], biases['bc1'], 5)\n",
    "\n",
    "                # Conv Layer 2\n",
    "                conv2 = conv1d(conv1, weights['wc2'], biases['bc2'], 5)\n",
    "        \n",
    "                #Conv Layer 3\n",
    "                # conv3 = conv1d(conv2, weights['wc3'], biases['bc3'], 1)\n",
    "    \n",
    "\n",
    "                # Fully connected layer \n",
    "                # Changed relu to sigmoid\n",
    "                fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "                fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "                fc1 = activation_function(fc1)\n",
    "                fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "                # Output Layer - class prediction \n",
    "                out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "                return out\n",
    "        elif conv_layers == 3:\n",
    "            def conv_net(x, weights, biases, dropout, activation_function):\n",
    "                # Conv Layer 1\n",
    "                conv1 = conv1d(x, weights['wc1'], biases['bc1'], 5)\n",
    "\n",
    "                # Conv Layer 2\n",
    "                conv2 = conv1d(conv1, weights['wc2'], biases['bc2'], 5)\n",
    "    \n",
    "                #Conv Layer 3\n",
    "                conv3 = conv1d(conv2, weights['wc3'], biases['bc3'], 1)\n",
    "    \n",
    "\n",
    "                # Fully connected layer \n",
    "                # Changed relu to sigmoid\n",
    "                fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "                fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "                fc1 = activation_function(fc1)\n",
    "                fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "                # Output Layer - class prediction \n",
    "                out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "                return out\n",
    "\n",
    "        x = tf.placeholder(tf.float32, [None, 250, 9])\n",
    "        y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "        # Model\n",
    "        logits = conv_net(x, weights, biases, keep_prob, activation_function)\n",
    "    \n",
    "        # Define loss and optimizer\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # Accuracy\n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Launch the graph\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                sess.run(optimizer, feed_dict={\n",
    "                    x: f_train,\n",
    "                    y: l_train_one_hot.eval(),\n",
    "                    keep_prob: dropout})\n",
    "    \n",
    "                # Calculate batch loss and accuracy\n",
    "                loss = sess.run(cost, feed_dict={\n",
    "                    x: f_train,\n",
    "                    y: l_train_one_hot.eval(),\n",
    "                    keep_prob: 1.})\n",
    "    \n",
    "                if epoch % 100 == 0:\n",
    "                    print('Epoch {:>2} - '\n",
    "                        'Loss: {:>10.4f}'.format(\n",
    "                        epoch + 1,\n",
    "                        loss))\n",
    "\n",
    "        # Calculate Test Accuracy\n",
    "            test_acc = sess.run(accuracy, feed_dict={\n",
    "                x: f_test,\n",
    "                y: l_test_one_hot.eval(),\n",
    "                keep_prob: 1.})\n",
    "    return 'Testing Accuracy: {}'.format(test_acc)\n",
    "            \n",
    "def random_forest(activation_function=None, conv_layers=None, learning_rate=None, epochs=None, dropout=None, n_filters=None, seed=None):\n",
    "    def transform_3d_2d(features):\n",
    "        nsamples, nx, ny = features.shape\n",
    "        return features.reshape((nsamples,nx*ny))\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(transform_3d_2d(f_train), l_train)\n",
    "    pred = clf.predict(transform_3d_2d(f_test))\n",
    "    test_acc = metrics.accuracy_score(l_test, pred)  \n",
    "    return 'Testing Accuracy: {}'.format(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(clf, activation_function=None, conv_layers=None, learning_rate=None, epochs=None, dropout=None, n_filters=None, seed=None):    \n",
    "    acc = clf(activation_function, conv_layers, learning_rate, epochs, dropou, n_filters, seed)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train, f_test, l_train, l_test, l_train_one_hot, l_test_one_hot = preprocess(col_array, 9, path, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neural_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2a91fe56fe0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m123\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'neural_net' is not defined"
     ]
    }
   ],
   "source": [
    "neural_net(random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 - Loss:    14.7844\n",
      "Epoch 101 - Loss:    10.3681\n",
      "Epoch 201 - Loss:     9.9865\n",
      "Epoch 301 - Loss:     8.3634\n",
      "Epoch 401 - Loss:     7.1704\n",
      "Epoch 501 - Loss:     6.4957\n",
      "Epoch 601 - Loss:     6.1401\n",
      "Epoch 701 - Loss:     5.7386\n",
      "Epoch 801 - Loss:     5.0314\n",
      "Epoch 901 - Loss:     5.0131\n",
      "Epoch 1001 - Loss:     4.4731\n",
      "Epoch 1101 - Loss:     4.4357\n",
      "Epoch 1201 - Loss:     3.7790\n",
      "Epoch 1301 - Loss:     3.8026\n",
      "Epoch 1401 - Loss:     3.9143\n",
      "Epoch 1501 - Loss:     3.8617\n",
      "Epoch 1601 - Loss:     3.6310\n",
      "Epoch 1701 - Loss:     3.3495\n",
      "Epoch 1801 - Loss:     3.3783\n",
      "Epoch 1901 - Loss:     3.2074\n",
      "Testing Accuracy: 0.6250820159912109\n"
     ]
    }
   ],
   "source": [
    "neural_net(cnn, tf.nn.sigmoid, 2, 0.0005, 2000, 0.8, 5, 123)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
